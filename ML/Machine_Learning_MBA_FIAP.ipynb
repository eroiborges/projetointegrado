{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8dde89",
   "metadata": {},
   "source": [
    "# Resumo e Pr√°tica de Machine Learning (MBA FIAP)\n",
    "Este notebook cont√©m o conte√∫do das aulas de Machine Learning do MBA, incluindo teoria, cen√°rios de uso, exemplos pr√°ticos e conceitos como overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0b918",
   "metadata": {},
   "source": [
    "## üìã Resumo das Aulas\n",
    "| Aula         | T√≥pico                        | Resumo |\n",
    "|--------------|-------------------------------|--------|\n",
    "| Aula 0       | Introdu√ß√£o ao ML              | Conceitos b√°sicos, ciclo de vida de dados e modelos, perfis profissionais, import√¢ncia dos dados. |\n",
    "| Aula 1       | Tipos de Analytics            | Descritivo, Diagn√≥stico, Preditivo, Prescritivo. Diferen√ßas e aplica√ß√µes no neg√≥cio. |\n",
    "| Aula 1       | Tipos de Aprendizado          | Supervisionado, n√£o supervisionado, refor√ßo. Exemplos e aplica√ß√µes. |\n",
    "| Aula 1       | Ciclo de vida do modelo       | Defini√ß√£o do caso de uso, aquisi√ß√£o de dados, sele√ß√£o de algoritmo, constru√ß√£o, valida√ß√£o, implanta√ß√£o. |\n",
    "| Aula 2       | SVM (Support Vector Machines) | Algoritmo de classifica√ß√£o que busca o hiperplano de maior margem. Uso de kernels para dados n√£o lineares. |\n",
    "| Aula 2       | M√©tricas de Avalia√ß√£o         | Matriz de confus√£o, acur√°cia, precision, recall, F1-score, ROC e AUC. |\n",
    "| Aula 3       | K-means                       | Algoritmo de agrupamento particional. Sens√≠vel √† inicializa√ß√£o, bom para clusters esf√©ricos. |\n",
    "| Aula 3       | Hierarchical Clustering       | Agrupamento hier√°rquico (bottom-up/top-down), uso de dendrogramas, estrat√©gias de linkage. |\n",
    "| Aula 3       | DBSCAN                        | Algoritmo de agrupamento por densidade, identifica clusters de formas arbitr√°rias e outliers. |\n",
    "| Aula 4       | CART (√Årvores de Decis√£o)     | Algoritmo para classifica√ß√£o e regress√£o. Usa m√©tricas como Gini e RSS/MSE. Pruning para evitar overfitting. |\n",
    "| Aula 5       | Ensemble Methods              | Combina√ß√£o de modelos para melhorar performance. Bagging, Boosting, Voting. |\n",
    "| Aula 5       | Random Forest                 | Ensemble de √°rvores de decis√£o com amostragem bootstrap e sele√ß√£o aleat√≥ria de features. |\n",
    "| Aula 6       | XGBoost                       | Algoritmo de boosting eficiente, regulariza√ß√£o L1/L2, otimiza√ß√µes para grandes volumes de dados. |\n",
    "| Aula 7       | LightGBM                      | Boosting baseado em √°rvores, otimiza√ß√µes GOSS e EFB para efici√™ncia e escalabilidade. |\n",
    "| Regulariza√ß√£o| Ridge, Lasso, Elastic Net     | T√©cnicas para evitar overfitting em modelos lineares, penalizando coeficientes (L2, L1, combina√ß√£o). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43ce8b1",
   "metadata": {},
   "source": [
    "Exemplos de Resumos de T√≥picos\n",
    "\n",
    "K-means: Algoritmo de agrupamento que particiona dados em k clusters, minimizando a soma das dist√¢ncias intra-cluster. Sens√≠vel √† inicializa√ß√£o dos centr√≥ides e √† presen√ßa de outliers.\n",
    "DBSCAN: Algoritmo de agrupamento baseado em densidade. N√£o requer n√∫mero de clusters a priori, identifica ru√≠do e √© robusto a outliers, mas sens√≠vel √† escolha dos par√¢metros Œµ e min_samples.\n",
    "CART: Algoritmo de √°rvore de decis√£o para classifica√ß√£o e regress√£o. Utiliza m√©tricas como Gini (classifica√ß√£o) e RSS/MSE (regress√£o). Pruning √© usado para evitar overfitting.\n",
    "XGBoost: Algoritmo de boosting de √°rvores com regulariza√ß√£o, otimiza√ß√µes para dados esparsos, paraleliza√ß√£o e uso eficiente de mem√≥ria. Muito utilizado em competi√ß√µes de ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a429858",
   "metadata": {},
   "source": [
    "1. K-means (Agrupamento N√£o Supervisionado)\n",
    "Teoria\n",
    "O K-means √© um algoritmo de agrupamento (clustering) que divide os dados em K grupos (clusters) baseando-se na proximidade dos pontos ao centr√≥ide do grupo. O objetivo √© minimizar a soma das dist√¢ncias quadradas entre os pontos e o centr√≥ide do seu cluster.\n",
    "\n",
    "Iterativo: Inicializa K centr√≥ides aleat√≥rios, atribui cada ponto ao centr√≥ide mais pr√≥ximo, recalcula os centr√≥ides e repete at√© convergir.\n",
    "Fun√ß√£o objetivo: Minimizar a soma das dist√¢ncias quadradas intra-cluster.\n",
    "\n",
    "Cen√°rios de uso\n",
    "\n",
    "Segmenta√ß√£o de clientes (marketing)\n",
    "Compress√£o de imagens (redu√ß√£o de cores)\n",
    "Agrupamento de documentos/textos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e12c61",
   "metadata": {},
   "source": [
    "## Exemplo pr√°tico em Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Gerar dados sint√©ticos\n",
    "X, y = make_blobs(n_samples=300, centers=4, random_state=42)\n",
    "\n",
    "# Aplicar K-means\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "kmeans.fit(X)\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Visualizar resultado\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.6)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200)\n",
    "plt.title('K-means clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f3a831",
   "metadata": {},
   "source": [
    "2. DBSCAN (Agrupamento por Densidade)\n",
    "Teoria\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) identifica clusters como regi√µes de alta densidade separadas por regi√µes de baixa densidade. N√£o precisa do n√∫mero de clusters a priori e identifica outliers.\n",
    "\n",
    "Par√¢metros: eps (raio de vizinhan√ßa), min_samples (m√≠nimo de pontos para formar um cluster).\n",
    "Tipos de pontos: core, border, noise.\n",
    "\n",
    "Cen√°rios de uso\n",
    "\n",
    "Detec√ß√£o de anomalias (fraudes, falhas)\n",
    "Agrupamento de dados espaciais/geogr√°ficos\n",
    "Dados com clusters de formas arbitr√°rias\n",
    "\n",
    "## Exemplo pr√°tico em Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0f4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='plasma')\n",
    "plt.title('DBSCAN clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c937822",
   "metadata": {},
   "source": [
    "3. SVM (Support Vector Machine)\n",
    "Teoria\n",
    "SVM √© um algoritmo de classifica√ß√£o (e regress√£o) que busca o hiperplano que melhor separa as classes, maximizando a margem entre elas. Pode usar kernels para separar dados n√£o linearmente separ√°veis.\n",
    "\n",
    "Kernel trick: transforma dados para espa√ßo de maior dimens√£o.\n",
    "Soft margin: permite alguns erros para maior generaliza√ß√£o.\n",
    "\n",
    "Cen√°rios de uso\n",
    "\n",
    "Classifica√ß√£o de textos (spam vs. n√£o-spam)\n",
    "Reconhecimento de imagens\n",
    "Diagn√≥stico m√©dico\n",
    "\n",
    "## Exemplo pr√°tico em Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3943623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=100, n_features=2, n_redundant=0, n_clusters_per_class=1)\n",
    "svc = SVC(kernel='rbf', C=1, gamma=1)\n",
    "svc.fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\n",
    "plt.title('SVM Classification')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bc7c3a",
   "metadata": {},
   "source": [
    "4. √Årvores de Decis√£o (CART)\n",
    "Teoria\n",
    "√Årvores de decis√£o s√£o modelos que particionam o espa√ßo de atributos em regi√µes, tomando decis√µes bin√°rias em cada n√≥. Usam m√©tricas como Gini (classifica√ß√£o) ou MSE (regress√£o).\n",
    "\n",
    "Pruning: poda para evitar overfitting.\n",
    "Interpreta√ß√£o f√°cil.\n",
    "\n",
    "Cen√°rios de uso\n",
    "\n",
    "Decis√£o de cr√©dito\n",
    "Diagn√≥stico m√©dico\n",
    "Previs√£o de vendas\n",
    "\n",
    "## Exemplo pr√°tico em Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7681336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "tree = DecisionTreeClassifier(max_depth=3)\n",
    "tree.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plot_tree(tree, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b884be",
   "metadata": {},
   "source": [
    "5. Random Forest (Ensemble de √Årvores)\n",
    "Teoria\n",
    "Random Forest √© um ensemble de √°rvores de decis√£o, cada uma treinada em uma amostra diferente dos dados e com subconjunto aleat√≥rio de features. Reduz overfitting e melhora a generaliza√ß√£o.\n",
    "Cen√°rios de uso\n",
    "\n",
    "Classifica√ß√£o de clientes\n",
    "Previs√£o de churn\n",
    "Diagn√≥stico de doen√ßas\n",
    "\n",
    "## Exemplo pr√°tico em Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def35bcb",
   "metadata": {},
   "source": [
    "6. XGBoost (Extreme Gradient Boosting)\n",
    "Teoria\n",
    "XGBoost √© um algoritmo de boosting de √°rvores, altamente eficiente, com regulariza√ß√£o L1/L2, paraleliza√ß√£o e otimiza√ß√µes para grandes volumes de dados.\n",
    "Cen√°rios de uso\n",
    "\n",
    "Competi√ß√µes de ML (Kaggle)\n",
    "Previs√£o de inadimpl√™ncia\n",
    "Modelos de risco\n",
    "\n",
    "## Exemplo pr√°tico em Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1345f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Acur√°cia:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd440f",
   "metadata": {},
   "source": [
    "# 7. Regulariza√ß√£o (Ridge, Lasso, Elastic Net)\n",
    "Teoria\n",
    "Regulariza√ß√£o √© usada para evitar overfitting em modelos lineares, penalizando coeficientes grandes.\n",
    "\n",
    "Ridge (L2): penaliza o quadrado dos coeficientes.\n",
    "Lasso (L1): penaliza o valor absoluto dos coeficientes (pode zerar alguns).\n",
    "Elastic Net: combina√ß√£o de L1 e L2.\n",
    "\n",
    "Cen√°rios de uso\n",
    "\n",
    "Regress√£o com muitos atributos\n",
    "Sele√ß√£o autom√°tica de vari√°veis\n",
    "\n",
    "## Exemplo pr√°tico em Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b404d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=10)\n",
    "ridge = Ridge(alpha=1.0)\n",
    "lasso = Lasso(alpha=0.1)\n",
    "enet = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "\n",
    "ridge.fit(X, y)\n",
    "lasso.fit(X, y)\n",
    "enet.fit(X, y)\n",
    "\n",
    "print(\"Ridge coef:\", ridge.coef_)\n",
    "print(\"Lasso coef:\", lasso.coef_)\n",
    "print(\"ElasticNet coef:\", enet.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de29b22",
   "metadata": {},
   "source": [
    "# üß† Overfitting\n",
    "Overfitting ocorre quando um modelo aprende n√£o s√≥ os padr√µes reais dos dados de treinamento, mas tamb√©m o ‚Äúru√≠do‚Äù ou as particularidades espec√≠ficas desses dados. Isso faz com que o modelo tenha um desempenho excelente nos dados de treino, mas ruim em dados novos.\n",
    "\n",
    "### Conceito\n",
    "Overfitting √© um conceito central em Machine Learning e estat√≠stica. Ele ocorre quando um modelo aprende n√£o s√≥ os padr√µes reais dos dados de treinamento, mas tamb√©m o ‚Äúru√≠do‚Äù ou as particularidades espec√≠ficas desses dados. Isso faz com que o modelo tenha um desempenho excelente nos dados de treino, mas um desempenho ruim em dados novos (teste ou produ√ß√£o).\n",
    "\n",
    "### O que √©?\n",
    "Overfitting acontece quando o modelo √© complexo demais para a quantidade de dados ou para a tarefa, ajustando-se at√© mesmo √†s pequenas flutua√ß√µes aleat√≥rias dos dados de treino.\n",
    "O modelo ‚Äúmemoriza‚Äù os dados de treino, mas n√£o ‚Äúgeneraliza‚Äù para novos dados.\n",
    "\n",
    "### Sintomas:\n",
    "Alta acur√°cia (ou baixo erro) no treino.\n",
    "Baixa acur√°cia (ou alto erro) no teste/valida√ß√£o.\n",
    "\n",
    "\n",
    "### Causas comuns:\n",
    "Modelo muito complexo (muitos par√¢metros, √°rvores profundas, polin√¥mios de grau alto, etc.).\n",
    "Poucos dados de treino.\n",
    "Falta de regulariza√ß√£o.\n",
    "\n",
    "## Cen√°rios de uso\n",
    "\n",
    "### Quando √© um problema?\n",
    "Sempre que voc√™ precisa que o modelo funcione bem em dados que nunca viu antes (produ√ß√£o, novos clientes, etc.).\n",
    "Exemplo: Um modelo de cr√©dito que aprende detalhes dos clientes antigos, mas n√£o consegue prever o risco de novos clientes.\n",
    "\n",
    "\n",
    "### Como evitar?\n",
    "Usar valida√ß√£o cruzada.\n",
    "Regulariza√ß√£o (Ridge, Lasso, Dropout em redes neurais).\n",
    "Reduzir a complexidade do modelo.\n",
    "Aumentar a quantidade de dados de treino.\n",
    "\n",
    "### O que voc√™ ver√°:\n",
    "Grau 1 (linear): subajuste (underfitting), erro alto em ambos.\n",
    "Grau 4: bom ajuste, erro baixo em ambos.\n",
    "Grau 12: overfitting, erro quase zero no treino, mas alto no teste.\n",
    "\n",
    "### Resumindo\n",
    "Overfitting = modelo aprende demais os dados de treino, perde capacidade de generaliza√ß√£o.\n",
    "Solu√ß√£o: simplifique o modelo, use mais dados, regulariza√ß√£o, valida√ß√£o cruzada.\n",
    "Sempre compare o desempenho em treino e teste!\n",
    "\n",
    "## Exemplo pr√°tico em Python\n",
    "Um exemplo visual com regress√£o polinomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e257f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.linspace(0, 1, 15).reshape(-1, 1)\n",
    "y = np.sin(2 * np.pi * X).ravel() + np.random.normal(0, 0.15, X.shape[0])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "degrees = [1, 4, 12]\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i, d in enumerate(degrees):\n",
    "    poly = PolynomialFeatures(degree=d)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    y_train_pred = model.predict(X_train_poly)\n",
    "    y_test_pred = model.predict(X_test_poly)\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.scatter(X_train, y_train, color='blue', label='Treino')\n",
    "    plt.scatter(X_test, y_test, color='red', label='Teste')\n",
    "    X_plot = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "    plt.plot(X_plot, model.predict(poly.transform(X_plot)), color='green')\n",
    "    plt.title(f'Grau {d}\n",
    "Erro treino: {mean_squared_error(y_train, y_train_pred):.2f}\n",
    "Erro teste: {mean_squared_error(y_test, y_test_pred):.2f}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
