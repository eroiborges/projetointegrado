{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "888169a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# 🤖 Importações Iron Man (Tecnologia Avançada)\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verificar GPU disponível\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🚀 Iron Man Tech Stack carregado!\")\n",
    "print(f\"🔧 Device: {device}\")\n",
    "print(f\"🧠 PyTorch version: {torch.__version__}\")\n",
    "print(f\"💡 Sistema pronto para Deep Q-Learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2085f",
   "metadata": {},
   "source": [
    "## ⚙️ CONFIGURAÇÃO IRON MAN - Estado da Arte\n",
    "\n",
    "Sistema avançado com redes neurais para qualquer ativo. Simplesmente altere `TICKER_SYMBOL`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7bd6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 CONFIGURAÇÃO IRON MAN - Tecnologia de Ponta\n",
    "TICKER_SYMBOL = \"PETR3.SA\"    # Flexível para qualquer ativo\n",
    "PERIOD = \"2y\"                 # Mais dados para Deep Learning\n",
    "INITIAL_CAPITAL = 10000.0\n",
    "\n",
    "# Parâmetros DQN (Iron Man Tech)\n",
    "STATE_SIZE = 20               # Features do estado (preços + indicadores)\n",
    "HIDDEN_SIZE = 128             # Neurônios na camada oculta\n",
    "LEARNING_RATE = 0.001         # Learning rate para Adam\n",
    "BATCH_SIZE = 64               # Batch size para treinamento\n",
    "MEMORY_SIZE = 10000           # Tamanho do replay buffer\n",
    "TARGET_UPDATE = 100           # Frequência de atualização da target network\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "GAMMA = 0.99                  # Discount factor\n",
    "\n",
    "# Parâmetros de treinamento\n",
    "NUM_EPISODES = 2000           # Mais episódios para DQN\n",
    "WARMUP_EPISODES = 100         # Episódios de aquecimento\n",
    "\n",
    "# Indicadores técnicos (Iron Man usa features avançadas)\n",
    "WINDOW_SIZE = 10              # Janela para médias móveis\n",
    "RSI_PERIOD = 14               # Período do RSI\n",
    "\n",
    "print(f\"🤖 Iron Man DQN configurado para: {TICKER_SYMBOL}\")\n",
    "print(f\"🧠 Arquitetura: {STATE_SIZE} → {HIDDEN_SIZE} → 3 ações\")\n",
    "print(f\"💾 Memory buffer: {MEMORY_SIZE:,} experiências\")\n",
    "print(f\"🎯 Episodes: {NUM_EPISODES}\")\n",
    "print(f\"⚡ Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d2f4b",
   "metadata": {},
   "source": [
    "## 📊 SISTEMA AVANÇADO DE DADOS IRON MAN\n",
    "\n",
    "Carregamento inteligente com features avançadas para Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85280f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔬 Sistema Avançado de Features Iron Man\n",
    "def load_advanced_data(ticker_symbol, period=\"2y\"):\n",
    "    \"\"\"\n",
    "    Carrega dados com features avançadas para Deep Learning\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"🚀 Iron Man carregando dados de {ticker_symbol}...\")\n",
    "        ticker = yf.Ticker(ticker_symbol)\n",
    "        df = ticker.history(period=period)\n",
    "        \n",
    "        if df.empty:\n",
    "            raise ValueError(f\"Dados não encontrados para {ticker_symbol}\")\n",
    "        \n",
    "        # Features básicas\n",
    "        df['Returns'] = df['Close'].pct_change()\n",
    "        df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "        \n",
    "        # Médias móveis\n",
    "        df['SMA_5'] = df['Close'].rolling(5).mean()\n",
    "        df['SMA_10'] = df['Close'].rolling(10).mean()\n",
    "        df['SMA_20'] = df['Close'].rolling(20).mean()\n",
    "        \n",
    "        # Exponential Moving Average\n",
    "        df['EMA_12'] = df['Close'].ewm(span=12).mean()\n",
    "        df['EMA_26'] = df['Close'].ewm(span=26).mean()\n",
    "        \n",
    "        # MACD\n",
    "        df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
    "        df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        df['BB_Middle'] = df['Close'].rolling(20).mean()\n",
    "        bb_std = df['Close'].rolling(20).std()\n",
    "        df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)\n",
    "        df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)\n",
    "        df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
    "        \n",
    "        # RSI\n",
    "        delta = df['Close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(RSI_PERIOD).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(RSI_PERIOD).mean()\n",
    "        rs = gain / loss\n",
    "        df['RSI'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # Volatilidade\n",
    "        df['Volatility'] = df['Returns'].rolling(10).std()\n",
    "        df['ATR'] = df[['High', 'Low', 'Close']].apply(lambda x: x['High'] - x['Low'], axis=1).rolling(14).mean()\n",
    "        \n",
    "        # Volume features\n",
    "        df['Volume_SMA'] = df['Volume'].rolling(10).mean()\n",
    "        df['Volume_Ratio'] = df['Volume'] / df['Volume_SMA']\n",
    "        \n",
    "        # Momentum\n",
    "        df['Momentum_5'] = df['Close'] / df['Close'].shift(5)\n",
    "        df['Momentum_10'] = df['Close'] / df['Close'].shift(10)\n",
    "        \n",
    "        # Price position\n",
    "        df['Price_Position'] = (df['Close'] - df['Close'].rolling(20).min()) / (df['Close'].rolling(20).max() - df['Close'].rolling(20).min())\n",
    "        \n",
    "        # Remover NaN\n",
    "        df = df.dropna()\n",
    "        \n",
    "        info = ticker.info\n",
    "        company_name = info.get('longName', ticker_symbol)\n",
    "        \n",
    "        print(f\"✅ Dados Iron Man carregados: {company_name}\")\n",
    "        print(f\"📊 Features: {len(df.columns)} colunas\")\n",
    "        print(f\"📅 Período: {df.index[0].date()} até {df.index[-1].date()}\")\n",
    "        print(f\"📈 Observações: {len(df)}\")\n",
    "        print(f\"💰 Preço atual: R$ {df['Close'].iloc[-1]:.2f}\")\n",
    "        \n",
    "        return df, info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro Iron Man: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def create_feature_matrix(df, window_size=WINDOW_SIZE):\n",
    "    \"\"\"\n",
    "    Cria matriz de features para DQN\n",
    "    \"\"\"\n",
    "    feature_columns = [\n",
    "        'Close', 'Volume', 'Returns', 'SMA_5', 'SMA_10', 'SMA_20',\n",
    "        'EMA_12', 'EMA_26', 'MACD', 'MACD_Signal', 'BB_Position', \n",
    "        'RSI', 'Volatility', 'ATR', 'Volume_Ratio', 'Momentum_5', \n",
    "        'Momentum_10', 'Price_Position'\n",
    "    ]\n",
    "    \n",
    "    # Normalizar features (exceto Close que será normalizado por janela)\n",
    "    df_norm = df.copy()\n",
    "    for col in feature_columns[1:]:  # Pular Close\n",
    "        if col in df_norm.columns:\n",
    "            mean_val = df_norm[col].mean()\n",
    "            std_val = df_norm[col].std()\n",
    "            if std_val > 0:\n",
    "                df_norm[col] = (df_norm[col] - mean_val) / std_val\n",
    "    \n",
    "    return df_norm[feature_columns].values, feature_columns\n",
    "\n",
    "# Carregar dados Iron Man\n",
    "df_ironman, ironman_info = load_advanced_data(TICKER_SYMBOL, PERIOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82687d84",
   "metadata": {},
   "source": [
    "## 🧠 DEEP Q-NETWORK IRON MAN\n",
    "\n",
    "Rede neural state-of-the-art com PyTorch para aproximação da função Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f467a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧠 Deep Q-Network Iron Man (Estado da Arte)\n",
    "class IronManDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Rede Neural Avançada Iron Man para Q-Learning\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, hidden_size=HIDDEN_SIZE, output_size=3):\n",
    "        super(IronManDQN, self).__init__()\n",
    "        \n",
    "        # Arquitetura avançada com dropout e batch normalization\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size // 2, output_size)\n",
    "        )\n",
    "        \n",
    "        # Inicialização Xavier\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Experience Replay Buffer\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class IronManReplayBuffer:\n",
    "    \"\"\"\n",
    "    Buffer de experiências para DQN com priorização\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity=MEMORY_SIZE):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size=BATCH_SIZE):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Ações Iron Man\n",
    "class IronManActions:\n",
    "    HOLD = 0\n",
    "    BUY = 1\n",
    "    SELL = 2\n",
    "    \n",
    "    @classmethod\n",
    "    def get_actions(cls):\n",
    "        return [cls.HOLD, cls.BUY, cls.SELL]\n",
    "    \n",
    "    @classmethod\n",
    "    def action_name(cls, action):\n",
    "        names = {cls.HOLD: \"HOLD\", cls.BUY: \"BUY\", cls.SELL: \"SELL\"}\n",
    "        return names.get(action, \"UNKNOWN\")\n",
    "\n",
    "# Verificar dados e inicializar componentes\n",
    "if df_ironman is not None:\n",
    "    # Criar matriz de features\n",
    "    feature_matrix, feature_names = create_feature_matrix(df_ironman, WINDOW_SIZE)\n",
    "    prices_ironman = df_ironman['Close'].values\n",
    "    \n",
    "    print(f\"🧠 Rede Neural Iron Man:\")\n",
    "    print(f\"   🔢 Input size: {STATE_SIZE}\")\n",
    "    print(f\"   🏗️ Hidden size: {HIDDEN_SIZE}\")\n",
    "    print(f\"   🎯 Output size: 3 ações\")\n",
    "    print(f\"   📊 Features disponíveis: {len(feature_names)}\")\n",
    "    print(f\"   🖥️ Device: {device}\")\n",
    "    \n",
    "    # Inicializar redes\n",
    "    main_net = IronManDQN(STATE_SIZE, HIDDEN_SIZE).to(device)\n",
    "    target_net = IronManDQN(STATE_SIZE, HIDDEN_SIZE).to(device)\n",
    "    target_net.load_state_dict(main_net.state_dict())\n",
    "    \n",
    "    # Otimizador\n",
    "    optimizer = optim.Adam(main_net.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Replay buffer\n",
    "    replay_buffer = IronManReplayBuffer(MEMORY_SIZE)\n",
    "    \n",
    "    print(\"✅ Componentes Iron Man inicializados!\")\n",
    "    print(f\"   🧠 Redes: Main + Target\")\n",
    "    print(f\"   💾 Buffer: {MEMORY_SIZE:,} experiências\")\n",
    "    print(f\"   ⚙️ Otimizador: Adam (lr={LEARNING_RATE})\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Erro: Dados Iron Man não carregados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc015f99",
   "metadata": {},
   "source": [
    "## 🏗️ AMBIENTE AVANÇADO IRON MAN\n",
    "\n",
    "Ambiente de trading com estados contínuos e features avançadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7232a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏗️ Ambiente de Trading Avançado Iron Man\n",
    "class IronManTradingEnvironment:\n",
    "    \"\"\"\n",
    "    Ambiente avançado com estados contínuos e múltiplas features\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_matrix, prices, initial_capital=INITIAL_CAPITAL, window_size=WINDOW_SIZE):\n",
    "        self.feature_matrix = feature_matrix\n",
    "        self.prices = prices\n",
    "        self.initial_capital = initial_capital\n",
    "        self.window_size = window_size\n",
    "        self.reset()\n",
    "        \n",
    "        # Estatísticas para normalização\n",
    "        self.price_mean = np.mean(prices)\n",
    "        self.price_std = np.std(prices)\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia ambiente para novo episódio\"\"\"\n",
    "        self.current_step = self.window_size\n",
    "        self.cash = self.initial_capital\n",
    "        self.shares = 0\n",
    "        self.portfolio_history = []\n",
    "        self.action_history = []\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"\n",
    "        Cria estado avançado com janela de features\n",
    "        \"\"\"\n",
    "        if self.current_step < self.window_size:\n",
    "            # Padding para início\n",
    "            padding = np.zeros((self.window_size - self.current_step - 1, self.feature_matrix.shape[1]))\n",
    "            window_data = np.vstack([padding, self.feature_matrix[:self.current_step + 1]])\n",
    "        else:\n",
    "            window_data = self.feature_matrix[self.current_step - self.window_size + 1:self.current_step + 1]\n",
    "        \n",
    "        # Normalizar preços na janela pelo preço atual\n",
    "        current_price = self.prices[self.current_step]\n",
    "        price_normalized = window_data[:, 0] / current_price  # Close price é primeira coluna\n",
    "        \n",
    "        # Combinar preços normalizados com outras features\n",
    "        other_features = window_data[:, 1:].flatten()  # Outras features\n",
    "        \n",
    "        # Estado final: preços + features + posição atual\n",
    "        portfolio_value = self.cash + self.shares * current_price\n",
    "        position_info = np.array([\n",
    "            self.cash / self.initial_capital,  # Cash ratio\n",
    "            self.shares * current_price / self.initial_capital,  # Position ratio\n",
    "            portfolio_value / self.initial_capital,  # Total value ratio\n",
    "        ])\n",
    "        \n",
    "        # Combinar tudo\n",
    "        state = np.concatenate([\n",
    "            price_normalized,\n",
    "            other_features[:STATE_SIZE-self.window_size-3],  # Limitar tamanho\n",
    "            position_info\n",
    "        ])\n",
    "        \n",
    "        # Garantir tamanho fixo\n",
    "        if len(state) > STATE_SIZE:\n",
    "            state = state[:STATE_SIZE]\n",
    "        elif len(state) < STATE_SIZE:\n",
    "            padding = np.zeros(STATE_SIZE - len(state))\n",
    "            state = np.concatenate([state, padding])\n",
    "        \n",
    "        return state.astype(np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Executa ação e retorna (state, reward, done, info)\"\"\"\n",
    "        current_price = self.prices[self.current_step]\n",
    "        portfolio_before = self.cash + self.shares * current_price\n",
    "        \n",
    "        # Executar ação\n",
    "        action_executed = False\n",
    "        transaction_cost = 0\n",
    "        \n",
    "        if action == IronManActions.BUY and self.cash >= current_price:\n",
    "            shares_to_buy = int(self.cash // current_price)  # Comprar máximo possível\n",
    "            if shares_to_buy > 0:\n",
    "                self.shares += shares_to_buy\n",
    "                cost = shares_to_buy * current_price\n",
    "                transaction_cost = cost * 0.001  # 0.1% de taxa\n",
    "                self.cash -= (cost + transaction_cost)\n",
    "                action_executed = True\n",
    "                \n",
    "        elif action == IronManActions.SELL and self.shares > 0:\n",
    "            shares_to_sell = self.shares\n",
    "            revenue = shares_to_sell * current_price\n",
    "            transaction_cost = revenue * 0.001  # 0.1% de taxa\n",
    "            self.cash += (revenue - transaction_cost)\n",
    "            self.shares = 0\n",
    "            action_executed = True\n",
    "        \n",
    "        # HOLD sempre é válido\n",
    "        if action == IronManActions.HOLD:\n",
    "            action_executed = True\n",
    "        \n",
    "        # Calcular recompensa avançada\n",
    "        portfolio_after = self.cash + self.shares * current_price\n",
    "        \n",
    "        # Recompensa base: mudança no portfólio\n",
    "        portfolio_return = (portfolio_after - portfolio_before) / portfolio_before if portfolio_before > 0 else 0\n",
    "        \n",
    "        # Recompensa por performance vs mercado\n",
    "        if self.current_step > 0:\n",
    "            market_return = (current_price - self.prices[self.current_step - 1]) / self.prices[self.current_step - 1]\n",
    "            alpha_reward = (portfolio_return - market_return) * 100  # Alpha reward\n",
    "        else:\n",
    "            alpha_reward = 0\n",
    "        \n",
    "        # Penalidade por transação\n",
    "        transaction_penalty = transaction_cost\n",
    "        \n",
    "        # Recompensa final\n",
    "        reward = portfolio_return * 1000 + alpha_reward - transaction_penalty\n",
    "        \n",
    "        # Registrar histórico\n",
    "        self.portfolio_history.append(portfolio_after)\n",
    "        self.action_history.append(action)\n",
    "        \n",
    "        # Próximo step\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.prices) - 1\n",
    "        \n",
    "        next_state = self._get_state() if not done else None\n",
    "        \n",
    "        info = {\n",
    "            'portfolio_value': portfolio_after,\n",
    "            'cash': self.cash,\n",
    "            'shares': self.shares,\n",
    "            'current_price': current_price,\n",
    "            'action_executed': action_executed,\n",
    "            'transaction_cost': transaction_cost,\n",
    "            'alpha_reward': alpha_reward\n",
    "        }\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def get_performance_metrics(self):\n",
    "        \"\"\"Calcula métricas de performance\"\"\"\n",
    "        if not self.portfolio_history:\n",
    "            return None\n",
    "            \n",
    "        returns = np.diff(self.portfolio_history) / self.portfolio_history[:-1]\n",
    "        \n",
    "        total_return = (self.portfolio_history[-1] - self.initial_capital) / self.initial_capital\n",
    "        \n",
    "        # Sharpe ratio\n",
    "        if len(returns) > 1 and np.std(returns) > 0:\n",
    "            sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)  # Anualizado\n",
    "        else:\n",
    "            sharpe = 0\n",
    "        \n",
    "        # Maximum drawdown\n",
    "        peak = np.maximum.accumulate(self.portfolio_history)\n",
    "        drawdown = (self.portfolio_history - peak) / peak\n",
    "        max_drawdown = np.min(drawdown)\n",
    "        \n",
    "        return {\n",
    "            'total_return': total_return,\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'final_value': self.portfolio_history[-1],\n",
    "            'num_trades': len([a for a in self.action_history if a != IronManActions.HOLD])\n",
    "        }\n",
    "\n",
    "# Inicializar ambiente Iron Man\n",
    "if df_ironman is not None:\n",
    "    ironman_env = IronManTradingEnvironment(\n",
    "        feature_matrix, \n",
    "        prices_ironman, \n",
    "        INITIAL_CAPITAL, \n",
    "        WINDOW_SIZE\n",
    "    )\n",
    "    \n",
    "    print(\"🏗️ Ambiente Iron Man inicializado!\")\n",
    "    print(f\"   🎯 Estado size: {STATE_SIZE}\")\n",
    "    print(f\"   📊 Features: {feature_matrix.shape[1]}\")\n",
    "    print(f\"   💰 Capital: R$ {INITIAL_CAPITAL:,.2f}\")\n",
    "    print(f\"   📈 Dados: {len(prices_ironman)} dias\")\n",
    "    \n",
    "    # Testar estado\n",
    "    test_state = ironman_env.reset()\n",
    "    print(f\"   🧪 Estado teste: shape {test_state.shape}, range [{test_state.min():.3f}, {test_state.max():.3f}]\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Ambiente Iron Man não pode ser inicializado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da925dbb",
   "metadata": {},
   "source": [
    "## 🤖 AGENTE DQN IRON MAN\n",
    "\n",
    "Agente avançado com Deep Q-Learning, Experience Replay e Target Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce375dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🤖 Agente DQN Iron Man (Estado da Arte)\n",
    "class IronManDQNAgent:\n",
    "    \"\"\"\n",
    "    Agente Deep Q-Learning avançado com todas as técnicas modernas\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size=STATE_SIZE, action_size=3, lr=LEARNING_RATE):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Parâmetros de exploração\n",
    "        self.epsilon = EPSILON_START\n",
    "        self.epsilon_min = EPSILON_MIN\n",
    "        self.epsilon_decay = EPSILON_DECAY\n",
    "        \n",
    "        # Redes neurais\n",
    "        self.main_net = IronManDQN(state_size, HIDDEN_SIZE, action_size).to(device)\n",
    "        self.target_net = IronManDQN(state_size, HIDDEN_SIZE, action_size).to(device)\n",
    "        self.target_net.load_state_dict(self.main_net.state_dict())\n",
    "        \n",
    "        # Otimizador\n",
    "        self.optimizer = optim.Adam(self.main_net.parameters(), lr=lr)\n",
    "        \n",
    "        # Experience replay\n",
    "        self.memory = IronManReplayBuffer(MEMORY_SIZE)\n",
    "        \n",
    "        # Estatísticas\n",
    "        self.losses = []\n",
    "        self.episode_rewards = []\n",
    "        self.episode_returns = []\n",
    "        self.q_values_history = []\n",
    "        \n",
    "        print(f\"🤖 Agente Iron Man DQN inicializado!\")\n",
    "        print(f\"   🧠 Arquitetura: {state_size} → {HIDDEN_SIZE} → {action_size}\")\n",
    "        print(f\"   ⚡ Device: {device}\")\n",
    "        print(f\"   🎯 Epsilon: {self.epsilon}\")\n",
    "        \n",
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"\n",
    "        Seleciona ação usando ε-greedy com rede neural\n",
    "        \"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            # Exploração\n",
    "            return random.choice(range(self.action_size)), True\n",
    "        \n",
    "        # Exploitação usando rede neural\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = self.main_net(state_tensor)\n",
    "            action = q_values.argmax().item()\n",
    "            \n",
    "        # Registrar Q-values para análise\n",
    "        if training:\n",
    "            self.q_values_history.append(q_values.cpu().numpy().flatten())\n",
    "            \n",
    "        return action, False\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Armazena experiência no buffer\"\"\"\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"\n",
    "        Treina a rede usando batch de experiências (Experience Replay)\n",
    "        \"\"\"\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return None\n",
    "            \n",
    "        # Sample batch\n",
    "        experiences = self.memory.sample(BATCH_SIZE)\n",
    "        \n",
    "        states = torch.FloatTensor([e.state for e in experiences]).to(device)\n",
    "        actions = torch.LongTensor([e.action for e in experiences]).to(device)\n",
    "        rewards = torch.FloatTensor([e.reward for e in experiences]).to(device)\n",
    "        next_states = torch.FloatTensor([e.next_state if not e.done else np.zeros(self.state_size) for e in experiences]).to(device)\n",
    "        dones = torch.BoolTensor([e.done for e in experiences]).to(device)\n",
    "        \n",
    "        # Q-values atuais\n",
    "        current_q_values = self.main_net(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Q-values do próximo estado (usando target network)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (GAMMA * next_q_values * ~dones)\n",
    "        \n",
    "        # Loss (Huber loss para estabilidade)\n",
    "        loss = F.smooth_l1_loss(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping para estabilidade\n",
    "        torch.nn.utils.clip_grad_norm_(self.main_net.parameters(), 1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "        self.losses.append(loss.item())\n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Atualiza target network\"\"\"\n",
    "        self.target_net.load_state_dict(self.main_net.state_dict())\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        \"\"\"Treina um episódio completo\"\"\"\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_loss = 0\n",
    "        loss_count = 0\n",
    "        \n",
    "        while True:\n",
    "            # Selecionar ação\n",
    "            action, is_exploration = self.get_action(state, training=True)\n",
    "            \n",
    "            # Executar ação\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Armazenar experiência\n",
    "            self.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Treinar se temos experiências suficientes\n",
    "            if len(self.memory) >= BATCH_SIZE:\n",
    "                loss = self.replay()\n",
    "                if loss is not None:\n",
    "                    episode_loss += loss\n",
    "                    loss_count += 1\n",
    "            \n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "        \n",
    "        # Calcular métricas do episódio\n",
    "        performance = env.get_performance_metrics()\n",
    "        \n",
    "        # Registrar estatísticas\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        if performance:\n",
    "            self.episode_returns.append(performance['total_return'])\n",
    "        \n",
    "        return {\n",
    "            'episode_reward': episode_reward,\n",
    "            'episode_loss': episode_loss / max(1, loss_count),\n",
    "            'epsilon': self.epsilon,\n",
    "            'performance': performance,\n",
    "            'memory_size': len(self.memory)\n",
    "        }\n",
    "    \n",
    "    def get_training_stats(self, window=100):\n",
    "        \"\"\"Retorna estatísticas de treinamento\"\"\"\n",
    "        if not self.episode_rewards:\n",
    "            return None\n",
    "            \n",
    "        recent_rewards = self.episode_rewards[-window:]\n",
    "        recent_returns = self.episode_returns[-window:] if self.episode_returns else [0]\n",
    "        recent_losses = self.losses[-window:] if self.losses else [0]\n",
    "        \n",
    "        return {\n",
    "            'episodes': len(self.episode_rewards),\n",
    "            'avg_reward': np.mean(recent_rewards),\n",
    "            'avg_return': np.mean(recent_returns),\n",
    "            'avg_loss': np.mean(recent_losses),\n",
    "            'epsilon': self.epsilon,\n",
    "            'memory_size': len(self.memory)\n",
    "        }\n",
    "\n",
    "# Inicializar agente Iron Man\n",
    "if df_ironman is not None:\n",
    "    ironman_agent = IronManDQNAgent(STATE_SIZE, len(IronManActions.get_actions()), LEARNING_RATE)\n",
    "    \n",
    "    print(\"✅ Agente Iron Man DQN pronto!\")\n",
    "    print(f\"   🧠 Parâmetros: {sum(p.numel() for p in ironman_agent.main_net.parameters()):,}\")\n",
    "    print(f\"   💾 Memory capacity: {MEMORY_SIZE:,}\")\n",
    "    print(f\"   🎯 Target update: cada {TARGET_UPDATE} episódios\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Agente Iron Man não pode ser inicializado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569ee81a",
   "metadata": {},
   "source": [
    "## 🚀 TREINAMENTO IRON MAN\n",
    "\n",
    "Treinamento avançado com monitoramento em tempo real e otimizações modernas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ddb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 Treinamento Avançado Iron Man\n",
    "def train_ironman_agent(agent, env, num_episodes=NUM_EPISODES, print_every=200):\n",
    "    \"\"\"\n",
    "    Treinamento avançado com todas as técnicas modernas\n",
    "    \"\"\"\n",
    "    print(f\"🚀 Iniciando treinamento Iron Man DQN - {num_episodes} episódios\")\n",
    "    print(f\"🧠 Arquitetura: {agent.state_size} inputs → {HIDDEN_SIZE} hidden → {agent.action_size} outputs\")\n",
    "    print(f\"💾 Experience replay: {MEMORY_SIZE:,} experiências\")\n",
    "    print(f\"🎯 Target network update: cada {TARGET_UPDATE} episódios\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    training_history = {\n",
    "        'episodes': [],\n",
    "        'avg_reward': [],\n",
    "        'avg_return': [],\n",
    "        'avg_loss': [],\n",
    "        'epsilon': [],\n",
    "        'memory_size': [],\n",
    "        'sharpe_ratio': [],\n",
    "        'max_drawdown': []\n",
    "    }\n",
    "    \n",
    "    best_performance = -float('inf')\n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        # Treinar episódio\n",
    "        episode_info = agent.train_episode(env)\n",
    "        \n",
    "        # Atualizar target network\n",
    "        if episode % TARGET_UPDATE == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        # Relatório periódico\n",
    "        if episode % print_every == 0 or episode <= WARMUP_EPISODES:\n",
    "            stats = agent.get_training_stats()\n",
    "            performance = episode_info.get('performance', {})\n",
    "            \n",
    "            print(f\"🚀 Episódio {episode}/{num_episodes}\")\n",
    "            print(f\"   💰 Reward médio: {stats['avg_reward']:+.2f}\")\n",
    "            print(f\"   📊 Retorno médio: {stats['avg_return']:+.2%}\")\n",
    "            print(f\"   🧠 Loss médio: {stats['avg_loss']:.4f}\")\n",
    "            print(f\"   🔍 Epsilon: {stats['epsilon']:.3f}\")\n",
    "            print(f\"   💾 Memory: {stats['memory_size']:,}/{MEMORY_SIZE:,}\")\n",
    "            \n",
    "            if performance:\n",
    "                print(f\"   📈 Último retorno: {performance['total_return']:+.2%}\")\n",
    "                print(f\"   ⚡ Sharpe ratio: {performance['sharpe_ratio']:.3f}\")\n",
    "                print(f\"   📉 Max drawdown: {performance['max_drawdown']:+.2%}\")\n",
    "                print(f\"   🔄 Trades: {performance['num_trades']}\")\n",
    "                \n",
    "                # Salvar melhor modelo\n",
    "                if performance['total_return'] > best_performance:\n",
    "                    best_performance = performance['total_return']\n",
    "                    print(f\"   🏆 NOVO RECORD: {best_performance:+.2%}!\")\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Salvar histórico\n",
    "            training_history['episodes'].append(episode)\n",
    "            training_history['avg_reward'].append(stats['avg_reward'])\n",
    "            training_history['avg_return'].append(stats['avg_return'])\n",
    "            training_history['avg_loss'].append(stats['avg_loss'])\n",
    "            training_history['epsilon'].append(stats['epsilon'])\n",
    "            training_history['memory_size'].append(stats['memory_size'])\n",
    "            \n",
    "            if performance:\n",
    "                training_history['sharpe_ratio'].append(performance['sharpe_ratio'])\n",
    "                training_history['max_drawdown'].append(performance['max_drawdown'])\n",
    "            else:\n",
    "                training_history['sharpe_ratio'].append(0)\n",
    "                training_history['max_drawdown'].append(0)\n",
    "    \n",
    "    print(\"✅ Treinamento Iron Man concluído!\")\n",
    "    final_stats = agent.get_training_stats()\n",
    "    print(f\"📊 Estatísticas finais:\")\n",
    "    print(f\"   🧠 Parâmetros da rede: {sum(p.numel() for p in agent.main_net.parameters()):,}\")\n",
    "    print(f\"   💾 Experiências coletadas: {final_stats['memory_size']:,}\")\n",
    "    print(f\"   🎯 Epsilon final: {final_stats['epsilon']:.4f}\")\n",
    "    print(f\"   💰 Reward médio final: {final_stats['avg_reward']:+.2f}\")\n",
    "    print(f\"   📈 Retorno médio final: {final_stats['avg_return']:+.2%}\")\n",
    "    print(f\"   🏆 Melhor performance: {best_performance:+.2%}\")\n",
    "    \n",
    "    return training_history\n",
    "\n",
    "# Executar treinamento Iron Man\n",
    "if df_ironman is not None and 'ironman_agent' in locals():\n",
    "    print(f\"🚀 Iniciando treinamento Iron Man para {TICKER_SYMBOL}\")\n",
    "    print(f\"🧮 Usando {device}\")\n",
    "    \n",
    "    # Treinamento principal\n",
    "    ironman_training_history = train_ironman_agent(\n",
    "        ironman_agent, \n",
    "        ironman_env, \n",
    "        NUM_EPISODES, \n",
    "        print_every=250\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Componentes Iron Man não disponíveis para treinamento!\")\n",
    "    print(\"💡 Verifique se PyTorch está instalado: pip install torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b03739",
   "metadata": {},
   "source": [
    "## 📊 AVALIAÇÃO E ANÁLISE IRON MAN\n",
    "\n",
    "Avaliação completa com métricas avançadas e visualizações modernas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07459950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Avaliação Completa Iron Man\n",
    "def evaluate_ironman_agent(agent, env, num_episodes=50):\n",
    "    \"\"\"\n",
    "    Avaliação avançada com métricas de trading profissionais\n",
    "    \"\"\"\n",
    "    print(\"🧪 Avaliando agente Iron Man DQN...\")\n",
    "    \n",
    "    test_results = []\n",
    "    portfolio_curves = []\n",
    "    \n",
    "    # Desativar exploração para teste\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_portfolio = [env.initial_capital]\n",
    "        \n",
    "        while True:\n",
    "            action, _ = agent.get_action(state, training=False)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            episode_portfolio.append(info['portfolio_value'])\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        \n",
    "        performance = env.get_performance_metrics()\n",
    "        if performance:\n",
    "            test_results.append(performance)\n",
    "            portfolio_curves.append(episode_portfolio)\n",
    "    \n",
    "    # Restaurar epsilon\n",
    "    agent.epsilon = original_epsilon\n",
    "    \n",
    "    # Análise dos resultados\n",
    "    if test_results:\n",
    "        returns = [r['total_return'] for r in test_results]\n",
    "        sharpes = [r['sharpe_ratio'] for r in test_results]\n",
    "        drawdowns = [r['max_drawdown'] for r in test_results]\n",
    "        \n",
    "        avg_return = np.mean(returns)\n",
    "        avg_sharpe = np.mean(sharpes)\n",
    "        avg_drawdown = np.mean(drawdowns)\n",
    "        win_rate = len([r for r in returns if r > 0]) / len(returns)\n",
    "        volatility = np.std(returns)\n",
    "        \n",
    "        # Buy & Hold comparison\n",
    "        buy_hold_return = (prices_ironman[-1] - prices_ironman[env.window_size]) / prices_ironman[env.window_size]\n",
    "        \n",
    "        print(f\"📈 Resultados Iron Man DQN ({num_episodes} episódios):\")\n",
    "        print(f\"   💰 Retorno médio: {avg_return:+.2%}\")\n",
    "        print(f\"   ⚡ Sharpe ratio: {avg_sharpe:.3f}\")\n",
    "        print(f\"   📉 Max drawdown: {avg_drawdown:+.2%}\")\n",
    "        print(f\"   🎯 Taxa de sucesso: {win_rate:.1%}\")\n",
    "        print(f\"   📊 Volatilidade: {volatility:.2%}\")\n",
    "        print(f\"   📈 Buy & Hold: {buy_hold_return:+.2%}\")\n",
    "        print(f\"   🏆 Alpha vs B&H: {avg_return - buy_hold_return:+.2%}\")\n",
    "        \n",
    "        # Information Ratio\n",
    "        if volatility > 0:\n",
    "            info_ratio = (avg_return - buy_hold_return) / volatility\n",
    "            print(f\"   📊 Information Ratio: {info_ratio:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'avg_return': avg_return,\n",
    "            'avg_sharpe': avg_sharpe,\n",
    "            'avg_drawdown': avg_drawdown,\n",
    "            'win_rate': win_rate,\n",
    "            'volatility': volatility,\n",
    "            'buy_hold_return': buy_hold_return,\n",
    "            'alpha': avg_return - buy_hold_return,\n",
    "            'test_results': test_results,\n",
    "            'portfolio_curves': portfolio_curves\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "def plot_ironman_results(training_history, evaluation_results):\n",
    "    \"\"\"\n",
    "    Visualizações avançadas dos resultados Iron Man\n",
    "    \"\"\"\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'🤖 Iron Man DQN Results - {TICKER_SYMBOL}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Learning Curve (Retorno)\n",
    "    episodes = training_history['episodes']\n",
    "    axes[0,0].plot(episodes, training_history['avg_return'], 'b-', linewidth=2, label='DQN Return')\n",
    "    axes[0,0].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "    if evaluation_results:\n",
    "        axes[0,0].axhline(y=evaluation_results['buy_hold_return'], color='orange', linestyle='--', label='Buy & Hold')\n",
    "    axes[0,0].set_title('Evolução do Retorno (Treinamento)')\n",
    "    axes[0,0].set_xlabel('Episódio')\n",
    "    axes[0,0].set_ylabel('Retorno Médio')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Loss Function\n",
    "    axes[0,1].plot(episodes, training_history['avg_loss'], 'r-', linewidth=2)\n",
    "    axes[0,1].set_title('Loss Function (DQN)')\n",
    "    axes[0,1].set_xlabel('Episódio')\n",
    "    axes[0,1].set_ylabel('Loss Médio')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Epsilon Decay\n",
    "    axes[1,0].plot(episodes, training_history['epsilon'], 'g-', linewidth=2)\n",
    "    axes[1,0].set_title('Exploration Decay (Epsilon)')\n",
    "    axes[1,0].set_xlabel('Episódio')\n",
    "    axes[1,0].set_ylabel('Epsilon')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Sharpe Ratio Evolution\n",
    "    axes[1,1].plot(episodes, training_history['sharpe_ratio'], 'purple', linewidth=2)\n",
    "    axes[1,1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[1,1].set_title('Sharpe Ratio Evolution')\n",
    "    axes[1,1].set_xlabel('Episódio')\n",
    "    axes[1,1].set_ylabel('Sharpe Ratio')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Memory Usage\n",
    "    axes[2,0].plot(episodes, training_history['memory_size'], 'brown', linewidth=2)\n",
    "    axes[2,0].axhline(y=MEMORY_SIZE, color='r', linestyle='--', alpha=0.5, label='Max Capacity')\n",
    "    axes[2,0].set_title('Experience Replay Buffer')\n",
    "    axes[2,0].set_xlabel('Episódio')\n",
    "    axes[2,0].set_ylabel('Experiências Armazenadas')\n",
    "    axes[2,0].legend()\n",
    "    axes[2,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Performance Comparison\n",
    "    if evaluation_results:\n",
    "        methods = ['Iron Man DQN', 'Buy & Hold']\n",
    "        returns = [evaluation_results['avg_return'], evaluation_results['buy_hold_return']]\n",
    "        colors = ['red', 'orange']\n",
    "        \n",
    "        bars = axes[2,1].bar(methods, returns, color=colors, alpha=0.7)\n",
    "        axes[2,1].set_title('Performance Comparison')\n",
    "        axes[2,1].set_ylabel('Retorno')\n",
    "        axes[2,1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        for bar, return_val in zip(bars, returns):\n",
    "            height = bar.get_height()\n",
    "            axes[2,1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                          f'{return_val:+.2%}',\n",
    "                          ha='center', va='bottom' if height > 0 else 'top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Portfolio curves (se disponível)\n",
    "    if evaluation_results and evaluation_results['portfolio_curves']:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plotar algumas curvas de portfólio\n",
    "        curves_to_plot = min(10, len(evaluation_results['portfolio_curves']))\n",
    "        for i in range(curves_to_plot):\n",
    "            curve = evaluation_results['portfolio_curves'][i]\n",
    "            plt.plot(curve, alpha=0.3, color='blue')\n",
    "        \n",
    "        # Média das curvas\n",
    "        avg_curve = np.mean(evaluation_results['portfolio_curves'], axis=0)\n",
    "        plt.plot(avg_curve, color='red', linewidth=3, label='Média Iron Man')\n",
    "        \n",
    "        # Buy & Hold\n",
    "        buy_hold_curve = [INITIAL_CAPITAL * (1 + evaluation_results['buy_hold_return'] * i / len(avg_curve)) \n",
    "                         for i in range(len(avg_curve))]\n",
    "        plt.plot(buy_hold_curve, color='orange', linewidth=2, linestyle='--', label='Buy & Hold')\n",
    "        \n",
    "        plt.axhline(y=INITIAL_CAPITAL, color='black', linestyle=':', alpha=0.5, label='Capital Inicial')\n",
    "        plt.title('Evolução do Portfólio - Iron Man DQN')\n",
    "        plt.xlabel('Dias de Trading')\n",
    "        plt.ylabel('Valor do Portfólio (R$)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "# Executar avaliação Iron Man\n",
    "if df_ironman is not None and 'ironman_training_history' in locals():\n",
    "    print(\"📊 Executando avaliação completa Iron Man...\")\n",
    "    ironman_evaluation = evaluate_ironman_agent(ironman_agent, ironman_env, num_episodes=30)\n",
    "    plot_ironman_results(ironman_training_history, ironman_evaluation)\n",
    "else:\n",
    "    print(\"⚠️ Execute primeiro o treinamento para avaliar o agente Iron Man!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ad03d",
   "metadata": {},
   "source": [
    "## 🔧 FLEXIBILIDADE IRON MAN\n",
    "\n",
    "Sistema completamente flexível - use com qualquer ativo alterando apenas `TICKER_SYMBOL`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff17290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Sistema Iron Man - Flexibilidade Total\n",
    "\"\"\"\n",
    "🤖 IRON MAN DQN - GUIA DE USO AVANÇADO\n",
    "\n",
    "Para testar com outros ativos:\n",
    "\n",
    "1️⃣ Altere TICKER_SYMBOL na segunda célula:\n",
    "   TICKER_SYMBOL = \"VALE3.SA\"  # ou BRFS3.SA, ITUB4.SA, etc.\n",
    "\n",
    "2️⃣ Re-execute todas as células do notebook\n",
    "\n",
    "3️⃣ O sistema Iron Man automaticamente:\n",
    "   ✅ Baixa dados históricos do novo ativo\n",
    "   ✅ Calcula features avançadas (RSI, MACD, Bollinger, etc.)\n",
    "   ✅ Reconstrói a rede neural DQN\n",
    "   ✅ Treina o agente com Deep Q-Learning\n",
    "   ✅ Avalia performance com métricas profissionais\n",
    "   ✅ Gera visualizações avançadas\n",
    "\n",
    "🎯 ATIVOS RECOMENDADOS PARA TESTE:\n",
    "- PETR3.SA, PETR4.SA (Petrobras) - Alta liquidez\n",
    "- VALE3.SA (Vale) - Commodities\n",
    "- BRFS3.SA (BRF) - Consumo\n",
    "- ITUB4.SA (Itaú) - Financeiro\n",
    "- ABEV3.SA (Ambev) - Bebidas\n",
    "- WEGE3.SA (WEG) - Industrial\n",
    "\n",
    "⚡ DIFERENÇAS vs BATMAN:\n",
    "✅ Estados contínuos (vs discretos)\n",
    "✅ Redes neurais (vs tabela Q)\n",
    "✅ Experience replay (vs aprendizado direto)\n",
    "✅ Target networks (vs rede única)\n",
    "✅ Features avançadas (vs preços simples)\n",
    "✅ Métricas profissionais (vs básicas)\n",
    "\n",
    "💡 REQUISITOS:\n",
    "- PyTorch instalado: pip install torch\n",
    "- Mais RAM para redes neurais\n",
    "- GPU opcional (acelera treinamento)\n",
    "\n",
    "🔧 OTIMIZAÇÕES DISPONÍVEIS:\n",
    "- Ajustar HIDDEN_SIZE para complexidade\n",
    "- Modificar BATCH_SIZE para velocidade\n",
    "- Alterar LEARNING_RATE para convergência\n",
    "- Aumentar MEMORY_SIZE para mais experiência\n",
    "\"\"\"\n",
    "\n",
    "def save_ironman_model(agent, filepath=\"ironman_model.pth\"):\n",
    "    \"\"\"Salva o modelo treinado\"\"\"\n",
    "    torch.save({\n",
    "        'main_net_state_dict': agent.main_net.state_dict(),\n",
    "        'target_net_state_dict': agent.target_net.state_dict(),\n",
    "        'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "        'epsilon': agent.epsilon,\n",
    "        'training_stats': {\n",
    "            'episode_rewards': agent.episode_rewards,\n",
    "            'episode_returns': agent.episode_returns,\n",
    "            'losses': agent.losses\n",
    "        }\n",
    "    }, filepath)\n",
    "    print(f\"💾 Modelo Iron Man salvo em: {filepath}\")\n",
    "\n",
    "def load_ironman_model(agent, filepath=\"ironman_model.pth\"):\n",
    "    \"\"\"Carrega modelo salvo\"\"\"\n",
    "    try:\n",
    "        checkpoint = torch.load(filepath, map_location=device)\n",
    "        agent.main_net.load_state_dict(checkpoint['main_net_state_dict'])\n",
    "        agent.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "        agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        agent.epsilon = checkpoint['epsilon']\n",
    "        print(f\"📥 Modelo Iron Man carregado de: {filepath}\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ Arquivo não encontrado: {filepath}\")\n",
    "        return False\n",
    "\n",
    "# Resumo final Iron Man\n",
    "print(\"🤖 SISTEMA IRON MAN DQN COMPLETO!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"✅ Deep Q-Network com PyTorch\")\n",
    "print(\"✅ Estados contínuos avançados\")\n",
    "print(\"✅ Experience Replay Buffer\")  \n",
    "print(\"✅ Target Network para estabilidade\")\n",
    "print(\"✅ Features técnicas profissionais\")\n",
    "print(\"✅ Métricas de trading avançadas\")\n",
    "print(\"✅ Visualizações state-of-the-art\")\n",
    "print(\"✅ Sistema flexível para qualquer ativo\")\n",
    "print(\"✅ Otimizações modernas (batch norm, dropout)\")\n",
    "print(\"✅ Saving/Loading de modelos\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"🎯 Configurado para: {TICKER_SYMBOL}\")\n",
    "print(f\"🧠 Arquitetura: {STATE_SIZE} → {HIDDEN_SIZE} → 3\")\n",
    "print(f\"⚡ Device: {device}\")\n",
    "\n",
    "if 'ironman_agent' in locals():\n",
    "    print(f\"🤖 Status: Agente treinado e pronto!\")\n",
    "    print(\"💡 Para salvar modelo: save_ironman_model(ironman_agent)\")\n",
    "else:\n",
    "    print(\"⚠️ Status: Execute as células para treinar\")\n",
    "\n",
    "print(\"\\n🚀 Iron Man tech at your service!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5832a97",
   "metadata": {},
   "source": [
    "# 🤖 IRON MAN APPROACH - Reinforcement Learning Trading\n",
    "\n",
    "## Estratégia: Inovadora e Tecnológica\n",
    "\n",
    "### Filosofia Iron Man\n",
    "- **Tecnologia de ponta**: Deep Q-Networks (DQN) com redes neurais\n",
    "- **Abordagem moderna**: Estados contínuos e approximação de função\n",
    "- **Inovação constante**: Experience replay, target networks, double DQN\n",
    "- **Performance focada**: Otimizado para resultados reais de trading\n",
    "- **Escalabilidade**: Arquitetura preparada para múltiplos ativos\n",
    "\n",
    "### Objetivo\n",
    "Desenvolver um agente de Reinforcement Learning avançado usando **Deep Q-Learning**.\n",
    "O sistema deve ser **state-of-the-art** e funcionar com qualquer ativo (PETR3, VALE3, BRFS3, etc.).\n",
    "\n",
    "### Características da Implementação\n",
    "- ✅ Deep Q-Network (DQN) com PyTorch/TensorFlow\n",
    "- ✅ Estados contínuos (preços normalizados)\n",
    "- ✅ Experience Replay Buffer\n",
    "- ✅ Target Network para estabilidade\n",
    "- ✅ Indicadores técnicos como features\n",
    "- ✅ Arquitetura moderna e escalável\n",
    "- ✅ Performance otimizada"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
