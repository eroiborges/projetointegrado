{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "888169a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# ü§ñ Importa√ß√µes Iron Man (Tecnologia Avan√ßada)\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verificar GPU dispon√≠vel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Iron Man Tech Stack carregado!\")\n",
    "print(f\"üîß Device: {device}\")\n",
    "print(f\"üß† PyTorch version: {torch.__version__}\")\n",
    "print(f\"üí° Sistema pronto para Deep Q-Learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b2085f",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è CONFIGURA√á√ÉO IRON MAN - Estado da Arte\n",
    "\n",
    "Sistema avan√ßado com redes neurais para qualquer ativo. Simplesmente altere `TICKER_SYMBOL`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7bd6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ CONFIGURA√á√ÉO IRON MAN - Tecnologia de Ponta\n",
    "TICKER_SYMBOL = \"PETR3.SA\"    # Flex√≠vel para qualquer ativo\n",
    "PERIOD = \"2y\"                 # Mais dados para Deep Learning\n",
    "INITIAL_CAPITAL = 10000.0\n",
    "\n",
    "# Par√¢metros DQN (Iron Man Tech)\n",
    "STATE_SIZE = 20               # Features do estado (pre√ßos + indicadores)\n",
    "HIDDEN_SIZE = 128             # Neur√¥nios na camada oculta\n",
    "LEARNING_RATE = 0.001         # Learning rate para Adam\n",
    "BATCH_SIZE = 64               # Batch size para treinamento\n",
    "MEMORY_SIZE = 10000           # Tamanho do replay buffer\n",
    "TARGET_UPDATE = 100           # Frequ√™ncia de atualiza√ß√£o da target network\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DECAY = 0.995\n",
    "GAMMA = 0.99                  # Discount factor\n",
    "\n",
    "# Par√¢metros de treinamento\n",
    "NUM_EPISODES = 2000           # Mais epis√≥dios para DQN\n",
    "WARMUP_EPISODES = 100         # Epis√≥dios de aquecimento\n",
    "\n",
    "# Indicadores t√©cnicos (Iron Man usa features avan√ßadas)\n",
    "WINDOW_SIZE = 10              # Janela para m√©dias m√≥veis\n",
    "RSI_PERIOD = 14               # Per√≠odo do RSI\n",
    "\n",
    "print(f\"ü§ñ Iron Man DQN configurado para: {TICKER_SYMBOL}\")\n",
    "print(f\"üß† Arquitetura: {STATE_SIZE} ‚Üí {HIDDEN_SIZE} ‚Üí 3 a√ß√µes\")\n",
    "print(f\"üíæ Memory buffer: {MEMORY_SIZE:,} experi√™ncias\")\n",
    "print(f\"üéØ Episodes: {NUM_EPISODES}\")\n",
    "print(f\"‚ö° Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d2f4b",
   "metadata": {},
   "source": [
    "## üìä SISTEMA AVAN√áADO DE DADOS IRON MAN\n",
    "\n",
    "Carregamento inteligente com features avan√ßadas para Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85280f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî¨ Sistema Avan√ßado de Features Iron Man\n",
    "def load_advanced_data(ticker_symbol, period=\"2y\"):\n",
    "    \"\"\"\n",
    "    Carrega dados com features avan√ßadas para Deep Learning\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üöÄ Iron Man carregando dados de {ticker_symbol}...\")\n",
    "        ticker = yf.Ticker(ticker_symbol)\n",
    "        df = ticker.history(period=period)\n",
    "        \n",
    "        if df.empty:\n",
    "            raise ValueError(f\"Dados n√£o encontrados para {ticker_symbol}\")\n",
    "        \n",
    "        # Features b√°sicas\n",
    "        df['Returns'] = df['Close'].pct_change()\n",
    "        df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "        \n",
    "        # M√©dias m√≥veis\n",
    "        df['SMA_5'] = df['Close'].rolling(5).mean()\n",
    "        df['SMA_10'] = df['Close'].rolling(10).mean()\n",
    "        df['SMA_20'] = df['Close'].rolling(20).mean()\n",
    "        \n",
    "        # Exponential Moving Average\n",
    "        df['EMA_12'] = df['Close'].ewm(span=12).mean()\n",
    "        df['EMA_26'] = df['Close'].ewm(span=26).mean()\n",
    "        \n",
    "        # MACD\n",
    "        df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
    "        df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
    "        \n",
    "        # Bollinger Bands\n",
    "        df['BB_Middle'] = df['Close'].rolling(20).mean()\n",
    "        bb_std = df['Close'].rolling(20).std()\n",
    "        df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)\n",
    "        df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)\n",
    "        df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
    "        \n",
    "        # RSI\n",
    "        delta = df['Close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(RSI_PERIOD).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(RSI_PERIOD).mean()\n",
    "        rs = gain / loss\n",
    "        df['RSI'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # Volatilidade\n",
    "        df['Volatility'] = df['Returns'].rolling(10).std()\n",
    "        df['ATR'] = df[['High', 'Low', 'Close']].apply(lambda x: x['High'] - x['Low'], axis=1).rolling(14).mean()\n",
    "        \n",
    "        # Volume features\n",
    "        df['Volume_SMA'] = df['Volume'].rolling(10).mean()\n",
    "        df['Volume_Ratio'] = df['Volume'] / df['Volume_SMA']\n",
    "        \n",
    "        # Momentum\n",
    "        df['Momentum_5'] = df['Close'] / df['Close'].shift(5)\n",
    "        df['Momentum_10'] = df['Close'] / df['Close'].shift(10)\n",
    "        \n",
    "        # Price position\n",
    "        df['Price_Position'] = (df['Close'] - df['Close'].rolling(20).min()) / (df['Close'].rolling(20).max() - df['Close'].rolling(20).min())\n",
    "        \n",
    "        # Remover NaN\n",
    "        df = df.dropna()\n",
    "        \n",
    "        info = ticker.info\n",
    "        company_name = info.get('longName', ticker_symbol)\n",
    "        \n",
    "        print(f\"‚úÖ Dados Iron Man carregados: {company_name}\")\n",
    "        print(f\"üìä Features: {len(df.columns)} colunas\")\n",
    "        print(f\"üìÖ Per√≠odo: {df.index[0].date()} at√© {df.index[-1].date()}\")\n",
    "        print(f\"üìà Observa√ß√µes: {len(df)}\")\n",
    "        print(f\"üí∞ Pre√ßo atual: R$ {df['Close'].iloc[-1]:.2f}\")\n",
    "        \n",
    "        return df, info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro Iron Man: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def create_feature_matrix(df, window_size=WINDOW_SIZE):\n",
    "    \"\"\"\n",
    "    Cria matriz de features para DQN\n",
    "    \"\"\"\n",
    "    feature_columns = [\n",
    "        'Close', 'Volume', 'Returns', 'SMA_5', 'SMA_10', 'SMA_20',\n",
    "        'EMA_12', 'EMA_26', 'MACD', 'MACD_Signal', 'BB_Position', \n",
    "        'RSI', 'Volatility', 'ATR', 'Volume_Ratio', 'Momentum_5', \n",
    "        'Momentum_10', 'Price_Position'\n",
    "    ]\n",
    "    \n",
    "    # Normalizar features (exceto Close que ser√° normalizado por janela)\n",
    "    df_norm = df.copy()\n",
    "    for col in feature_columns[1:]:  # Pular Close\n",
    "        if col in df_norm.columns:\n",
    "            mean_val = df_norm[col].mean()\n",
    "            std_val = df_norm[col].std()\n",
    "            if std_val > 0:\n",
    "                df_norm[col] = (df_norm[col] - mean_val) / std_val\n",
    "    \n",
    "    return df_norm[feature_columns].values, feature_columns\n",
    "\n",
    "# Carregar dados Iron Man\n",
    "df_ironman, ironman_info = load_advanced_data(TICKER_SYMBOL, PERIOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82687d84",
   "metadata": {},
   "source": [
    "## üß† DEEP Q-NETWORK IRON MAN\n",
    "\n",
    "Rede neural state-of-the-art com PyTorch para aproxima√ß√£o da fun√ß√£o Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f467a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Deep Q-Network Iron Man (Estado da Arte)\n",
    "class IronManDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Rede Neural Avan√ßada Iron Man para Q-Learning\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, hidden_size=HIDDEN_SIZE, output_size=3):\n",
    "        super(IronManDQN, self).__init__()\n",
    "        \n",
    "        # Arquitetura avan√ßada com dropout e batch normalization\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(hidden_size // 2, output_size)\n",
    "        )\n",
    "        \n",
    "        # Inicializa√ß√£o Xavier\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Experience Replay Buffer\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class IronManReplayBuffer:\n",
    "    \"\"\"\n",
    "    Buffer de experi√™ncias para DQN com prioriza√ß√£o\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity=MEMORY_SIZE):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size=BATCH_SIZE):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# A√ß√µes Iron Man\n",
    "class IronManActions:\n",
    "    HOLD = 0\n",
    "    BUY = 1\n",
    "    SELL = 2\n",
    "    \n",
    "    @classmethod\n",
    "    def get_actions(cls):\n",
    "        return [cls.HOLD, cls.BUY, cls.SELL]\n",
    "    \n",
    "    @classmethod\n",
    "    def action_name(cls, action):\n",
    "        names = {cls.HOLD: \"HOLD\", cls.BUY: \"BUY\", cls.SELL: \"SELL\"}\n",
    "        return names.get(action, \"UNKNOWN\")\n",
    "\n",
    "# Verificar dados e inicializar componentes\n",
    "if df_ironman is not None:\n",
    "    # Criar matriz de features\n",
    "    feature_matrix, feature_names = create_feature_matrix(df_ironman, WINDOW_SIZE)\n",
    "    prices_ironman = df_ironman['Close'].values\n",
    "    \n",
    "    print(f\"üß† Rede Neural Iron Man:\")\n",
    "    print(f\"   üî¢ Input size: {STATE_SIZE}\")\n",
    "    print(f\"   üèóÔ∏è Hidden size: {HIDDEN_SIZE}\")\n",
    "    print(f\"   üéØ Output size: 3 a√ß√µes\")\n",
    "    print(f\"   üìä Features dispon√≠veis: {len(feature_names)}\")\n",
    "    print(f\"   üñ•Ô∏è Device: {device}\")\n",
    "    \n",
    "    # Inicializar redes\n",
    "    main_net = IronManDQN(STATE_SIZE, HIDDEN_SIZE).to(device)\n",
    "    target_net = IronManDQN(STATE_SIZE, HIDDEN_SIZE).to(device)\n",
    "    target_net.load_state_dict(main_net.state_dict())\n",
    "    \n",
    "    # Otimizador\n",
    "    optimizer = optim.Adam(main_net.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Replay buffer\n",
    "    replay_buffer = IronManReplayBuffer(MEMORY_SIZE)\n",
    "    \n",
    "    print(\"‚úÖ Componentes Iron Man inicializados!\")\n",
    "    print(f\"   üß† Redes: Main + Target\")\n",
    "    print(f\"   üíæ Buffer: {MEMORY_SIZE:,} experi√™ncias\")\n",
    "    print(f\"   ‚öôÔ∏è Otimizador: Adam (lr={LEARNING_RATE})\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Erro: Dados Iron Man n√£o carregados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc015f99",
   "metadata": {},
   "source": [
    "## üèóÔ∏è AMBIENTE AVAN√áADO IRON MAN\n",
    "\n",
    "Ambiente de trading com estados cont√≠nuos e features avan√ßadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7232a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèóÔ∏è Ambiente de Trading Avan√ßado Iron Man\n",
    "class IronManTradingEnvironment:\n",
    "    \"\"\"\n",
    "    Ambiente avan√ßado com estados cont√≠nuos e m√∫ltiplas features\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_matrix, prices, initial_capital=INITIAL_CAPITAL, window_size=WINDOW_SIZE):\n",
    "        self.feature_matrix = feature_matrix\n",
    "        self.prices = prices\n",
    "        self.initial_capital = initial_capital\n",
    "        self.window_size = window_size\n",
    "        self.reset()\n",
    "        \n",
    "        # Estat√≠sticas para normaliza√ß√£o\n",
    "        self.price_mean = np.mean(prices)\n",
    "        self.price_std = np.std(prices)\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia ambiente para novo epis√≥dio\"\"\"\n",
    "        self.current_step = self.window_size\n",
    "        self.cash = self.initial_capital\n",
    "        self.shares = 0\n",
    "        self.portfolio_history = []\n",
    "        self.action_history = []\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"\n",
    "        Cria estado avan√ßado com janela de features\n",
    "        \"\"\"\n",
    "        if self.current_step < self.window_size:\n",
    "            # Padding para in√≠cio\n",
    "            padding = np.zeros((self.window_size - self.current_step - 1, self.feature_matrix.shape[1]))\n",
    "            window_data = np.vstack([padding, self.feature_matrix[:self.current_step + 1]])\n",
    "        else:\n",
    "            window_data = self.feature_matrix[self.current_step - self.window_size + 1:self.current_step + 1]\n",
    "        \n",
    "        # Normalizar pre√ßos na janela pelo pre√ßo atual\n",
    "        current_price = self.prices[self.current_step]\n",
    "        price_normalized = window_data[:, 0] / current_price  # Close price √© primeira coluna\n",
    "        \n",
    "        # Combinar pre√ßos normalizados com outras features\n",
    "        other_features = window_data[:, 1:].flatten()  # Outras features\n",
    "        \n",
    "        # Estado final: pre√ßos + features + posi√ß√£o atual\n",
    "        portfolio_value = self.cash + self.shares * current_price\n",
    "        position_info = np.array([\n",
    "            self.cash / self.initial_capital,  # Cash ratio\n",
    "            self.shares * current_price / self.initial_capital,  # Position ratio\n",
    "            portfolio_value / self.initial_capital,  # Total value ratio\n",
    "        ])\n",
    "        \n",
    "        # Combinar tudo\n",
    "        state = np.concatenate([\n",
    "            price_normalized,\n",
    "            other_features[:STATE_SIZE-self.window_size-3],  # Limitar tamanho\n",
    "            position_info\n",
    "        ])\n",
    "        \n",
    "        # Garantir tamanho fixo\n",
    "        if len(state) > STATE_SIZE:\n",
    "            state = state[:STATE_SIZE]\n",
    "        elif len(state) < STATE_SIZE:\n",
    "            padding = np.zeros(STATE_SIZE - len(state))\n",
    "            state = np.concatenate([state, padding])\n",
    "        \n",
    "        return state.astype(np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Executa a√ß√£o e retorna (state, reward, done, info)\"\"\"\n",
    "        current_price = self.prices[self.current_step]\n",
    "        portfolio_before = self.cash + self.shares * current_price\n",
    "        \n",
    "        # Executar a√ß√£o\n",
    "        action_executed = False\n",
    "        transaction_cost = 0\n",
    "        \n",
    "        if action == IronManActions.BUY and self.cash >= current_price:\n",
    "            shares_to_buy = int(self.cash // current_price)  # Comprar m√°ximo poss√≠vel\n",
    "            if shares_to_buy > 0:\n",
    "                self.shares += shares_to_buy\n",
    "                cost = shares_to_buy * current_price\n",
    "                transaction_cost = cost * 0.001  # 0.1% de taxa\n",
    "                self.cash -= (cost + transaction_cost)\n",
    "                action_executed = True\n",
    "                \n",
    "        elif action == IronManActions.SELL and self.shares > 0:\n",
    "            shares_to_sell = self.shares\n",
    "            revenue = shares_to_sell * current_price\n",
    "            transaction_cost = revenue * 0.001  # 0.1% de taxa\n",
    "            self.cash += (revenue - transaction_cost)\n",
    "            self.shares = 0\n",
    "            action_executed = True\n",
    "        \n",
    "        # HOLD sempre √© v√°lido\n",
    "        if action == IronManActions.HOLD:\n",
    "            action_executed = True\n",
    "        \n",
    "        # Calcular recompensa avan√ßada\n",
    "        portfolio_after = self.cash + self.shares * current_price\n",
    "        \n",
    "        # Recompensa base: mudan√ßa no portf√≥lio\n",
    "        portfolio_return = (portfolio_after - portfolio_before) / portfolio_before if portfolio_before > 0 else 0\n",
    "        \n",
    "        # Recompensa por performance vs mercado\n",
    "        if self.current_step > 0:\n",
    "            market_return = (current_price - self.prices[self.current_step - 1]) / self.prices[self.current_step - 1]\n",
    "            alpha_reward = (portfolio_return - market_return) * 100  # Alpha reward\n",
    "        else:\n",
    "            alpha_reward = 0\n",
    "        \n",
    "        # Penalidade por transa√ß√£o\n",
    "        transaction_penalty = transaction_cost\n",
    "        \n",
    "        # Recompensa final\n",
    "        reward = portfolio_return * 1000 + alpha_reward - transaction_penalty\n",
    "        \n",
    "        # Registrar hist√≥rico\n",
    "        self.portfolio_history.append(portfolio_after)\n",
    "        self.action_history.append(action)\n",
    "        \n",
    "        # Pr√≥ximo step\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.prices) - 1\n",
    "        \n",
    "        next_state = self._get_state() if not done else None\n",
    "        \n",
    "        info = {\n",
    "            'portfolio_value': portfolio_after,\n",
    "            'cash': self.cash,\n",
    "            'shares': self.shares,\n",
    "            'current_price': current_price,\n",
    "            'action_executed': action_executed,\n",
    "            'transaction_cost': transaction_cost,\n",
    "            'alpha_reward': alpha_reward\n",
    "        }\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def get_performance_metrics(self):\n",
    "        \"\"\"Calcula m√©tricas de performance\"\"\"\n",
    "        if not self.portfolio_history:\n",
    "            return None\n",
    "            \n",
    "        returns = np.diff(self.portfolio_history) / self.portfolio_history[:-1]\n",
    "        \n",
    "        total_return = (self.portfolio_history[-1] - self.initial_capital) / self.initial_capital\n",
    "        \n",
    "        # Sharpe ratio\n",
    "        if len(returns) > 1 and np.std(returns) > 0:\n",
    "            sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)  # Anualizado\n",
    "        else:\n",
    "            sharpe = 0\n",
    "        \n",
    "        # Maximum drawdown\n",
    "        peak = np.maximum.accumulate(self.portfolio_history)\n",
    "        drawdown = (self.portfolio_history - peak) / peak\n",
    "        max_drawdown = np.min(drawdown)\n",
    "        \n",
    "        return {\n",
    "            'total_return': total_return,\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'final_value': self.portfolio_history[-1],\n",
    "            'num_trades': len([a for a in self.action_history if a != IronManActions.HOLD])\n",
    "        }\n",
    "\n",
    "# Inicializar ambiente Iron Man\n",
    "if df_ironman is not None:\n",
    "    ironman_env = IronManTradingEnvironment(\n",
    "        feature_matrix, \n",
    "        prices_ironman, \n",
    "        INITIAL_CAPITAL, \n",
    "        WINDOW_SIZE\n",
    "    )\n",
    "    \n",
    "    print(\"üèóÔ∏è Ambiente Iron Man inicializado!\")\n",
    "    print(f\"   üéØ Estado size: {STATE_SIZE}\")\n",
    "    print(f\"   üìä Features: {feature_matrix.shape[1]}\")\n",
    "    print(f\"   üí∞ Capital: R$ {INITIAL_CAPITAL:,.2f}\")\n",
    "    print(f\"   üìà Dados: {len(prices_ironman)} dias\")\n",
    "    \n",
    "    # Testar estado\n",
    "    test_state = ironman_env.reset()\n",
    "    print(f\"   üß™ Estado teste: shape {test_state.shape}, range [{test_state.min():.3f}, {test_state.max():.3f}]\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Ambiente Iron Man n√£o pode ser inicializado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da925dbb",
   "metadata": {},
   "source": [
    "## ü§ñ AGENTE DQN IRON MAN\n",
    "\n",
    "Agente avan√ßado com Deep Q-Learning, Experience Replay e Target Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce375dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ Agente DQN Iron Man (Estado da Arte)\n",
    "class IronManDQNAgent:\n",
    "    \"\"\"\n",
    "    Agente Deep Q-Learning avan√ßado com todas as t√©cnicas modernas\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size=STATE_SIZE, action_size=3, lr=LEARNING_RATE):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Par√¢metros de explora√ß√£o\n",
    "        self.epsilon = EPSILON_START\n",
    "        self.epsilon_min = EPSILON_MIN\n",
    "        self.epsilon_decay = EPSILON_DECAY\n",
    "        \n",
    "        # Redes neurais\n",
    "        self.main_net = IronManDQN(state_size, HIDDEN_SIZE, action_size).to(device)\n",
    "        self.target_net = IronManDQN(state_size, HIDDEN_SIZE, action_size).to(device)\n",
    "        self.target_net.load_state_dict(self.main_net.state_dict())\n",
    "        \n",
    "        # Otimizador\n",
    "        self.optimizer = optim.Adam(self.main_net.parameters(), lr=lr)\n",
    "        \n",
    "        # Experience replay\n",
    "        self.memory = IronManReplayBuffer(MEMORY_SIZE)\n",
    "        \n",
    "        # Estat√≠sticas\n",
    "        self.losses = []\n",
    "        self.episode_rewards = []\n",
    "        self.episode_returns = []\n",
    "        self.q_values_history = []\n",
    "        \n",
    "        print(f\"ü§ñ Agente Iron Man DQN inicializado!\")\n",
    "        print(f\"   üß† Arquitetura: {state_size} ‚Üí {HIDDEN_SIZE} ‚Üí {action_size}\")\n",
    "        print(f\"   ‚ö° Device: {device}\")\n",
    "        print(f\"   üéØ Epsilon: {self.epsilon}\")\n",
    "        \n",
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"\n",
    "        Seleciona a√ß√£o usando Œµ-greedy com rede neural\n",
    "        \"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            # Explora√ß√£o\n",
    "            return random.choice(range(self.action_size)), True\n",
    "        \n",
    "        # Exploita√ß√£o usando rede neural\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = self.main_net(state_tensor)\n",
    "            action = q_values.argmax().item()\n",
    "            \n",
    "        # Registrar Q-values para an√°lise\n",
    "        if training:\n",
    "            self.q_values_history.append(q_values.cpu().numpy().flatten())\n",
    "            \n",
    "        return action, False\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Armazena experi√™ncia no buffer\"\"\"\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"\n",
    "        Treina a rede usando batch de experi√™ncias (Experience Replay)\n",
    "        \"\"\"\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return None\n",
    "            \n",
    "        # Sample batch\n",
    "        experiences = self.memory.sample(BATCH_SIZE)\n",
    "        \n",
    "        states = torch.FloatTensor([e.state for e in experiences]).to(device)\n",
    "        actions = torch.LongTensor([e.action for e in experiences]).to(device)\n",
    "        rewards = torch.FloatTensor([e.reward for e in experiences]).to(device)\n",
    "        next_states = torch.FloatTensor([e.next_state if not e.done else np.zeros(self.state_size) for e in experiences]).to(device)\n",
    "        dones = torch.BoolTensor([e.done for e in experiences]).to(device)\n",
    "        \n",
    "        # Q-values atuais\n",
    "        current_q_values = self.main_net(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Q-values do pr√≥ximo estado (usando target network)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (GAMMA * next_q_values * ~dones)\n",
    "        \n",
    "        # Loss (Huber loss para estabilidade)\n",
    "        loss = F.smooth_l1_loss(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping para estabilidade\n",
    "        torch.nn.utils.clip_grad_norm_(self.main_net.parameters(), 1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "        self.losses.append(loss.item())\n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Atualiza target network\"\"\"\n",
    "        self.target_net.load_state_dict(self.main_net.state_dict())\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        \"\"\"Treina um epis√≥dio completo\"\"\"\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_loss = 0\n",
    "        loss_count = 0\n",
    "        \n",
    "        while True:\n",
    "            # Selecionar a√ß√£o\n",
    "            action, is_exploration = self.get_action(state, training=True)\n",
    "            \n",
    "            # Executar a√ß√£o\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Armazenar experi√™ncia\n",
    "            self.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Treinar se temos experi√™ncias suficientes\n",
    "            if len(self.memory) >= BATCH_SIZE:\n",
    "                loss = self.replay()\n",
    "                if loss is not None:\n",
    "                    episode_loss += loss\n",
    "                    loss_count += 1\n",
    "            \n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "        \n",
    "        # Calcular m√©tricas do epis√≥dio\n",
    "        performance = env.get_performance_metrics()\n",
    "        \n",
    "        # Registrar estat√≠sticas\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        if performance:\n",
    "            self.episode_returns.append(performance['total_return'])\n",
    "        \n",
    "        return {\n",
    "            'episode_reward': episode_reward,\n",
    "            'episode_loss': episode_loss / max(1, loss_count),\n",
    "            'epsilon': self.epsilon,\n",
    "            'performance': performance,\n",
    "            'memory_size': len(self.memory)\n",
    "        }\n",
    "    \n",
    "    def get_training_stats(self, window=100):\n",
    "        \"\"\"Retorna estat√≠sticas de treinamento\"\"\"\n",
    "        if not self.episode_rewards:\n",
    "            return None\n",
    "            \n",
    "        recent_rewards = self.episode_rewards[-window:]\n",
    "        recent_returns = self.episode_returns[-window:] if self.episode_returns else [0]\n",
    "        recent_losses = self.losses[-window:] if self.losses else [0]\n",
    "        \n",
    "        return {\n",
    "            'episodes': len(self.episode_rewards),\n",
    "            'avg_reward': np.mean(recent_rewards),\n",
    "            'avg_return': np.mean(recent_returns),\n",
    "            'avg_loss': np.mean(recent_losses),\n",
    "            'epsilon': self.epsilon,\n",
    "            'memory_size': len(self.memory)\n",
    "        }\n",
    "\n",
    "# Inicializar agente Iron Man\n",
    "if df_ironman is not None:\n",
    "    ironman_agent = IronManDQNAgent(STATE_SIZE, len(IronManActions.get_actions()), LEARNING_RATE)\n",
    "    \n",
    "    print(\"‚úÖ Agente Iron Man DQN pronto!\")\n",
    "    print(f\"   üß† Par√¢metros: {sum(p.numel() for p in ironman_agent.main_net.parameters()):,}\")\n",
    "    print(f\"   üíæ Memory capacity: {MEMORY_SIZE:,}\")\n",
    "    print(f\"   üéØ Target update: cada {TARGET_UPDATE} epis√≥dios\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Agente Iron Man n√£o pode ser inicializado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569ee81a",
   "metadata": {},
   "source": [
    "## üöÄ TREINAMENTO IRON MAN\n",
    "\n",
    "Treinamento avan√ßado com monitoramento em tempo real e otimiza√ß√µes modernas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ddb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Treinamento Avan√ßado Iron Man\n",
    "def train_ironman_agent(agent, env, num_episodes=NUM_EPISODES, print_every=200):\n",
    "    \"\"\"\n",
    "    Treinamento avan√ßado com todas as t√©cnicas modernas\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Iniciando treinamento Iron Man DQN - {num_episodes} epis√≥dios\")\n",
    "    print(f\"üß† Arquitetura: {agent.state_size} inputs ‚Üí {HIDDEN_SIZE} hidden ‚Üí {agent.action_size} outputs\")\n",
    "    print(f\"üíæ Experience replay: {MEMORY_SIZE:,} experi√™ncias\")\n",
    "    print(f\"üéØ Target network update: cada {TARGET_UPDATE} epis√≥dios\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    training_history = {\n",
    "        'episodes': [],\n",
    "        'avg_reward': [],\n",
    "        'avg_return': [],\n",
    "        'avg_loss': [],\n",
    "        'epsilon': [],\n",
    "        'memory_size': [],\n",
    "        'sharpe_ratio': [],\n",
    "        'max_drawdown': []\n",
    "    }\n",
    "    \n",
    "    best_performance = -float('inf')\n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        # Treinar epis√≥dio\n",
    "        episode_info = agent.train_episode(env)\n",
    "        \n",
    "        # Atualizar target network\n",
    "        if episode % TARGET_UPDATE == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        # Relat√≥rio peri√≥dico\n",
    "        if episode % print_every == 0 or episode <= WARMUP_EPISODES:\n",
    "            stats = agent.get_training_stats()\n",
    "            performance = episode_info.get('performance', {})\n",
    "            \n",
    "            print(f\"üöÄ Epis√≥dio {episode}/{num_episodes}\")\n",
    "            print(f\"   üí∞ Reward m√©dio: {stats['avg_reward']:+.2f}\")\n",
    "            print(f\"   üìä Retorno m√©dio: {stats['avg_return']:+.2%}\")\n",
    "            print(f\"   üß† Loss m√©dio: {stats['avg_loss']:.4f}\")\n",
    "            print(f\"   üîç Epsilon: {stats['epsilon']:.3f}\")\n",
    "            print(f\"   üíæ Memory: {stats['memory_size']:,}/{MEMORY_SIZE:,}\")\n",
    "            \n",
    "            if performance:\n",
    "                print(f\"   üìà √öltimo retorno: {performance['total_return']:+.2%}\")\n",
    "                print(f\"   ‚ö° Sharpe ratio: {performance['sharpe_ratio']:.3f}\")\n",
    "                print(f\"   üìâ Max drawdown: {performance['max_drawdown']:+.2%}\")\n",
    "                print(f\"   üîÑ Trades: {performance['num_trades']}\")\n",
    "                \n",
    "                # Salvar melhor modelo\n",
    "                if performance['total_return'] > best_performance:\n",
    "                    best_performance = performance['total_return']\n",
    "                    print(f\"   üèÜ NOVO RECORD: {best_performance:+.2%}!\")\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Salvar hist√≥rico\n",
    "            training_history['episodes'].append(episode)\n",
    "            training_history['avg_reward'].append(stats['avg_reward'])\n",
    "            training_history['avg_return'].append(stats['avg_return'])\n",
    "            training_history['avg_loss'].append(stats['avg_loss'])\n",
    "            training_history['epsilon'].append(stats['epsilon'])\n",
    "            training_history['memory_size'].append(stats['memory_size'])\n",
    "            \n",
    "            if performance:\n",
    "                training_history['sharpe_ratio'].append(performance['sharpe_ratio'])\n",
    "                training_history['max_drawdown'].append(performance['max_drawdown'])\n",
    "            else:\n",
    "                training_history['sharpe_ratio'].append(0)\n",
    "                training_history['max_drawdown'].append(0)\n",
    "    \n",
    "    print(\"‚úÖ Treinamento Iron Man conclu√≠do!\")\n",
    "    final_stats = agent.get_training_stats()\n",
    "    print(f\"üìä Estat√≠sticas finais:\")\n",
    "    print(f\"   üß† Par√¢metros da rede: {sum(p.numel() for p in agent.main_net.parameters()):,}\")\n",
    "    print(f\"   üíæ Experi√™ncias coletadas: {final_stats['memory_size']:,}\")\n",
    "    print(f\"   üéØ Epsilon final: {final_stats['epsilon']:.4f}\")\n",
    "    print(f\"   üí∞ Reward m√©dio final: {final_stats['avg_reward']:+.2f}\")\n",
    "    print(f\"   üìà Retorno m√©dio final: {final_stats['avg_return']:+.2%}\")\n",
    "    print(f\"   üèÜ Melhor performance: {best_performance:+.2%}\")\n",
    "    \n",
    "    return training_history\n",
    "\n",
    "# Executar treinamento Iron Man\n",
    "if df_ironman is not None and 'ironman_agent' in locals():\n",
    "    print(f\"üöÄ Iniciando treinamento Iron Man para {TICKER_SYMBOL}\")\n",
    "    print(f\"üßÆ Usando {device}\")\n",
    "    \n",
    "    # Treinamento principal\n",
    "    ironman_training_history = train_ironman_agent(\n",
    "        ironman_agent, \n",
    "        ironman_env, \n",
    "        NUM_EPISODES, \n",
    "        print_every=250\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Componentes Iron Man n√£o dispon√≠veis para treinamento!\")\n",
    "    print(\"üí° Verifique se PyTorch est√° instalado: pip install torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b03739",
   "metadata": {},
   "source": [
    "## üìä AVALIA√á√ÉO E AN√ÅLISE IRON MAN\n",
    "\n",
    "Avalia√ß√£o completa com m√©tricas avan√ßadas e visualiza√ß√µes modernas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07459950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Avalia√ß√£o Completa Iron Man\n",
    "def evaluate_ironman_agent(agent, env, num_episodes=50):\n",
    "    \"\"\"\n",
    "    Avalia√ß√£o avan√ßada com m√©tricas de trading profissionais\n",
    "    \"\"\"\n",
    "    print(\"üß™ Avaliando agente Iron Man DQN...\")\n",
    "    \n",
    "    test_results = []\n",
    "    portfolio_curves = []\n",
    "    \n",
    "    # Desativar explora√ß√£o para teste\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_portfolio = [env.initial_capital]\n",
    "        \n",
    "        while True:\n",
    "            action, _ = agent.get_action(state, training=False)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            episode_portfolio.append(info['portfolio_value'])\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        \n",
    "        performance = env.get_performance_metrics()\n",
    "        if performance:\n",
    "            test_results.append(performance)\n",
    "            portfolio_curves.append(episode_portfolio)\n",
    "    \n",
    "    # Restaurar epsilon\n",
    "    agent.epsilon = original_epsilon\n",
    "    \n",
    "    # An√°lise dos resultados\n",
    "    if test_results:\n",
    "        returns = [r['total_return'] for r in test_results]\n",
    "        sharpes = [r['sharpe_ratio'] for r in test_results]\n",
    "        drawdowns = [r['max_drawdown'] for r in test_results]\n",
    "        \n",
    "        avg_return = np.mean(returns)\n",
    "        avg_sharpe = np.mean(sharpes)\n",
    "        avg_drawdown = np.mean(drawdowns)\n",
    "        win_rate = len([r for r in returns if r > 0]) / len(returns)\n",
    "        volatility = np.std(returns)\n",
    "        \n",
    "        # Buy & Hold comparison\n",
    "        buy_hold_return = (prices_ironman[-1] - prices_ironman[env.window_size]) / prices_ironman[env.window_size]\n",
    "        \n",
    "        print(f\"üìà Resultados Iron Man DQN ({num_episodes} epis√≥dios):\")\n",
    "        print(f\"   üí∞ Retorno m√©dio: {avg_return:+.2%}\")\n",
    "        print(f\"   ‚ö° Sharpe ratio: {avg_sharpe:.3f}\")\n",
    "        print(f\"   üìâ Max drawdown: {avg_drawdown:+.2%}\")\n",
    "        print(f\"   üéØ Taxa de sucesso: {win_rate:.1%}\")\n",
    "        print(f\"   üìä Volatilidade: {volatility:.2%}\")\n",
    "        print(f\"   üìà Buy & Hold: {buy_hold_return:+.2%}\")\n",
    "        print(f\"   üèÜ Alpha vs B&H: {avg_return - buy_hold_return:+.2%}\")\n",
    "        \n",
    "        # Information Ratio\n",
    "        if volatility > 0:\n",
    "            info_ratio = (avg_return - buy_hold_return) / volatility\n",
    "            print(f\"   üìä Information Ratio: {info_ratio:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'avg_return': avg_return,\n",
    "            'avg_sharpe': avg_sharpe,\n",
    "            'avg_drawdown': avg_drawdown,\n",
    "            'win_rate': win_rate,\n",
    "            'volatility': volatility,\n",
    "            'buy_hold_return': buy_hold_return,\n",
    "            'alpha': avg_return - buy_hold_return,\n",
    "            'test_results': test_results,\n",
    "            'portfolio_curves': portfolio_curves\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "def plot_ironman_results(training_history, evaluation_results):\n",
    "    \"\"\"\n",
    "    Visualiza√ß√µes avan√ßadas dos resultados Iron Man\n",
    "    \"\"\"\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'ü§ñ Iron Man DQN Results - {TICKER_SYMBOL}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Learning Curve (Retorno)\n",
    "    episodes = training_history['episodes']\n",
    "    axes[0,0].plot(episodes, training_history['avg_return'], 'b-', linewidth=2, label='DQN Return')\n",
    "    axes[0,0].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "    if evaluation_results:\n",
    "        axes[0,0].axhline(y=evaluation_results['buy_hold_return'], color='orange', linestyle='--', label='Buy & Hold')\n",
    "    axes[0,0].set_title('Evolu√ß√£o do Retorno (Treinamento)')\n",
    "    axes[0,0].set_xlabel('Epis√≥dio')\n",
    "    axes[0,0].set_ylabel('Retorno M√©dio')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Loss Function\n",
    "    axes[0,1].plot(episodes, training_history['avg_loss'], 'r-', linewidth=2)\n",
    "    axes[0,1].set_title('Loss Function (DQN)')\n",
    "    axes[0,1].set_xlabel('Epis√≥dio')\n",
    "    axes[0,1].set_ylabel('Loss M√©dio')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Epsilon Decay\n",
    "    axes[1,0].plot(episodes, training_history['epsilon'], 'g-', linewidth=2)\n",
    "    axes[1,0].set_title('Exploration Decay (Epsilon)')\n",
    "    axes[1,0].set_xlabel('Epis√≥dio')\n",
    "    axes[1,0].set_ylabel('Epsilon')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Sharpe Ratio Evolution\n",
    "    axes[1,1].plot(episodes, training_history['sharpe_ratio'], 'purple', linewidth=2)\n",
    "    axes[1,1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "    axes[1,1].set_title('Sharpe Ratio Evolution')\n",
    "    axes[1,1].set_xlabel('Epis√≥dio')\n",
    "    axes[1,1].set_ylabel('Sharpe Ratio')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Memory Usage\n",
    "    axes[2,0].plot(episodes, training_history['memory_size'], 'brown', linewidth=2)\n",
    "    axes[2,0].axhline(y=MEMORY_SIZE, color='r', linestyle='--', alpha=0.5, label='Max Capacity')\n",
    "    axes[2,0].set_title('Experience Replay Buffer')\n",
    "    axes[2,0].set_xlabel('Epis√≥dio')\n",
    "    axes[2,0].set_ylabel('Experi√™ncias Armazenadas')\n",
    "    axes[2,0].legend()\n",
    "    axes[2,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Performance Comparison\n",
    "    if evaluation_results:\n",
    "        methods = ['Iron Man DQN', 'Buy & Hold']\n",
    "        returns = [evaluation_results['avg_return'], evaluation_results['buy_hold_return']]\n",
    "        colors = ['red', 'orange']\n",
    "        \n",
    "        bars = axes[2,1].bar(methods, returns, color=colors, alpha=0.7)\n",
    "        axes[2,1].set_title('Performance Comparison')\n",
    "        axes[2,1].set_ylabel('Retorno')\n",
    "        axes[2,1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        for bar, return_val in zip(bars, returns):\n",
    "            height = bar.get_height()\n",
    "            axes[2,1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                          f'{return_val:+.2%}',\n",
    "                          ha='center', va='bottom' if height > 0 else 'top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Portfolio curves (se dispon√≠vel)\n",
    "    if evaluation_results and evaluation_results['portfolio_curves']:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plotar algumas curvas de portf√≥lio\n",
    "        curves_to_plot = min(10, len(evaluation_results['portfolio_curves']))\n",
    "        for i in range(curves_to_plot):\n",
    "            curve = evaluation_results['portfolio_curves'][i]\n",
    "            plt.plot(curve, alpha=0.3, color='blue')\n",
    "        \n",
    "        # M√©dia das curvas\n",
    "        avg_curve = np.mean(evaluation_results['portfolio_curves'], axis=0)\n",
    "        plt.plot(avg_curve, color='red', linewidth=3, label='M√©dia Iron Man')\n",
    "        \n",
    "        # Buy & Hold\n",
    "        buy_hold_curve = [INITIAL_CAPITAL * (1 + evaluation_results['buy_hold_return'] * i / len(avg_curve)) \n",
    "                         for i in range(len(avg_curve))]\n",
    "        plt.plot(buy_hold_curve, color='orange', linewidth=2, linestyle='--', label='Buy & Hold')\n",
    "        \n",
    "        plt.axhline(y=INITIAL_CAPITAL, color='black', linestyle=':', alpha=0.5, label='Capital Inicial')\n",
    "        plt.title('Evolu√ß√£o do Portf√≥lio - Iron Man DQN')\n",
    "        plt.xlabel('Dias de Trading')\n",
    "        plt.ylabel('Valor do Portf√≥lio (R$)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "# Executar avalia√ß√£o Iron Man\n",
    "if df_ironman is not None and 'ironman_training_history' in locals():\n",
    "    print(\"üìä Executando avalia√ß√£o completa Iron Man...\")\n",
    "    ironman_evaluation = evaluate_ironman_agent(ironman_agent, ironman_env, num_episodes=30)\n",
    "    plot_ironman_results(ironman_training_history, ironman_evaluation)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Execute primeiro o treinamento para avaliar o agente Iron Man!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ad03d",
   "metadata": {},
   "source": [
    "## üîß FLEXIBILIDADE IRON MAN\n",
    "\n",
    "Sistema completamente flex√≠vel - use com qualquer ativo alterando apenas `TICKER_SYMBOL`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff17290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Sistema Iron Man - Flexibilidade Total\n",
    "\"\"\"\n",
    "ü§ñ IRON MAN DQN - GUIA DE USO AVAN√áADO\n",
    "\n",
    "Para testar com outros ativos:\n",
    "\n",
    "1Ô∏è‚É£ Altere TICKER_SYMBOL na segunda c√©lula:\n",
    "   TICKER_SYMBOL = \"VALE3.SA\"  # ou BRFS3.SA, ITUB4.SA, etc.\n",
    "\n",
    "2Ô∏è‚É£ Re-execute todas as c√©lulas do notebook\n",
    "\n",
    "3Ô∏è‚É£ O sistema Iron Man automaticamente:\n",
    "   ‚úÖ Baixa dados hist√≥ricos do novo ativo\n",
    "   ‚úÖ Calcula features avan√ßadas (RSI, MACD, Bollinger, etc.)\n",
    "   ‚úÖ Reconstr√≥i a rede neural DQN\n",
    "   ‚úÖ Treina o agente com Deep Q-Learning\n",
    "   ‚úÖ Avalia performance com m√©tricas profissionais\n",
    "   ‚úÖ Gera visualiza√ß√µes avan√ßadas\n",
    "\n",
    "üéØ ATIVOS RECOMENDADOS PARA TESTE:\n",
    "- PETR3.SA, PETR4.SA (Petrobras) - Alta liquidez\n",
    "- VALE3.SA (Vale) - Commodities\n",
    "- BRFS3.SA (BRF) - Consumo\n",
    "- ITUB4.SA (Ita√∫) - Financeiro\n",
    "- ABEV3.SA (Ambev) - Bebidas\n",
    "- WEGE3.SA (WEG) - Industrial\n",
    "\n",
    "‚ö° DIFEREN√áAS vs BATMAN:\n",
    "‚úÖ Estados cont√≠nuos (vs discretos)\n",
    "‚úÖ Redes neurais (vs tabela Q)\n",
    "‚úÖ Experience replay (vs aprendizado direto)\n",
    "‚úÖ Target networks (vs rede √∫nica)\n",
    "‚úÖ Features avan√ßadas (vs pre√ßos simples)\n",
    "‚úÖ M√©tricas profissionais (vs b√°sicas)\n",
    "\n",
    "üí° REQUISITOS:\n",
    "- PyTorch instalado: pip install torch\n",
    "- Mais RAM para redes neurais\n",
    "- GPU opcional (acelera treinamento)\n",
    "\n",
    "üîß OTIMIZA√á√ïES DISPON√çVEIS:\n",
    "- Ajustar HIDDEN_SIZE para complexidade\n",
    "- Modificar BATCH_SIZE para velocidade\n",
    "- Alterar LEARNING_RATE para converg√™ncia\n",
    "- Aumentar MEMORY_SIZE para mais experi√™ncia\n",
    "\"\"\"\n",
    "\n",
    "def save_ironman_model(agent, filepath=\"ironman_model.pth\"):\n",
    "    \"\"\"Salva o modelo treinado\"\"\"\n",
    "    torch.save({\n",
    "        'main_net_state_dict': agent.main_net.state_dict(),\n",
    "        'target_net_state_dict': agent.target_net.state_dict(),\n",
    "        'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "        'epsilon': agent.epsilon,\n",
    "        'training_stats': {\n",
    "            'episode_rewards': agent.episode_rewards,\n",
    "            'episode_returns': agent.episode_returns,\n",
    "            'losses': agent.losses\n",
    "        }\n",
    "    }, filepath)\n",
    "    print(f\"üíæ Modelo Iron Man salvo em: {filepath}\")\n",
    "\n",
    "def load_ironman_model(agent, filepath=\"ironman_model.pth\"):\n",
    "    \"\"\"Carrega modelo salvo\"\"\"\n",
    "    try:\n",
    "        checkpoint = torch.load(filepath, map_location=device)\n",
    "        agent.main_net.load_state_dict(checkpoint['main_net_state_dict'])\n",
    "        agent.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "        agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        agent.epsilon = checkpoint['epsilon']\n",
    "        print(f\"üì• Modelo Iron Man carregado de: {filepath}\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è Arquivo n√£o encontrado: {filepath}\")\n",
    "        return False\n",
    "\n",
    "# Resumo final Iron Man\n",
    "print(\"ü§ñ SISTEMA IRON MAN DQN COMPLETO!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Deep Q-Network com PyTorch\")\n",
    "print(\"‚úÖ Estados cont√≠nuos avan√ßados\")\n",
    "print(\"‚úÖ Experience Replay Buffer\")  \n",
    "print(\"‚úÖ Target Network para estabilidade\")\n",
    "print(\"‚úÖ Features t√©cnicas profissionais\")\n",
    "print(\"‚úÖ M√©tricas de trading avan√ßadas\")\n",
    "print(\"‚úÖ Visualiza√ß√µes state-of-the-art\")\n",
    "print(\"‚úÖ Sistema flex√≠vel para qualquer ativo\")\n",
    "print(\"‚úÖ Otimiza√ß√µes modernas (batch norm, dropout)\")\n",
    "print(\"‚úÖ Saving/Loading de modelos\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üéØ Configurado para: {TICKER_SYMBOL}\")\n",
    "print(f\"üß† Arquitetura: {STATE_SIZE} ‚Üí {HIDDEN_SIZE} ‚Üí 3\")\n",
    "print(f\"‚ö° Device: {device}\")\n",
    "\n",
    "if 'ironman_agent' in locals():\n",
    "    print(f\"ü§ñ Status: Agente treinado e pronto!\")\n",
    "    print(\"üí° Para salvar modelo: save_ironman_model(ironman_agent)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Status: Execute as c√©lulas para treinar\")\n",
    "\n",
    "print(\"\\nüöÄ Iron Man tech at your service!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5832a97",
   "metadata": {},
   "source": [
    "# ü§ñ IRON MAN APPROACH - Reinforcement Learning Trading\n",
    "\n",
    "## Estrat√©gia: Inovadora e Tecnol√≥gica\n",
    "\n",
    "### Filosofia Iron Man\n",
    "- **Tecnologia de ponta**: Deep Q-Networks (DQN) com redes neurais\n",
    "- **Abordagem moderna**: Estados cont√≠nuos e approxima√ß√£o de fun√ß√£o\n",
    "- **Inova√ß√£o constante**: Experience replay, target networks, double DQN\n",
    "- **Performance focada**: Otimizado para resultados reais de trading\n",
    "- **Escalabilidade**: Arquitetura preparada para m√∫ltiplos ativos\n",
    "\n",
    "### Objetivo\n",
    "Desenvolver um agente de Reinforcement Learning avan√ßado usando **Deep Q-Learning**.\n",
    "O sistema deve ser **state-of-the-art** e funcionar com qualquer ativo (PETR3, VALE3, BRFS3, etc.).\n",
    "\n",
    "### Caracter√≠sticas da Implementa√ß√£o\n",
    "- ‚úÖ Deep Q-Network (DQN) com PyTorch/TensorFlow\n",
    "- ‚úÖ Estados cont√≠nuos (pre√ßos normalizados)\n",
    "- ‚úÖ Experience Replay Buffer\n",
    "- ‚úÖ Target Network para estabilidade\n",
    "- ‚úÖ Indicadores t√©cnicos como features\n",
    "- ‚úÖ Arquitetura moderna e escal√°vel\n",
    "- ‚úÖ Performance otimizada"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
