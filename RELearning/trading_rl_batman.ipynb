{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13489014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Bibliotecas importadas com sucesso!\n",
      "ğŸ¦‡ Batman estÃ¡ preparado para o trading sistemÃ¡tico!\n"
     ]
    }
   ],
   "source": [
    "# ImportaÃ§Ãµes necessÃ¡rias\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ“š Bibliotecas importadas com sucesso!\")\n",
    "print(\"ğŸ¦‡ Batman estÃ¡ preparado para o trading sistemÃ¡tico!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77e2ba",
   "metadata": {},
   "source": [
    "## ğŸ¯ CONFIGURAÃ‡ÃƒO FLEXÃVEL DO SISTEMA\n",
    "\n",
    "Sistema genÃ©rico que aceita qualquer ativo da B3. Basta alterar o `TICKER_SYMBOL` para usar com VALE3, BRFS3, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "928ee4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Configurado para: PETR3.SA\n",
      "ğŸ’° Capital inicial: R$ 50,000.00\n",
      "ğŸ“Š PerÃ­odo: 20y\n",
      "ğŸ§  Episodes: 1000\n"
     ]
    }
   ],
   "source": [
    "# ğŸ® CONFIGURAÃ‡ÃƒO PRINCIPAL - ALTERE AQUI PARA USAR OUTRO ATIVO\n",
    "TICKER_SYMBOL = \"PETR3.SA\"  # Pode ser: PETR3.SA, VALE3.SA, BRFS3.SA, etc.\n",
    "PERIOD = \"20y\"              # PerÃ­odo dos dados: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max\n",
    "INITIAL_CAPITAL = 50000.0   # Capital inicial em R$\n",
    "\n",
    "# ParÃ¢metros do Q-Learning (Abordagem Batman - Conservadora)\n",
    "LEARNING_RATE = 0.1        # Taxa de aprendizado\n",
    "DISCOUNT_FACTOR = 0.95     # Fator de desconto (gamma)\n",
    "EPSILON_START = 1.0        # ExploraÃ§Ã£o inicial\n",
    "EPSILON_MIN = 0.01         # ExploraÃ§Ã£o mÃ­nima  \n",
    "EPSILON_DECAY = 0.995      # Decaimento da exploraÃ§Ã£o\n",
    "NUM_EPISODES = 1000        # NÃºmero de episÃ³dios de treinamento\n",
    "\n",
    "# Estados discretos (Batman usa faixas simples)\n",
    "NUM_PRICE_BINS = 5       # NÃºmero de faixas de preÃ§o\n",
    "WINDOW_SIZE = 30            # Janela histÃ³rica\n",
    "\n",
    "print(f\"ğŸ¯ Configurado para: {TICKER_SYMBOL}\")\n",
    "print(f\"ğŸ’° Capital inicial: R$ {INITIAL_CAPITAL:,.2f}\")\n",
    "print(f\"ğŸ“Š PerÃ­odo: {PERIOD}\")\n",
    "print(f\"ğŸ§  Episodes: {NUM_EPISODES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a33f9",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ COLETA DE DADOS GENÃ‰RICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6479a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¡ Baixando dados de PETR3.SA...\n",
      "âœ… Dados carregados com sucesso!\n",
      "   ğŸ“Š Empresa: PetrÃ³leo Brasileiro S.A. - Petrobras\n",
      "   ğŸ¢ Setor: Energy\n",
      "   ğŸ“… PerÃ­odo: 2005-10-31 atÃ© 2025-10-31\n",
      "   ğŸ“ˆ Total de dias: 4977\n",
      "   ğŸ’° PreÃ§o atual: R$ 31.51\n",
      "âœ… Dados carregados com sucesso!\n",
      "   ğŸ“Š Empresa: PetrÃ³leo Brasileiro S.A. - Petrobras\n",
      "   ğŸ¢ Setor: Energy\n",
      "   ğŸ“… PerÃ­odo: 2005-10-31 atÃ© 2025-10-31\n",
      "   ğŸ“ˆ Total de dias: 4977\n",
      "   ğŸ’° PreÃ§o atual: R$ 31.51\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š FunÃ§Ã£o genÃ©rica para carregar dados de qualquer ativo\n",
    "def load_stock_data(ticker_symbol, period=\"1y\"):\n",
    "    \"\"\"\n",
    "    Carrega dados histÃ³ricos de qualquer ativo da B3\n",
    "    \n",
    "    Args:\n",
    "        ticker_symbol (str): SÃ­mbolo do ativo (ex: 'PETR3.SA', 'VALE3.SA')\n",
    "        period (str): PerÃ­odo dos dados\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Dados histÃ³ricos do ativo\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"ğŸ“¡ Baixando dados de {ticker_symbol}...\")\n",
    "        ticker = yf.Ticker(ticker_symbol)\n",
    "        \n",
    "        # Buscar informaÃ§Ãµes da empresa\n",
    "        info = ticker.info\n",
    "        company_name = info.get('longName', 'N/A')\n",
    "        sector = info.get('sector', 'N/A')\n",
    "        \n",
    "        # Buscar dados histÃ³ricos\n",
    "        df = ticker.history(period=period)\n",
    "        \n",
    "        if df.empty:\n",
    "            raise ValueError(f\"Nenhum dado encontrado para {ticker_symbol}\")\n",
    "        \n",
    "        print(f\"âœ… Dados carregados com sucesso!\")\n",
    "        print(f\"   ğŸ“Š Empresa: {company_name}\")\n",
    "        print(f\"   ğŸ¢ Setor: {sector}\")\n",
    "        print(f\"   ğŸ“… PerÃ­odo: {df.index[0].date()} atÃ© {df.index[-1].date()}\")\n",
    "        print(f\"   ğŸ“ˆ Total de dias: {len(df)}\")\n",
    "        print(f\"   ğŸ’° PreÃ§o atual: R$ {df['Close'].iloc[-1]:.2f}\")\n",
    "        \n",
    "        return df, info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro ao carregar dados: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Carregar dados do ativo configurado\n",
    "df_stock, stock_info = load_stock_data(TICKER_SYMBOL, PERIOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74889ba0",
   "metadata": {},
   "source": [
    "## ğŸ§  BATMAN Q-LEARNING SYSTEM\n",
    "\n",
    "### Estados Discretos (Abordagem Conservadora)\n",
    "Batman utiliza uma abordagem metodolÃ³gica com estados discretizados para garantir convergÃªncia estÃ¡vel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba541986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Estados Batman configurados:\n",
      "   ğŸ“Š Faixas de preÃ§o: 5 bins\n",
      "   ğŸ“ˆ Range: R$ 1.74 - R$ 39.56\n",
      "   ğŸ” Janela histÃ³rica: 30 dias\n",
      "   ğŸ§® Total de estados possÃ­veis: 931,322,574,615,478,515,625\n",
      "\n",
      "ğŸ“‹ Resumo dos dados:\n",
      "   ğŸ“Š Total de observaÃ§Ãµes: 4977\n",
      "   ğŸ“ˆ Primeiro preÃ§o: R$ 4.44\n",
      "   ğŸ“‰ Ãšltimo preÃ§o: R$ 31.51\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ Sistema de Estados Discretos (Batman Approach)\n",
    "class BatmanStateManager:\n",
    "    def __init__(self, prices, num_bins=10, window_size=5):\n",
    "        self.prices = prices\n",
    "        self.num_bins = num_bins\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Criar faixas de preÃ§os (discretizaÃ§Ã£o)\n",
    "        self.price_min = prices.min()\n",
    "        self.price_max = prices.max()\n",
    "        self.price_bins = np.linspace(self.price_min, self.price_max, num_bins + 1)\n",
    "        \n",
    "        print(f\"ğŸ¯ Estados Batman configurados:\")\n",
    "        print(f\"   ğŸ“Š Faixas de preÃ§o: {num_bins} bins\")\n",
    "        print(f\"   ğŸ“ˆ Range: R$ {self.price_min:.2f} - R$ {self.price_max:.2f}\")\n",
    "        print(f\"   ğŸ” Janela histÃ³rica: {window_size} dias\")\n",
    "        print(f\"   ğŸ§® Total de estados possÃ­veis: {num_bins ** window_size:,}\")\n",
    "    \n",
    "    def discretize_price(self, price):\n",
    "        \"\"\"Converte preÃ§o contÃ­nuo em faixa discreta\"\"\"\n",
    "        return np.digitize(price, self.price_bins) - 1\n",
    "    \n",
    "    def get_state(self, current_index):\n",
    "        \"\"\"Cria estado discreto baseado em janela histÃ³rica\"\"\"\n",
    "        if current_index < self.window_size:\n",
    "            # Para os primeiros dias, usar o primeiro preÃ§o\n",
    "            window_prices = [self.prices[0]] * (self.window_size - current_index - 1)\n",
    "            window_prices.extend(self.prices[:current_index + 1])\n",
    "        else:\n",
    "            window_prices = self.prices[current_index - self.window_size + 1:current_index + 1]\n",
    "        \n",
    "        # Discretizar cada preÃ§o da janela\n",
    "        discrete_state = tuple([self.discretize_price(price) for price in window_prices])\n",
    "        return discrete_state\n",
    "\n",
    "# AÃ§Ãµes disponÃ­veis (padrÃ£o do Prof. Paulo)\n",
    "class Actions:\n",
    "    HOLD = 0\n",
    "    BUY = 1\n",
    "    SELL = 2\n",
    "    \n",
    "    @classmethod\n",
    "    def get_all_actions(cls):\n",
    "        return [cls.HOLD, cls.BUY, cls.SELL]\n",
    "    \n",
    "    @classmethod \n",
    "    def action_name(cls, action):\n",
    "        names = {cls.HOLD: \"HOLD\", cls.BUY: \"BUY\", cls.SELL: \"SELL\"}\n",
    "        return names.get(action, \"UNKNOWN\")\n",
    "\n",
    "# Preparar dados\n",
    "if df_stock is not None:\n",
    "    prices = df_stock['Close'].values\n",
    "    state_manager = BatmanStateManager(prices, NUM_PRICE_BINS, WINDOW_SIZE)\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Resumo dos dados:\")\n",
    "    print(f\"   ğŸ“Š Total de observaÃ§Ãµes: {len(prices)}\")\n",
    "    print(f\"   ğŸ“ˆ Primeiro preÃ§o: R$ {prices[0]:.2f}\")\n",
    "    print(f\"   ğŸ“‰ Ãšltimo preÃ§o: R$ {prices[-1]:.2f}\")\n",
    "else:\n",
    "    print(\"âŒ Erro: Dados nÃ£o carregados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d35de",
   "metadata": {},
   "source": [
    "## ğŸ›ï¸ AMBIENTE DE TRADING BATMAN\n",
    "\n",
    "Ambiente simples e confiÃ¡vel, seguindo princÃ­pios de metodologia Batman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db684fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›ï¸ Ambiente Batman inicializado com sucesso!\n",
      "   ğŸ’° Capital inicial: R$ 50,000.00\n",
      "   ğŸ“Š Dados disponÃ­veis: 4977 dias\n",
      "   ğŸ¯ InÃ­cio do trading no dia 30\n"
     ]
    }
   ],
   "source": [
    "# ğŸ›ï¸ Ambiente de Trading Batman (EstÃ¡vel e MetodolÃ³gico)\n",
    "class BatmanTradingEnvironment:\n",
    "    def __init__(self, prices, state_manager, initial_capital=10000.0):\n",
    "        self.prices = prices\n",
    "        self.state_manager = state_manager\n",
    "        self.initial_capital = initial_capital\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia o ambiente para novo episÃ³dio\"\"\"\n",
    "        self.current_step = self.state_manager.window_size\n",
    "        self.cash = self.initial_capital\n",
    "        self.shares = 0\n",
    "        self.portfolio_values = []\n",
    "        self.actions_history = []\n",
    "        self.episode_rewards = []\n",
    "        \n",
    "        return self.get_current_state()\n",
    "    \n",
    "    def get_current_state(self):\n",
    "        \"\"\"Retorna estado atual discretizado\"\"\"\n",
    "        return self.state_manager.get_state(self.current_step)\n",
    "    \n",
    "    def get_portfolio_value(self):\n",
    "        \"\"\"Calcula valor total do portfÃ³lio\"\"\"\n",
    "        current_price = self.prices[self.current_step]\n",
    "        return self.cash + (self.shares * current_price)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Executa uma aÃ§Ã£o e retorna (next_state, reward, done, info)\"\"\"\n",
    "        current_price = self.prices[self.current_step]\n",
    "        portfolio_value_before = self.get_portfolio_value()\n",
    "        \n",
    "        # Executar aÃ§Ã£o\n",
    "        action_executed = False\n",
    "        if action == Actions.BUY and self.cash >= current_price:\n",
    "            # Comprar 1 aÃ§Ã£o\n",
    "            self.shares += 1\n",
    "            self.cash -= current_price\n",
    "            action_executed = True\n",
    "            \n",
    "        elif action == Actions.SELL and self.shares > 0:\n",
    "            # Vender 1 aÃ§Ã£o  \n",
    "            self.shares -= 1\n",
    "            self.cash += current_price\n",
    "            action_executed = True\n",
    "            \n",
    "        # HOLD nÃ£o executa nada, mas sempre Ã© vÃ¡lido\n",
    "        if action == Actions.HOLD:\n",
    "            action_executed = True\n",
    "        \n",
    "        # Calcular recompensa (Batman usa mudanÃ§a simples no portfÃ³lio)\n",
    "        portfolio_value_after = self.get_portfolio_value()\n",
    "        reward = portfolio_value_after - portfolio_value_before\n",
    "        \n",
    "        # Registrar histÃ³rico\n",
    "        self.portfolio_values.append(portfolio_value_after)\n",
    "        self.actions_history.append(action)\n",
    "        self.episode_rewards.append(reward)\n",
    "        \n",
    "        # AvanÃ§ar para prÃ³ximo dia\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.prices) - 1\n",
    "        \n",
    "        # PrÃ³ximo estado (ou None se terminado)\n",
    "        next_state = self.get_current_state() if not done else None\n",
    "        \n",
    "        # InformaÃ§Ãµes adicionais\n",
    "        info = {\n",
    "            'cash': self.cash,\n",
    "            'shares': self.shares, \n",
    "            'portfolio_value': portfolio_value_after,\n",
    "            'current_price': current_price,\n",
    "            'action_executed': action_executed,\n",
    "            'day': self.current_step\n",
    "        }\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def get_episode_summary(self):\n",
    "        \"\"\"Retorna resumo do episÃ³dio atual\"\"\"\n",
    "        if not self.portfolio_values:\n",
    "            return None\n",
    "            \n",
    "        total_return = (self.get_portfolio_value() - self.initial_capital) / self.initial_capital\n",
    "        max_value = max(self.portfolio_values)\n",
    "        min_value = min(self.portfolio_values) \n",
    "        \n",
    "        return {\n",
    "            'total_return': total_return,\n",
    "            'final_value': self.get_portfolio_value(),\n",
    "            'max_value': max_value,\n",
    "            'min_value': min_value,\n",
    "            'total_reward': sum(self.episode_rewards),\n",
    "            'num_days': len(self.portfolio_values),\n",
    "            'actions_taken': len([a for a in self.actions_history if a != Actions.HOLD])\n",
    "        }\n",
    "\n",
    "# Inicializar ambiente Batman\n",
    "if df_stock is not None:\n",
    "    env = BatmanTradingEnvironment(prices, state_manager, INITIAL_CAPITAL)\n",
    "    print(\"ğŸ›ï¸ Ambiente Batman inicializado com sucesso!\")\n",
    "    print(f\"   ğŸ’° Capital inicial: R$ {INITIAL_CAPITAL:,.2f}\")\n",
    "    print(f\"   ğŸ“Š Dados disponÃ­veis: {len(prices)} dias\")\n",
    "    print(f\"   ğŸ¯ InÃ­cio do trading no dia {env.current_step}\")\n",
    "else:\n",
    "    print(\"âŒ Erro: Ambiente nÃ£o pode ser inicializado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e69ba",
   "metadata": {},
   "source": [
    "## ğŸ¦‡ AGENTE Q-LEARNING BATMAN\n",
    "\n",
    "ImplementaÃ§Ã£o clÃ¡ssica do Q-Learning seguindo os padrÃµes das aulas do Prof. Paulo Caixeta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b026a624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦‡ Agente Batman Q-Learning inicializado!\n",
      "   ğŸ§  Learning rate: 0.1\n",
      "   ğŸ’° Discount factor: 0.95\n",
      "   ğŸ” Epsilon inicial: 1.0\n",
      "\n",
      "ğŸ¯ Agente pronto para treinamento com 1000 episÃ³dios!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¦‡ Agente Q-Learning Batman (ClÃ¡ssico e ConfiÃ¡vel)\n",
    "class BatmanQLearningAgent:\n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.95, \n",
    "                 epsilon_start=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        # Tabela Q (Batman usa dicionÃ¡rio tradicional)\n",
    "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
    "        self.actions = Actions.get_all_actions()\n",
    "        \n",
    "        # EstatÃ­sticas de treinamento\n",
    "        self.episode_rewards = []\n",
    "        self.episode_returns = []\n",
    "        self.exploration_counts = []\n",
    "        \n",
    "        print(\"ğŸ¦‡ Agente Batman Q-Learning inicializado!\")\n",
    "        print(f\"   ğŸ§  Learning rate: {learning_rate}\")\n",
    "        print(f\"   ğŸ’° Discount factor: {discount_factor}\")\n",
    "        print(f\"   ğŸ” Epsilon inicial: {epsilon_start}\")\n",
    "        \n",
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"Seleciona aÃ§Ã£o usando Îµ-greedy policy\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            # ExploraÃ§Ã£o\n",
    "            action = random.choice(self.actions)\n",
    "            return action, True  # True indica exploraÃ§Ã£o\n",
    "        else:\n",
    "            # ExploitaÃ§Ã£o (escolher melhor aÃ§Ã£o conhecida)\n",
    "            q_values = [self.q_table[state][action] for action in self.actions]\n",
    "            max_q = max(q_values)\n",
    "            \n",
    "            # Se mÃºltiplas aÃ§Ãµes tÃªm mesmo Q-value, escolher aleatoriamente\n",
    "            best_actions = [action for action, q_val in zip(self.actions, q_values) if q_val == max_q]\n",
    "            action = random.choice(best_actions)\n",
    "            return action, False  # False indica exploitaÃ§Ã£o\n",
    "    \n",
    "    def update_q_value(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Atualiza valor Q usando a equaÃ§Ã£o de Bellman\"\"\"\n",
    "        current_q = self.q_table[state][action]\n",
    "        \n",
    "        if done:\n",
    "            # Estado terminal\n",
    "            target_q = reward\n",
    "        else:\n",
    "            # Q-Learning: max Q-value do prÃ³ximo estado\n",
    "            next_q_values = [self.q_table[next_state][a] for a in self.actions]\n",
    "            max_next_q = max(next_q_values) if next_q_values else 0\n",
    "            target_q = reward + self.discount_factor * max_next_q\n",
    "        \n",
    "        # AtualizaÃ§Ã£o Q-Learning\n",
    "        self.q_table[state][action] = current_q + self.learning_rate * (target_q - current_q)\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Reduz epsilon para diminuir exploraÃ§Ã£o ao longo do tempo\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        \"\"\"Treina um episÃ³dio completo\"\"\"\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        exploration_count = 0\n",
    "        \n",
    "        while True:\n",
    "            # Escolher aÃ§Ã£o\n",
    "            action, is_exploration = self.get_action(state, training=True)\n",
    "            if is_exploration:\n",
    "                exploration_count += 1\n",
    "                \n",
    "            # Executar aÃ§Ã£o\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Atualizar Q-value\n",
    "            self.update_q_value(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Acumular recompensa\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "        \n",
    "        # Decair epsilon\n",
    "        self.decay_epsilon()\n",
    "        \n",
    "        # Registrar estatÃ­sticas\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        self.exploration_counts.append(exploration_count)\n",
    "        \n",
    "        # Calcular retorno do episÃ³dio\n",
    "        episode_summary = env.get_episode_summary()\n",
    "        if episode_summary:\n",
    "            self.episode_returns.append(episode_summary['total_return'])\n",
    "        \n",
    "        return episode_summary\n",
    "    \n",
    "    def get_q_table_size(self):\n",
    "        \"\"\"Retorna tamanho atual da tabela Q\"\"\"\n",
    "        return len(self.q_table)\n",
    "    \n",
    "    def get_training_stats(self):\n",
    "        \"\"\"Retorna estatÃ­sticas de treinamento\"\"\"\n",
    "        if not self.episode_rewards:\n",
    "            return None\n",
    "            \n",
    "        return {\n",
    "            'total_episodes': len(self.episode_rewards),\n",
    "            'avg_reward': np.mean(self.episode_rewards[-100:]),  # Ãšltimos 100\n",
    "            'avg_return': np.mean(self.episode_returns[-100:]) if self.episode_returns else 0,\n",
    "            'current_epsilon': self.epsilon,\n",
    "            'q_table_size': self.get_q_table_size(),\n",
    "            'avg_exploration': np.mean(self.exploration_counts[-100:]) if self.exploration_counts else 0\n",
    "        }\n",
    "\n",
    "# Inicializar agente Batman\n",
    "agent = BatmanQLearningAgent(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    discount_factor=DISCOUNT_FACTOR,\n",
    "    epsilon_start=EPSILON_START,\n",
    "    epsilon_min=EPSILON_MIN,\n",
    "    epsilon_decay=EPSILON_DECAY\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ¯ Agente pronto para treinamento com {NUM_EPISODES} episÃ³dios!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efaf64d",
   "metadata": {},
   "source": [
    "## ğŸ‹ï¸ TREINAMENTO BATMAN\n",
    "\n",
    "Treinamento metodolÃ³gico e monitorado, seguindo padrÃµes de estabilidade Batman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbe42182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Iniciando treinamento para PETR3.SA\n",
      "ğŸ¦‡ Iniciando treinamento Batman - 1000 episÃ³dios\n",
      "ğŸ“Š RelatÃ³rios a cada 200 episÃ³dios\n",
      "============================================================\n",
      "ğŸ“ˆ EpisÃ³dio 200/1000\n",
      "   ğŸ’° Reward mÃ©dio (Ãºltimos 100): +0.00\n",
      "   ğŸ“Š Retorno mÃ©dio (Ãºltimos 100): +2.19%\n",
      "   ğŸ” Epsilon atual: 0.367\n",
      "   ğŸ§  Tamanho Q-table: 960\n",
      "   ğŸ¯ ExploraÃ§Ã£o mÃ©dia: 2363.4 aÃ§Ãµes/episÃ³dio\n",
      "   ğŸ’µ Ãšltimo episÃ³dio: R$ 51,494.46 (+2.99%)\n",
      "----------------------------------------\n",
      "ğŸ“ˆ EpisÃ³dio 200/1000\n",
      "   ğŸ’° Reward mÃ©dio (Ãºltimos 100): +0.00\n",
      "   ğŸ“Š Retorno mÃ©dio (Ãºltimos 100): +2.19%\n",
      "   ğŸ” Epsilon atual: 0.367\n",
      "   ğŸ§  Tamanho Q-table: 960\n",
      "   ğŸ¯ ExploraÃ§Ã£o mÃ©dia: 2363.4 aÃ§Ãµes/episÃ³dio\n",
      "   ğŸ’µ Ãšltimo episÃ³dio: R$ 51,494.46 (+2.99%)\n",
      "----------------------------------------\n",
      "ğŸ“ˆ EpisÃ³dio 400/1000\n",
      "   ğŸ’° Reward mÃ©dio (Ãºltimos 100): +0.00\n",
      "   ğŸ“Š Retorno mÃ©dio (Ãºltimos 100): +2.26%\n",
      "   ğŸ” Epsilon atual: 0.135\n",
      "   ğŸ§  Tamanho Q-table: 960\n",
      "   ğŸ¯ ExploraÃ§Ã£o mÃ©dia: 867.0 aÃ§Ãµes/episÃ³dio\n",
      "   ğŸ’µ Ãšltimo episÃ³dio: R$ 50,984.43 (+1.97%)\n",
      "----------------------------------------\n",
      "ğŸ“ˆ EpisÃ³dio 400/1000\n",
      "   ğŸ’° Reward mÃ©dio (Ãºltimos 100): +0.00\n",
      "   ğŸ“Š Retorno mÃ©dio (Ãºltimos 100): +2.26%\n",
      "   ğŸ” Epsilon atual: 0.135\n",
      "   ğŸ§  Tamanho Q-table: 960\n",
      "   ğŸ¯ ExploraÃ§Ã£o mÃ©dia: 867.0 aÃ§Ãµes/episÃ³dio\n",
      "   ğŸ’µ Ãšltimo episÃ³dio: R$ 50,984.43 (+1.97%)\n",
      "----------------------------------------\n",
      "ğŸ“ˆ EpisÃ³dio 600/1000\n",
      "   ğŸ’° Reward mÃ©dio (Ãºltimos 100): +0.00\n",
      "   ğŸ“Š Retorno mÃ©dio (Ãºltimos 100): +2.05%\n",
      "   ğŸ” Epsilon atual: 0.049\n",
      "   ğŸ§  Tamanho Q-table: 960\n",
      "   ğŸ¯ ExploraÃ§Ã£o mÃ©dia: 314.2 aÃ§Ãµes/episÃ³dio\n",
      "   ğŸ’µ Ãšltimo episÃ³dio: R$ 50,973.74 (+1.95%)\n",
      "----------------------------------------\n",
      "ğŸ“ˆ EpisÃ³dio 600/1000\n",
      "   ğŸ’° Reward mÃ©dio (Ãºltimos 100): +0.00\n",
      "   ğŸ“Š Retorno mÃ©dio (Ãºltimos 100): +2.05%\n",
      "   ğŸ” Epsilon atual: 0.049\n",
      "   ğŸ§  Tamanho Q-table: 960\n",
      "   ğŸ¯ ExploraÃ§Ã£o mÃ©dia: 314.2 aÃ§Ãµes/episÃ³dio\n",
      "   ğŸ’µ Ãšltimo episÃ³dio: R$ 50,973.74 (+1.95%)\n",
      "----------------------------------------\n",
      "ğŸ“ˆ EpisÃ³dio 800/1000\n",
      "   ğŸ’° Reward mÃ©dio (Ãºltimos 100): +0.00\n",
      "   ğŸ“Š Retorno mÃ©dio (Ãºltimos 100): +2.26%\n",
      "   ğŸ” Epsilon atual: 0.018\n",
      "   ğŸ§  Tamanho Q-table: 960\n",
      "   ğŸ¯ ExploraÃ§Ã£o mÃ©dia: 116.2 aÃ§Ãµes/episÃ³dio\n",
      "   ğŸ’µ Ãšltimo episÃ³dio: R$ 50,438.95 (+0.88%)\n",
      "----------------------------------------\n",
      "ğŸ“ˆ EpisÃ³dio 800/1000\n",
      "   ğŸ’° Reward mÃ©dio (Ãºltimos 100): +0.00\n",
      "   ğŸ“Š Retorno mÃ©dio (Ãºltimos 100): +2.26%\n",
      "   ğŸ” Epsilon atual: 0.018\n",
      "   ğŸ§  Tamanho Q-table: 960\n",
      "   ğŸ¯ ExploraÃ§Ã£o mÃ©dia: 116.2 aÃ§Ãµes/episÃ³dio\n",
      "   ğŸ’µ Ãšltimo episÃ³dio: R$ 50,438.95 (+0.88%)\n",
      "----------------------------------------\n",
      "ğŸ“ˆ EpisÃ³dio 1000/1000\n",
      "   ğŸ’° Reward mÃ©dio (Ãºltimos 100): +0.00\n",
      "   ğŸ“Š Retorno mÃ©dio (Ãºltimos 100): +2.02%\n",
      "   ğŸ” Epsilon atual: 0.010\n",
      "   ğŸ§  Tamanho Q-table: 960\n",
      "   ğŸ¯ ExploraÃ§Ã£o mÃ©dia: 50.9 aÃ§Ãµes/episÃ³dio\n",
      "   ğŸ’µ Ãšltimo episÃ³dio: R$ 50,349.90 (+0.70%)\n",
      "----------------------------------------\n",
      "âœ… Treinamento Batman concluÃ­do!\n",
      "ğŸ“Š EstatÃ­sticas finais:\n",
      "   ğŸ§  Q-table final: 960 estados\n",
      "   ğŸ¯ Epsilon final: 0.010\n",
      "   ğŸ’° Reward mÃ©dio final: +0.00\n",
      "   ğŸ“ˆ Retorno mÃ©dio final: +2.02%\n",
      "ğŸ“ˆ EpisÃ³dio 1000/1000\n",
      "   ğŸ’° Reward mÃ©dio (Ãºltimos 100): +0.00\n",
      "   ğŸ“Š Retorno mÃ©dio (Ãºltimos 100): +2.02%\n",
      "   ğŸ” Epsilon atual: 0.010\n",
      "   ğŸ§  Tamanho Q-table: 960\n",
      "   ğŸ¯ ExploraÃ§Ã£o mÃ©dia: 50.9 aÃ§Ãµes/episÃ³dio\n",
      "   ğŸ’µ Ãšltimo episÃ³dio: R$ 50,349.90 (+0.70%)\n",
      "----------------------------------------\n",
      "âœ… Treinamento Batman concluÃ­do!\n",
      "ğŸ“Š EstatÃ­sticas finais:\n",
      "   ğŸ§  Q-table final: 960 estados\n",
      "   ğŸ¯ Epsilon final: 0.010\n",
      "   ğŸ’° Reward mÃ©dio final: +0.00\n",
      "   ğŸ“ˆ Retorno mÃ©dio final: +2.02%\n"
     ]
    }
   ],
   "source": [
    "# ğŸ‹ï¸ Loop de Treinamento Batman (MetodolÃ³gico e Monitorado)\n",
    "def train_batman_agent(agent, env, num_episodes, print_every=100):\n",
    "    \"\"\"\n",
    "    Treina o agente Batman com monitoramento detalhado\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ¦‡ Iniciando treinamento Batman - {num_episodes} episÃ³dios\")\n",
    "    print(f\"ğŸ“Š RelatÃ³rios a cada {print_every} episÃ³dios\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    training_history = {\n",
    "        'episode': [],\n",
    "        'avg_reward': [],\n",
    "        'avg_return': [],\n",
    "        'epsilon': [],\n",
    "        'q_table_size': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        # Treinar episÃ³dio\n",
    "        episode_summary = agent.train_episode(env)\n",
    "        \n",
    "        # RelatÃ³rio periÃ³dico\n",
    "        if episode % print_every == 0:\n",
    "            stats = agent.get_training_stats()\n",
    "            \n",
    "            print(f\"ğŸ“ˆ EpisÃ³dio {episode}/{num_episodes}\")\n",
    "            print(f\"   ğŸ’° Reward mÃ©dio (Ãºltimos 100): {stats['avg_reward']:+.2f}\")\n",
    "            print(f\"   ğŸ“Š Retorno mÃ©dio (Ãºltimos 100): {stats['avg_return']:+.2%}\")\n",
    "            print(f\"   ğŸ” Epsilon atual: {stats['current_epsilon']:.3f}\")\n",
    "            print(f\"   ğŸ§  Tamanho Q-table: {stats['q_table_size']:,}\")\n",
    "            print(f\"   ğŸ¯ ExploraÃ§Ã£o mÃ©dia: {stats['avg_exploration']:.1f} aÃ§Ãµes/episÃ³dio\")\n",
    "            \n",
    "            if episode_summary:\n",
    "                print(f\"   ğŸ’µ Ãšltimo episÃ³dio: R$ {episode_summary['final_value']:,.2f} \" + \n",
    "                      f\"({episode_summary['total_return']:+.2%})\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Salvar histÃ³rico\n",
    "            training_history['episode'].append(episode)\n",
    "            training_history['avg_reward'].append(stats['avg_reward'])\n",
    "            training_history['avg_return'].append(stats['avg_return'])\n",
    "            training_history['epsilon'].append(stats['current_epsilon'])\n",
    "            training_history['q_table_size'].append(stats['q_table_size'])\n",
    "    \n",
    "    print(\"âœ… Treinamento Batman concluÃ­do!\")\n",
    "    final_stats = agent.get_training_stats()\n",
    "    print(f\"ğŸ“Š EstatÃ­sticas finais:\")\n",
    "    print(f\"   ğŸ§  Q-table final: {final_stats['q_table_size']:,} estados\")\n",
    "    print(f\"   ğŸ¯ Epsilon final: {final_stats['current_epsilon']:.3f}\")\n",
    "    print(f\"   ğŸ’° Reward mÃ©dio final: {final_stats['avg_reward']:+.2f}\")\n",
    "    print(f\"   ğŸ“ˆ Retorno mÃ©dio final: {final_stats['avg_return']:+.2%}\")\n",
    "    \n",
    "    return training_history\n",
    "\n",
    "# Executar treinamento\n",
    "if df_stock is not None:\n",
    "    print(f\"ğŸš€ Iniciando treinamento para {TICKER_SYMBOL}\")\n",
    "    training_history = train_batman_agent(agent, env, NUM_EPISODES, print_every=200)\n",
    "else:\n",
    "    print(\"âŒ NÃ£o Ã© possÃ­vel treinar sem dados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7ef0eb",
   "metadata": {},
   "source": [
    "## ğŸ” DEBUG - DIAGNÃ“STICO DO TREINAMENTO\n",
    "\n",
    "CÃ©lulas de debug para investigar problemas de rewards zerados e Q-table nÃ£o crescendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f0fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” DEBUG 1: Verificar ambiente bÃ¡sico\n",
    "print(\"ğŸ” DEBUG 1: VERIFICAÃ‡ÃƒO DO AMBIENTE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Testar reset do ambiente\n",
    "state = env.reset()\n",
    "print(f\"   Estado inicial: {state}\")\n",
    "print(f\"   Tipo do estado: {type(state)}\")\n",
    "print(f\"   Tamanho do estado: {len(state)}\")\n",
    "print(f\"   Cash inicial: R$ {env.cash:.2f}\")\n",
    "print(f\"   Shares iniciais: {env.shares}\")\n",
    "print(f\"   PreÃ§o atual: R$ {env.prices[env.current_step]:.2f}\")\n",
    "print(f\"   Dia atual: {env.current_step}\")\n",
    "print(f\"   Pode comprar uma aÃ§Ã£o? {env.cash >= env.prices[env.current_step]}\")\n",
    "print(f\"   Quantas aÃ§Ãµes pode comprar? {int(env.cash // env.prices[env.current_step])}\")\n",
    "\n",
    "# Testar algumas aÃ§Ãµes manualmente\n",
    "print(f\"\\nğŸ¯ TESTE DE AÃ‡Ã•ES MANUAIS:\")\n",
    "for action_idx, action_name in [(0, 'HOLD'), (1, 'BUY'), (2, 'SELL')]:\n",
    "    # Criar ambiente limpo para cada teste\n",
    "    env_test = BatmanTradingEnvironment(prices, state_manager, INITIAL_CAPITAL)\n",
    "    state = env_test.reset()\n",
    "    \n",
    "    portfolio_before = env_test.get_portfolio_value()\n",
    "    next_state, reward, done, info = env_test.step(action_idx)\n",
    "    portfolio_after = env_test.get_portfolio_value()\n",
    "    \n",
    "    print(f\"   {action_name:4s}: Reward={reward:+8.2f} | Portfolio: R$ {portfolio_before:.0f} â†’ R$ {portfolio_after:.0f} | Executada: {info['action_executed']}\")\n",
    "    print(f\"          Cash: R$ {info['cash']:.0f} | Shares: {info['shares']} | PreÃ§o: R$ {info['current_price']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f80724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” DEBUG 2: AnÃ¡lise de discretizaÃ§Ã£o de estados\n",
    "print(\"\\nğŸ” DEBUG 2: ANÃLISE DE DISCRETIZAÃ‡ÃƒO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Testar discretizaÃ§Ã£o com diferentes janelas\n",
    "test_steps = [0, 10, 20, 50, 100]\n",
    "print(f\"   Bins de preÃ§o configurados: {NUM_PRICE_BINS}\")\n",
    "print(f\"   Tamanho da janela: {WINDOW_SIZE}\")\n",
    "print(f\"   Range de preÃ§os nos dados: R$ {prices.min():.2f} - R$ {prices.max():.2f}\")\n",
    "\n",
    "print(f\"\\n   Estados discretos em diferentes momentos:\")\n",
    "unique_states = set()\n",
    "for step in test_steps:\n",
    "    if step < len(prices):\n",
    "        state_discrete = state_manager.get_state(step)  # JÃ¡ retorna estado discreto\n",
    "        unique_states.add(state_discrete)\n",
    "        price = prices[step]\n",
    "        print(f\"      Passo {step:3d}: PreÃ§o R$ {price:6.2f} â†’ Estado {state_discrete}\")\n",
    "\n",
    "print(f\"\\n   Estados Ãºnicos encontrados: {len(unique_states)}\")\n",
    "print(f\"   Estados na Q-table atual: {len(agent.q_table)}\")\n",
    "\n",
    "# Verificar se estados estÃ£o sendo criados corretamente\n",
    "print(f\"\\n   Primeiros estados na Q-table:\")\n",
    "for i, (state_key, q_values) in enumerate(list(agent.q_table.items())[:5]):\n",
    "    print(f\"      {state_key}: {q_values}\")\n",
    "    if i >= 4:  # Mostrar apenas os primeiros 5\n",
    "        break\n",
    "\n",
    "# Testar diferentes bins para o mesmo passo\n",
    "print(f\"\\n   AnÃ¡lise detalhada de discretizaÃ§Ã£o:\")\n",
    "test_step = 50\n",
    "if test_step < len(prices):\n",
    "    price_at_step = prices[test_step]\n",
    "    bin_number = state_manager.discretize_price(price_at_step)\n",
    "    bin_range_min = state_manager.price_bins[bin_number] if bin_number < len(state_manager.price_bins)-1 else state_manager.price_bins[-2]\n",
    "    bin_range_max = state_manager.price_bins[bin_number + 1] if bin_number < len(state_manager.price_bins)-1 else state_manager.price_bins[-1]\n",
    "    \n",
    "    print(f\"      PreÃ§o no passo {test_step}: R$ {price_at_step:.2f}\")\n",
    "    print(f\"      Bin atribuÃ­do: {bin_number} (de 0 a {NUM_PRICE_BINS-1})\")\n",
    "    print(f\"      Range do bin: R$ {bin_range_min:.2f} - R$ {bin_range_max:.2f}\")\n",
    "    print(f\"      Tamanho do bin: R$ {bin_range_max - bin_range_min:.2f}\")\n",
    "\n",
    "# Verificar distribuiÃ§Ã£o dos preÃ§os nos bins\n",
    "print(f\"\\n   DistribuiÃ§Ã£o dos preÃ§os nos bins:\")\n",
    "all_bins = [state_manager.discretize_price(p) for p in prices[:100]]  # Primeiros 100 preÃ§os\n",
    "bin_counts = {}\n",
    "for bin_num in all_bins:\n",
    "    bin_counts[bin_num] = bin_counts.get(bin_num, 0) + 1\n",
    "\n",
    "for bin_num in sorted(bin_counts.keys())[:10]:  # Mostrar primeiros 10 bins\n",
    "    print(f\"      Bin {bin_num:2d}: {bin_counts[bin_num]:3d} preÃ§os\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c3f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” DEBUG 3: EpisÃ³dio de treinamento detalhado\n",
    "print(\"\\nğŸ” DEBUG 3: EPISÃ“DIO DETALHADO (1 episÃ³dio completo)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Resetar ambiente para debug\n",
    "env.reset()\n",
    "agent.epsilon = 0.5  # ForÃ§ar alguma exploraÃ§Ã£o\n",
    "\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "state_history = []\n",
    "reward_history = []\n",
    "\n",
    "print(f\"   Capital inicial: R$ {INITIAL_CAPITAL:.0f}\")\n",
    "print(f\"   Epsilon atual: {agent.epsilon:.3f}\")\n",
    "print(f\"   Tamanho da Q-table antes: {len(agent.q_table)}\")\n",
    "\n",
    "print(f\"\\n   Primeiros 10 passos do episÃ³dio:\")\n",
    "state = env.get_current_state()\n",
    "\n",
    "for step in range(min(10, len(prices) - WINDOW_SIZE - 1)):\n",
    "    # Estado atual (jÃ¡ discreto)\n",
    "    action, is_exploration = agent.get_action(state, training=True)\n",
    "    \n",
    "    # Executar aÃ§Ã£o\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    # Guardar histÃ³rico\n",
    "    state_history.append(state)\n",
    "    reward_history.append(reward)\n",
    "    \n",
    "    # Treinar agente\n",
    "    agent.update_q_value(state, action, reward, next_state, done)\n",
    "    \n",
    "    # Log detalhado\n",
    "    action_names = ['HOLD', 'BUY', 'SELL']\n",
    "    exploration_mark = \"(EXPLORE)\" if is_exploration else \"(EXPLOIT)\"\n",
    "    print(f\"      {step+1:2d}. Estado: {state} | AÃ§Ã£o: {action_names[action]} {exploration_mark}\")\n",
    "    print(f\"           Reward: {reward:+7.2f} | Portfolio: R$ {env.get_portfolio_value():.0f}\")\n",
    "    print(f\"           Cash: R$ {info['cash']:.0f} | Shares: {info['shares']} | PreÃ§o: R$ {info['current_price']:.2f} | Executada: {info['action_executed']}\")\n",
    "    \n",
    "    state = next_state\n",
    "    step_count += 1\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"\\n   RESUMO DO DEBUG:\")\n",
    "print(f\"      Passos executados: {step_count}\")\n",
    "print(f\"      Reward total: {total_reward:.2f}\")\n",
    "print(f\"      Estados Ãºnicos visitados: {len(set(state_history))}\")\n",
    "print(f\"      Tamanho da Q-table depois: {len(agent.q_table)}\")\n",
    "print(f\"      Rewards Ãºnicos: {set(reward_history)}\")\n",
    "print(f\"      Portfolio final: R$ {env.get_portfolio_value():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c991de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” DEBUG 4: AnÃ¡lise de recompensas e funÃ§Ã£o objetivo\n",
    "print(\"\\nğŸ” DEBUG 4: ANÃLISE DE RECOMPENSAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simular diferentes cenÃ¡rios de trading\n",
    "scenarios = [\n",
    "    {\"name\": \"Compra com alta\", \"action\": 1, \"price_change\": 0.05},\n",
    "    {\"name\": \"Compra com queda\", \"action\": 1, \"price_change\": -0.03},\n",
    "    {\"name\": \"Venda com alta\", \"action\": 2, \"price_change\": 0.04},\n",
    "    {\"name\": \"Hold com volatilidade\", \"action\": 0, \"price_change\": 0.02}\n",
    "]\n",
    "\n",
    "print(f\"   Simulando rewards para diferentes cenÃ¡rios:\")\n",
    "print(f\"   (usando preÃ§os artificiais para teste)\")\n",
    "\n",
    "for scenario in scenarios:\n",
    "    # Criar ambiente de teste com preÃ§os controlados (precisa ter WINDOW_SIZE + alguns dias)\n",
    "    base_price = 100.0\n",
    "    future_price = base_price * (1 + scenario[\"price_change\"])\n",
    "    \n",
    "    # Criar array com preÃ§os suficientes para o ambiente funcionar\n",
    "    test_prices = np.full(WINDOW_SIZE + 5, base_price)  # Preencher com preÃ§o base\n",
    "    test_prices[-1] = future_price  # Ãšltimo preÃ§o com a mudanÃ§a\n",
    "    \n",
    "    # Criar state_manager temporÃ¡rio para este teste\n",
    "    temp_state_manager = BatmanStateManager(test_prices, NUM_PRICE_BINS, WINDOW_SIZE)\n",
    "    test_env = BatmanTradingEnvironment(test_prices, temp_state_manager, INITIAL_CAPITAL)\n",
    "    \n",
    "    state = test_env.reset()\n",
    "    next_state, reward, done, info = test_env.step(scenario[\"action\"])\n",
    "    \n",
    "    print(f\"      {scenario['name']:20s}: Reward = {reward:+8.2f} | MudanÃ§a = {scenario['price_change']:+5.1%}\")\n",
    "    print(f\"                              Portfolio: R$ {info['portfolio_value']:.0f} | Executada: {info['action_executed']}\")\n",
    "\n",
    "# Verificar se o cÃ¡lculo de reward estÃ¡ funcionando\n",
    "print(f\"\\n   FÃ³rmula de reward atual:\")\n",
    "print(f\"      reward = portfolio_value_after - portfolio_value_before\")\n",
    "print(f\"      Isso significa que rewards sÃ£o em R$ (mudanÃ§a absoluta do portfolio)\")\n",
    "print(f\"      Um reward de 100.0 = R$ 100 de ganho\")\n",
    "print(f\"      Um reward de -50.0 = R$ 50 de perda\")\n",
    "print(f\"      Se reward = 0, significa que o portfolio nÃ£o mudou (aÃ§Ã£o HOLD ou nÃ£o executada)\")\n",
    "\n",
    "# Testar se aÃ§Ãµes estÃ£o sendo executadas corretamente\n",
    "print(f\"\\n   VerificaÃ§Ã£o de execuÃ§Ã£o de aÃ§Ãµes:\")\n",
    "test_env = BatmanTradingEnvironment(prices, state_manager, INITIAL_CAPITAL)\n",
    "test_env.reset()\n",
    "\n",
    "# Tentar comprar uma aÃ§Ã£o\n",
    "print(f\"      Antes da compra: Cash=R$ {test_env.cash:.0f}, Shares={test_env.shares}\")\n",
    "_, reward_buy, _, info_buy = test_env.step(1)  # BUY\n",
    "print(f\"      Depois da compra: Cash=R$ {info_buy['cash']:.0f}, Shares={info_buy['shares']}, Reward={reward_buy:+.4f}\")\n",
    "\n",
    "# Tentar vender\n",
    "if test_env.shares > 0:\n",
    "    _, reward_sell, _, info_sell = test_env.step(2)  # SELL\n",
    "    print(f\"      Depois da venda: Cash=R$ {info_sell['cash']:.0f}, Shares={info_sell['shares']}, Reward={reward_sell:+.4f}\")\n",
    "else:\n",
    "    print(f\"      NÃ£o hÃ¡ shares para vender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” DEBUG 5: DiagnÃ³stico final - possÃ­veis problemas\n",
    "print(\"\\nğŸ” DEBUG 5: DIAGNÃ“STICO DE PROBLEMAS COMUNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"ğŸ” CHECKLIST DE PROBLEMAS POTENCIAIS:\")\n",
    "\n",
    "# 1. Verificar se hÃ¡ diversidade nos estados\n",
    "unique_states_sample = set()\n",
    "for i in range(0, min(100, len(prices)), 10):\n",
    "    if i >= WINDOW_SIZE:  # SÃ³ pode calcular estado se tiver janela suficiente\n",
    "        state_discrete = state_manager.get_state(i)\n",
    "        unique_states_sample.add(state_discrete)\n",
    "\n",
    "print(f\"\\n   1ï¸âƒ£ DIVERSIDADE DE ESTADOS:\")\n",
    "print(f\"      Estados Ãºnicos em 100 passos: {len(unique_states_sample)}\")\n",
    "print(f\"      Estados Ãºnicos esperados: ~{NUM_PRICE_BINS * 3}\")  # AproximaÃ§Ã£o\n",
    "if len(unique_states_sample) < 10:\n",
    "    print(f\"      âš ï¸  PROBLEMA: Poucos estados Ãºnicos! Considere aumentar NUM_PRICE_BINS\")\n",
    "else:\n",
    "    print(f\"      âœ… OK: Boa diversidade de estados\")\n",
    "\n",
    "# 2. Verificar range de preÃ§os vs discretizaÃ§Ã£o\n",
    "price_range = prices.max() - prices.min()\n",
    "price_std = prices.std()\n",
    "print(f\"\\n   2ï¸âƒ£ DISCRETIZAÃ‡ÃƒO DE PREÃ‡OS:\")\n",
    "print(f\"      Range de preÃ§os: R$ {price_range:.2f}\")\n",
    "print(f\"      Desvio padrÃ£o: R$ {price_std:.2f}\")\n",
    "print(f\"      Bins configurados: {NUM_PRICE_BINS}\")\n",
    "print(f\"      Tamanho do bin: ~R$ {price_range/NUM_PRICE_BINS:.2f}\")\n",
    "if price_range/NUM_PRICE_BINS > price_std:\n",
    "    print(f\"      âš ï¸  PROBLEMA: Bins muito grandes, pouca sensibilidade a mudanÃ§as!\")\n",
    "else:\n",
    "    print(f\"      âœ… OK: Bins adequados para capturar variaÃ§Ãµes\")\n",
    "\n",
    "# 3. Verificar capacidade de trading\n",
    "min_price = prices.min()\n",
    "max_shares_possible = int(INITIAL_CAPITAL // min_price)\n",
    "print(f\"\\n   3ï¸âƒ£ CAPACIDADE DE TRADING:\")\n",
    "print(f\"      Capital inicial: R$ {INITIAL_CAPITAL:.0f}\")\n",
    "print(f\"      PreÃ§o mÃ­nimo: R$ {min_price:.2f}\")\n",
    "print(f\"      Max aÃ§Ãµes possÃ­veis: {max_shares_possible}\")\n",
    "if max_shares_possible < 10:\n",
    "    print(f\"      âš ï¸  PROBLEMA: Capital muito baixo, poucas oportunidades de trading!\")\n",
    "else:\n",
    "    print(f\"      âœ… OK: Capital suficiente para trading\")\n",
    "\n",
    "# 4. Verificar parÃ¢metros de aprendizagem\n",
    "print(f\"\\n   4ï¸âƒ£ PARÃ‚METROS DE APRENDIZAGEM:\")\n",
    "print(f\"      Learning rate: {agent.learning_rate}\")\n",
    "print(f\"      Discount factor: {agent.discount_factor}\")\n",
    "print(f\"      Epsilon inicial: {agent.epsilon}\")\n",
    "if agent.learning_rate < 0.01:\n",
    "    print(f\"      âš ï¸  ATENÃ‡ÃƒO: Learning rate muito baixo, aprendizagem pode ser lenta\")\n",
    "else:\n",
    "    print(f\"      âœ… OK: Learning rate adequado\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ PRÃ“XIMOS PASSOS SUGERIDOS:\")\n",
    "print(f\"   â€¢ Execute os debugs acima para identificar o problema especÃ­fico\")\n",
    "print(f\"   â€¢ Se rewards = 0: verifique se aÃ§Ãµes estÃ£o sendo executadas (DEBUG 1 e 4)\")\n",
    "print(f\"   â€¢ Se Q-table nÃ£o cresce: verifique diversidade de estados (DEBUG 2)\")\n",
    "print(f\"   â€¢ Considere ajustar: NUM_PRICE_BINS={NUM_PRICE_BINS*2}, INITIAL_CAPITAL={INITIAL_CAPITAL*2}\")\n",
    "print(f\"   â€¢ Para debugging ativo, execute um episÃ³dio com DEBUG 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bbbdc2",
   "metadata": {},
   "source": [
    "## ğŸ¯ PROPOSTA DE MELHORIA NO SISTEMA DE REWARDS\n",
    "\n",
    "Baseado no diagnÃ³stico, vamos melhorar o sistema de recompensas para incentivar aprendizagem mais efetiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f68a7d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ANÃLISE DOS PROBLEMAS IDENTIFICADOS:\n",
      "============================================================\n",
      "ğŸ“Š PROBLEMAS OBSERVADOS NO DIAGNÃ“STICO:\n",
      "   1ï¸âƒ£ Rewards = 0: Muitas aÃ§Ãµes nÃ£o executadas ou HOLD constante\n",
      "   2ï¸âƒ£ Q-table nÃ£o cresce: Estados pouco diversificados\n",
      "   3ï¸âƒ£ Aprendizagem lenta: Sem incentivos claros para explorar\n",
      "   4ï¸âƒ£ Foco apenas em portfolio: NÃ£o premia comportamento desejado\n",
      "\n",
      "ğŸ“ˆ CONFIGURAÃ‡ÃƒO ATUAL:\n",
      "   Capital inicial: R$ 50,000\n",
      "   Bins de preÃ§o: 5\n",
      "   Janela de estado: 30\n",
      "\n",
      "ğŸš¨ PROBLEMA COM REWARD ATUAL:\n",
      "   HOLD: Portfolio R$ 50000 â†’ Reward = 0.0\n",
      "   BUY (falhou): Portfolio R$ 10 â†’ Reward = 0.0, Executada: True\n",
      "\n",
      "ğŸ’¡ CONCLUSÃƒO: Sistema atual nÃ£o incentiva exploraÃ§Ã£o nem pune inaÃ§Ã£o!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” ANÃLISE DOS PROBLEMAS ATUAIS NO SISTEMA DE REWARDS\n",
    "print(\"ğŸ” ANÃLISE DOS PROBLEMAS IDENTIFICADOS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"ğŸ“Š PROBLEMAS OBSERVADOS NO DIAGNÃ“STICO:\")\n",
    "print(\"   1ï¸âƒ£ Rewards = 0: Muitas aÃ§Ãµes nÃ£o executadas ou HOLD constante\")\n",
    "print(\"   2ï¸âƒ£ Q-table nÃ£o cresce: Estados pouco diversificados\")  \n",
    "print(\"   3ï¸âƒ£ Aprendizagem lenta: Sem incentivos claros para explorar\")\n",
    "print(\"   4ï¸âƒ£ Foco apenas em portfolio: NÃ£o premia comportamento desejado\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ CONFIGURAÃ‡ÃƒO ATUAL:\")\n",
    "print(f\"   Capital inicial: R$ {INITIAL_CAPITAL:,.0f}\")\n",
    "print(f\"   Bins de preÃ§o: {NUM_PRICE_BINS}\")\n",
    "print(f\"   Janela de estado: {WINDOW_SIZE}\")\n",
    "\n",
    "# Simular problema atual\n",
    "print(f\"\\nğŸš¨ PROBLEMA COM REWARD ATUAL:\")\n",
    "test_env = BatmanTradingEnvironment(prices, state_manager, INITIAL_CAPITAL)\n",
    "test_env.reset()\n",
    "\n",
    "# CenÃ¡rio 1: HOLD (sempre reward = 0)\n",
    "portfolio_before = test_env.get_portfolio_value()\n",
    "_, reward_hold, _, _ = test_env.step(0)  # HOLD\n",
    "print(f\"   HOLD: Portfolio R$ {portfolio_before:.0f} â†’ Reward = {reward_hold}\")\n",
    "\n",
    "# CenÃ¡rio 2: Tentativa de compra sem capital suficiente\n",
    "test_env.cash = 10.0  # Muito pouco cash\n",
    "portfolio_before = test_env.get_portfolio_value()\n",
    "_, reward_failed_buy, _, info = test_env.step(1)  # BUY\n",
    "print(f\"   BUY (falhou): Portfolio R$ {portfolio_before:.0f} â†’ Reward = {reward_failed_buy}, Executada: {info['action_executed']}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ CONCLUSÃƒO: Sistema atual nÃ£o incentiva exploraÃ§Ã£o nem pune inaÃ§Ã£o!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fabbcf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ PROPOSTA DE NOVO SISTEMA DE REWARDS\n",
      "============================================================\n",
      "ğŸš€ SISTEMA PROPOSTO - 'Batman Smart Rewards':\n",
      "\n",
      "ğŸ“ˆ COMPONENTES DO REWARD:\n",
      "   1ï¸âƒ£ REWARD BASE: MudanÃ§a no portfolio (atual)\n",
      "   2ï¸âƒ£ BÃ”NUS DE AÃ‡ÃƒO: +5 por executar BUY/SELL com sucesso\n",
      "   3ï¸âƒ£ PENALIDADE DE INAÃ‡ÃƒO: -2 por HOLD excessivo\n",
      "   4ï¸âƒ£ PENALIDADE DE FALHA: -10 por tentar aÃ§Ã£o impossÃ­vel\n",
      "   5ï¸âƒ£ BÃ”NUS DE TIMING: +20 por boa decisÃ£o (comprar na baixa, vender na alta)\n",
      "   6ï¸âƒ£ INCENTIVO DE EXPLORAÃ‡ÃƒO: +1 por visitar estado novo\n",
      "\n",
      "ğŸ“Š FÃ“RMULA PROPOSTA:\n",
      "   reward = portfolio_change + action_bonus + timing_bonus + exploration_bonus - penalties\n",
      "\n",
      "ğŸ® EXEMPLOS DO NOVO SISTEMA:\n",
      "   â€¢ Comprar e preÃ§o subir: +100 (portfolio) +5 (aÃ§Ã£o) +20 (timing) = +125\n",
      "   â€¢ Vender na alta: +80 (portfolio) +5 (aÃ§Ã£o) +20 (timing) = +105\n",
      "   â€¢ HOLD por muitos passos: 0 (portfolio) -2 (inaÃ§Ã£o) = -2\n",
      "   â€¢ Tentar comprar sem cash: 0 (portfolio) -10 (falha) = -10\n",
      "   â€¢ Estado nunca visitado: reward atual +1 (exploraÃ§Ã£o)\n",
      "\n",
      "âš™ï¸ PARÃ‚METROS CONFIGURÃVEIS:\n",
      "   ACTION_BONUS = 5          # BÃ´nus por executar aÃ§Ã£o\n",
      "   INACTION_PENALTY = -2     # Penalidade por HOLD\n",
      "   FAILURE_PENALTY = -10     # Penalidade por aÃ§Ã£o falhada\n",
      "   TIMING_BONUS = 20         # BÃ´nus por bom timing\n",
      "   EXPLORATION_BONUS = 1     # BÃ´nus por estado novo\n",
      "   HOLD_TOLERANCE = 3        # Max HOLDs seguidos sem penalidade\n",
      "\n",
      "ğŸ§  BENEFÃCIOS ESPERADOS:\n",
      "   âœ… Incentiva trading ativo vs passivo\n",
      "   âœ… Pune tentativas invÃ¡lidas (aprende restriÃ§Ãµes)\n",
      "   âœ… Premia bom timing (essÃªncia do trading)\n",
      "   âœ… Encoraja exploraÃ§Ã£o (Q-table cresce)\n",
      "   âœ… Balanceado - nÃ£o sÃ³ portfolio matters\n",
      "\n",
      "ğŸ¤” QUER IMPLEMENTAR ESTE SISTEMA?\n",
      "   Responda 'sim' para implementar ou sugira modificaÃ§Ãµes!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ PROPOSTA DE SISTEMA DE REWARDS MELHORADO\n",
    "print(\"\\nğŸ¯ PROPOSTA DE NOVO SISTEMA DE REWARDS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"ğŸš€ SISTEMA PROPOSTO - 'Batman Smart Rewards':\")\n",
    "print()\n",
    "print(\"ğŸ“ˆ COMPONENTES DO REWARD:\")\n",
    "print(\"   1ï¸âƒ£ REWARD BASE: MudanÃ§a no portfolio (atual)\")\n",
    "print(\"   2ï¸âƒ£ BÃ”NUS DE AÃ‡ÃƒO: +5 por executar BUY/SELL com sucesso\") \n",
    "print(\"   3ï¸âƒ£ PENALIDADE DE INAÃ‡ÃƒO: -2 por HOLD excessivo\")\n",
    "print(\"   4ï¸âƒ£ PENALIDADE DE FALHA: -10 por tentar aÃ§Ã£o impossÃ­vel\")\n",
    "print(\"   5ï¸âƒ£ BÃ”NUS DE TIMING: +20 por boa decisÃ£o (comprar na baixa, vender na alta)\")\n",
    "print(\"   6ï¸âƒ£ INCENTIVO DE EXPLORAÃ‡ÃƒO: +1 por visitar estado novo\")\n",
    "\n",
    "print(f\"\\nğŸ“Š FÃ“RMULA PROPOSTA:\")\n",
    "print(f\"   reward = portfolio_change + action_bonus + timing_bonus + exploration_bonus - penalties\")\n",
    "\n",
    "print(f\"\\nğŸ® EXEMPLOS DO NOVO SISTEMA:\")\n",
    "print(f\"   â€¢ Comprar e preÃ§o subir: +100 (portfolio) +5 (aÃ§Ã£o) +20 (timing) = +125\")\n",
    "print(f\"   â€¢ Vender na alta: +80 (portfolio) +5 (aÃ§Ã£o) +20 (timing) = +105\")  \n",
    "print(f\"   â€¢ HOLD por muitos passos: 0 (portfolio) -2 (inaÃ§Ã£o) = -2\")\n",
    "print(f\"   â€¢ Tentar comprar sem cash: 0 (portfolio) -10 (falha) = -10\")\n",
    "print(f\"   â€¢ Estado nunca visitado: reward atual +1 (exploraÃ§Ã£o)\")\n",
    "\n",
    "print(f\"\\nâš™ï¸ PARÃ‚METROS CONFIGURÃVEIS:\")\n",
    "print(f\"   ACTION_BONUS = 5          # BÃ´nus por executar aÃ§Ã£o\")\n",
    "print(f\"   INACTION_PENALTY = -2     # Penalidade por HOLD\")\n",
    "print(f\"   FAILURE_PENALTY = -10     # Penalidade por aÃ§Ã£o falhada\")\n",
    "print(f\"   TIMING_BONUS = 20         # BÃ´nus por bom timing\")\n",
    "print(f\"   EXPLORATION_BONUS = 1     # BÃ´nus por estado novo\")\n",
    "print(f\"   HOLD_TOLERANCE = 3        # Max HOLDs seguidos sem penalidade\")\n",
    "\n",
    "print(f\"\\nğŸ§  BENEFÃCIOS ESPERADOS:\")\n",
    "print(f\"   âœ… Incentiva trading ativo vs passivo\")\n",
    "print(f\"   âœ… Pune tentativas invÃ¡lidas (aprende restriÃ§Ãµes)\")\n",
    "print(f\"   âœ… Premia bom timing (essÃªncia do trading)\")\n",
    "print(f\"   âœ… Encoraja exploraÃ§Ã£o (Q-table cresce)\")\n",
    "print(f\"   âœ… Balanceado - nÃ£o sÃ³ portfolio matters\")\n",
    "\n",
    "print(f\"\\nğŸ¤” QUER IMPLEMENTAR ESTE SISTEMA?\")\n",
    "print(f\"   Responda 'sim' para implementar ou sugira modificaÃ§Ãµes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17bb32e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§ª SIMULAÃ‡ÃƒO: COMO FUNCIONARIA O NOVO SISTEMA\n",
      "============================================================\n",
      "ğŸ“Š COMPARAÃ‡ÃƒO: Sistema Atual vs Proposto\n",
      "------------------------------------------------------------\n",
      "Compra bem sucedida (preÃ§o sobe)   \n",
      "   Atual: +100 | Proposto: +125 | DiferenÃ§a:  +25\n",
      "\n",
      "Venda inteligente (preÃ§o sobe apÃ³s)\n",
      "   Atual:  +80 | Proposto: +105 | DiferenÃ§a:  +25\n",
      "\n",
      "HOLD excessivo (5Âº seguido)        \n",
      "   Atual:   +0 | Proposto:   -2 | DiferenÃ§a:   -2\n",
      "\n",
      "Tentativa de compra sem cash       \n",
      "   Atual:   +0 | Proposto:  -10 | DiferenÃ§a:  -10\n",
      "\n",
      "ExploraÃ§Ã£o (estado novo)           \n",
      "   Atual:  -20 | Proposto:  -14 | DiferenÃ§a:   +6\n",
      "\n",
      "ğŸ’¡ OBSERVE: O novo sistema diferencia melhor entre boas e mÃ¡s decisÃµes!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª SIMULAÃ‡ÃƒO DO NOVO SISTEMA DE REWARDS\n",
    "print(\"\\nğŸ§ª SIMULAÃ‡ÃƒO: COMO FUNCIONARIA O NOVO SISTEMA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ParÃ¢metros do novo sistema\n",
    "ACTION_BONUS = 5\n",
    "INACTION_PENALTY = -2  \n",
    "FAILURE_PENALTY = -10\n",
    "TIMING_BONUS = 20\n",
    "EXPLORATION_BONUS = 1\n",
    "HOLD_TOLERANCE = 3\n",
    "\n",
    "def calculate_new_reward(portfolio_change, action, action_executed, price_change, is_new_state, consecutive_holds):\n",
    "    \"\"\"Simula o novo sistema de rewards\"\"\"\n",
    "    reward = portfolio_change  # Base reward (atual)\n",
    "    \n",
    "    # BÃ´nus por executar aÃ§Ã£o\n",
    "    if action != 0 and action_executed:  # BUY/SELL executado\n",
    "        reward += ACTION_BONUS\n",
    "        \n",
    "    # BÃ´nus de timing (boa decisÃ£o)\n",
    "    if action == 1 and action_executed and price_change > 0:  # Comprou e preÃ§o subiu\n",
    "        reward += TIMING_BONUS\n",
    "    elif action == 2 and action_executed and price_change > 0:  # Vendeu e preÃ§o subiu (bom!)\n",
    "        reward += TIMING_BONUS\n",
    "        \n",
    "    # Penalidade por inaÃ§Ã£o excessiva\n",
    "    if action == 0 and consecutive_holds > HOLD_TOLERANCE:\n",
    "        reward += INACTION_PENALTY\n",
    "        \n",
    "    # Penalidade por tentar aÃ§Ã£o impossÃ­vel\n",
    "    if action != 0 and not action_executed:\n",
    "        reward += FAILURE_PENALTY\n",
    "        \n",
    "    # BÃ´nus por exploraÃ§Ã£o\n",
    "    if is_new_state:\n",
    "        reward += EXPLORATION_BONUS\n",
    "        \n",
    "    return reward\n",
    "\n",
    "# CenÃ¡rios de teste\n",
    "scenarios = [\n",
    "    {\"name\": \"Compra bem sucedida (preÃ§o sobe)\", \"portfolio_change\": 100, \"action\": 1, \"executed\": True, \"price_change\": 0.02, \"new_state\": False, \"holds\": 0},\n",
    "    {\"name\": \"Venda inteligente (preÃ§o sobe apÃ³s)\", \"portfolio_change\": 80, \"action\": 2, \"executed\": True, \"price_change\": 0.03, \"new_state\": False, \"holds\": 0},\n",
    "    {\"name\": \"HOLD excessivo (5Âº seguido)\", \"portfolio_change\": 0, \"action\": 0, \"executed\": True, \"price_change\": 0, \"new_state\": False, \"holds\": 5},\n",
    "    {\"name\": \"Tentativa de compra sem cash\", \"portfolio_change\": 0, \"action\": 1, \"executed\": False, \"price_change\": 0, \"new_state\": False, \"holds\": 0},\n",
    "    {\"name\": \"ExploraÃ§Ã£o (estado novo)\", \"portfolio_change\": -20, \"action\": 1, \"executed\": True, \"price_change\": -0.01, \"new_state\": True, \"holds\": 0},\n",
    "]\n",
    "\n",
    "print(\"ğŸ“Š COMPARAÃ‡ÃƒO: Sistema Atual vs Proposto\")\n",
    "print(\"-\" * 60)\n",
    "for scenario in scenarios:\n",
    "    current_reward = scenario[\"portfolio_change\"]\n",
    "    new_reward = calculate_new_reward(\n",
    "        scenario[\"portfolio_change\"], \n",
    "        scenario[\"action\"], \n",
    "        scenario[\"executed\"],\n",
    "        scenario[\"price_change\"],\n",
    "        scenario[\"new_state\"],\n",
    "        scenario[\"holds\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"{scenario['name']:35s}\")\n",
    "    print(f\"   Atual: {current_reward:+4.0f} | Proposto: {new_reward:+4.0f} | DiferenÃ§a: {new_reward - current_reward:+4.0f}\")\n",
    "    print()\n",
    "\n",
    "print(\"ğŸ’¡ OBSERVE: O novo sistema diferencia melhor entre boas e mÃ¡s decisÃµes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0be72f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ CONFIGURANDO SISTEMA BATMAN SMART REWARDS\n",
      "============================================================\n",
      "ğŸ“Š PARÃ‚METROS CONFIGURADOS:\n",
      "   ACTION_BONUS        : 5\n",
      "   INACTION_PENALTY    : -2\n",
      "   FAILURE_PENALTY     : -10\n",
      "   TIMING_BONUS        : 20\n",
      "   EXPLORATION_BONUS   : 1\n",
      "   HOLD_TOLERANCE      : 3\n",
      "   ENABLED             : True\n",
      "\n",
      "ğŸ’¡ PARA AJUSTAR:\n",
      "   â€¢ Modifique os valores acima e re-execute as cÃ©lulas seguintes\n",
      "   â€¢ Set ENABLED=False para voltar ao sistema original\n",
      "   â€¢ Experimente diferentes combinaÃ§Ãµes para otimizar aprendizagem\n"
     ]
    }
   ],
   "source": [
    "# âš™ï¸ CONFIGURAÃ‡ÃƒO DO SISTEMA DE REWARDS MELHORADO\n",
    "print(\"âš™ï¸ CONFIGURANDO SISTEMA BATMAN SMART REWARDS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ParÃ¢metros do novo sistema de rewards (configurÃ¡veis)\n",
    "SMART_REWARDS_CONFIG = {\n",
    "    'ACTION_BONUS': 5,          # BÃ´nus por executar BUY/SELL com sucesso\n",
    "    'INACTION_PENALTY': -2,     # Penalidade por HOLD excessivo\n",
    "    'FAILURE_PENALTY': -10,     # Penalidade por tentar aÃ§Ã£o impossÃ­vel\n",
    "    'TIMING_BONUS': 20,         # BÃ´nus por bom timing (comprar baixo, vender alto)\n",
    "    'EXPLORATION_BONUS': 1,     # BÃ´nus por visitar estado novo\n",
    "    'HOLD_TOLERANCE': 3,        # MÃ¡ximo de HOLDs seguidos sem penalidade\n",
    "    'ENABLED': True             # Ativar/desativar sistema melhorado\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š PARÃ‚METROS CONFIGURADOS:\")\n",
    "for param, value in SMART_REWARDS_CONFIG.items():\n",
    "    print(f\"   {param:20s}: {value}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ PARA AJUSTAR:\")\n",
    "print(\"   â€¢ Modifique os valores acima e re-execute as cÃ©lulas seguintes\")\n",
    "print(\"   â€¢ Set ENABLED=False para voltar ao sistema original\")\n",
    "print(\"   â€¢ Experimente diferentes combinaÃ§Ãµes para otimizar aprendizagem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ced8578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Classe BatmanSmartTradingEnvironment implementada!\n",
      "   ğŸ“Š Suporte a rewards configurÃ¡veis\n",
      "   ğŸ¯ Tracking de exploraÃ§Ã£o automÃ¡tico\n",
      "   ğŸ“ˆ Componentes de reward detalhados\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ AMBIENTE BATMAN COM SMART REWARDS IMPLEMENTADO\n",
    "class BatmanSmartTradingEnvironment(BatmanTradingEnvironment):\n",
    "    \"\"\"\n",
    "    VersÃ£o melhorada do ambiente Batman com sistema de rewards inteligente\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, prices, state_manager, initial_capital=10000.0, smart_config=None):\n",
    "        # Inicializar atributos antes de chamar super().__init__()\n",
    "        self.smart_config = smart_config or SMART_REWARDS_CONFIG\n",
    "        self.visited_states = set()\n",
    "        self.consecutive_holds = 0\n",
    "        self.previous_price = None\n",
    "        \n",
    "        # Agora chamar o construtor pai\n",
    "        super().__init__(prices, state_manager, initial_capital)\n",
    "        \n",
    "        print(\"ğŸš€ Batman Smart Trading Environment inicializado!\")\n",
    "        if self.smart_config['ENABLED']:\n",
    "            print(\"   âœ… Smart Rewards: ATIVADO\")\n",
    "            print(f\"   ğŸ¯ Action Bonus: {self.smart_config['ACTION_BONUS']}\")\n",
    "            print(f\"   âš ï¸ Failure Penalty: {self.smart_config['FAILURE_PENALTY']}\")\n",
    "            print(f\"   ğŸ•°ï¸ Timing Bonus: {self.smart_config['TIMING_BONUS']}\")\n",
    "        else:\n",
    "            print(\"   âšª Smart Rewards: DESATIVADO (modo clÃ¡ssico)\")\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia ambiente com tracking de smart rewards\"\"\"\n",
    "        state = super().reset()\n",
    "        \n",
    "        # Reset smart rewards tracking\n",
    "        self.visited_states.clear()\n",
    "        self.consecutive_holds = 0\n",
    "        self.previous_price = self.prices[self.current_step] if len(self.prices) > self.current_step else None\n",
    "        \n",
    "        # Marcar estado inicial como visitado\n",
    "        self.visited_states.add(state)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def calculate_smart_reward(self, base_reward, action, action_executed, current_price, state):\n",
    "        \"\"\"Calcula reward usando sistema inteligente\"\"\"\n",
    "        \n",
    "        if not self.smart_config['ENABLED']:\n",
    "            return base_reward\n",
    "            \n",
    "        smart_reward = base_reward  # ComeÃ§a com reward base (mudanÃ§a portfolio)\n",
    "        reward_components = {'base': base_reward}\n",
    "        \n",
    "        # 1. BÃ´nus por executar aÃ§Ã£o (incentiva trading ativo)\n",
    "        if action != Actions.HOLD and action_executed:\n",
    "            action_bonus = self.smart_config['ACTION_BONUS']\n",
    "            smart_reward += action_bonus\n",
    "            reward_components['action_bonus'] = action_bonus\n",
    "        \n",
    "        # 2. Penalidade por falha (aprende restriÃ§Ãµes)\n",
    "        if action != Actions.HOLD and not action_executed:\n",
    "            failure_penalty = self.smart_config['FAILURE_PENALTY']\n",
    "            smart_reward += failure_penalty  # Penalty Ã© negativo\n",
    "            reward_components['failure_penalty'] = failure_penalty\n",
    "        \n",
    "        # 3. Penalidade por inaÃ§Ã£o excessiva\n",
    "        if action == Actions.HOLD:\n",
    "            self.consecutive_holds += 1\n",
    "            if self.consecutive_holds > self.smart_config['HOLD_TOLERANCE']:\n",
    "                inaction_penalty = self.smart_config['INACTION_PENALTY']\n",
    "                smart_reward += inaction_penalty  # Penalty Ã© negativo\n",
    "                reward_components['inaction_penalty'] = inaction_penalty\n",
    "        else:\n",
    "            self.consecutive_holds = 0\n",
    "        \n",
    "        # 4. BÃ´nus de timing (premia boas decisÃµes)\n",
    "        if self.previous_price is not None and action_executed:\n",
    "            price_change = (current_price - self.previous_price) / self.previous_price\n",
    "            \n",
    "            # Comprou e preÃ§o subiu = bom timing\n",
    "            if action == Actions.BUY and price_change > 0:\n",
    "                timing_bonus = self.smart_config['TIMING_BONUS']\n",
    "                smart_reward += timing_bonus\n",
    "                reward_components['timing_bonus'] = timing_bonus\n",
    "                \n",
    "            # Vendeu antes da alta = bom timing (conservador)\n",
    "            elif action == Actions.SELL and price_change > 0.01:  # 1% de alta\n",
    "                timing_bonus = self.smart_config['TIMING_BONUS']\n",
    "                smart_reward += timing_bonus\n",
    "                reward_components['timing_bonus'] = timing_bonus\n",
    "        \n",
    "        # 5. BÃ´nus de exploraÃ§Ã£o (incentiva visitar novos estados)\n",
    "        if state not in self.visited_states:\n",
    "            exploration_bonus = self.smart_config['EXPLORATION_BONUS']\n",
    "            smart_reward += exploration_bonus\n",
    "            reward_components['exploration_bonus'] = exploration_bonus\n",
    "            self.visited_states.add(state)\n",
    "        \n",
    "        # Atualizar preÃ§o anterior\n",
    "        self.previous_price = current_price\n",
    "        \n",
    "        return smart_reward, reward_components\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Executa aÃ§Ã£o com sistema de rewards inteligente\"\"\"\n",
    "        current_price = self.prices[self.current_step]\n",
    "        portfolio_value_before = self.get_portfolio_value()\n",
    "        \n",
    "        # Executar aÃ§Ã£o (mesmo cÃ³digo do ambiente original)\n",
    "        action_executed = False\n",
    "        if action == Actions.BUY and self.cash >= current_price:\n",
    "            self.shares += 1\n",
    "            self.cash -= current_price\n",
    "            action_executed = True\n",
    "            \n",
    "        elif action == Actions.SELL and self.shares > 0:\n",
    "            self.shares -= 1\n",
    "            self.cash += current_price\n",
    "            action_executed = True\n",
    "            \n",
    "        # HOLD sempre Ã© vÃ¡lido\n",
    "        if action == Actions.HOLD:\n",
    "            action_executed = True\n",
    "        \n",
    "        # Calcular reward base (mudanÃ§a no portfolio)\n",
    "        portfolio_value_after = self.get_portfolio_value()\n",
    "        base_reward = portfolio_value_after - portfolio_value_before\n",
    "        \n",
    "        # Estado atual para anÃ¡lise de exploraÃ§Ã£o\n",
    "        current_state = self.get_current_state()\n",
    "        \n",
    "        # Aplicar smart rewards\n",
    "        smart_reward, reward_components = self.calculate_smart_reward(\n",
    "            base_reward, action, action_executed, current_price, current_state\n",
    "        )\n",
    "        \n",
    "        # Registrar histÃ³rico (usar smart reward)\n",
    "        self.portfolio_values.append(portfolio_value_after)\n",
    "        self.actions_history.append(action)\n",
    "        self.episode_rewards.append(smart_reward)\n",
    "        \n",
    "        # AvanÃ§ar para prÃ³ximo dia\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.prices) - 1\n",
    "        \n",
    "        # PrÃ³ximo estado\n",
    "        next_state = self.get_current_state() if not done else None\n",
    "        \n",
    "        # InformaÃ§Ãµes detalhadas (incluindo componentes do reward)\n",
    "        info = {\n",
    "            'cash': self.cash,\n",
    "            'shares': self.shares, \n",
    "            'portfolio_value': portfolio_value_after,\n",
    "            'current_price': current_price,\n",
    "            'action_executed': action_executed,\n",
    "            'day': self.current_step,\n",
    "            'base_reward': base_reward,\n",
    "            'smart_reward': smart_reward,\n",
    "            'reward_components': reward_components,\n",
    "            'consecutive_holds': self.consecutive_holds,\n",
    "            'states_explored': len(self.visited_states)\n",
    "        }\n",
    "        \n",
    "        return next_state, smart_reward, done, info\n",
    "\n",
    "print(\"ğŸ”§ Classe BatmanSmartTradingEnvironment implementada!\")\n",
    "print(\"   ğŸ“Š Suporte a rewards configurÃ¡veis\")  \n",
    "print(\"   ğŸ¯ Tracking de exploraÃ§Ã£o automÃ¡tico\")\n",
    "print(\"   ğŸ“ˆ Componentes de reward detalhados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ece7caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª TESTANDO BATMAN SMART REWARDS\n",
      "============================================================\n",
      "ğŸš€ Batman Smart Trading Environment inicializado!\n",
      "   âœ… Smart Rewards: ATIVADO\n",
      "   ğŸ¯ Action Bonus: 5\n",
      "   âš ï¸ Failure Penalty: -10\n",
      "   ğŸ•°ï¸ Timing Bonus: 20\n",
      "\n",
      "ğŸ® TESTE COMPARATIVO: ClÃ¡ssico vs Smart\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“Š CenÃ¡rio: Compra bem-sucedida\n",
      "   ClÃ¡ssico: Reward =   +0.00 | Executada = True\n",
      "   Smart:    Reward =   +5.00 | Executada = True\n",
      "   Componentes Smart: {'base': np.float64(0.0), 'action_bonus': 5}\n",
      "   DiferenÃ§a:   +5.00 (+melhor)\n",
      "\n",
      "ğŸ“Š CenÃ¡rio: Tentativa de compra sem cash\n",
      "   ClÃ¡ssico: Reward =   +0.00 | Executada = True\n",
      "   Smart:    Reward =   +5.00 | Executada = True\n",
      "   Componentes Smart: {'base': np.float64(0.0), 'action_bonus': 5}\n",
      "   DiferenÃ§a:   +5.00 (+melhor)\n",
      "\n",
      "ğŸ“Š CenÃ¡rio: HOLD normal\n",
      "   ClÃ¡ssico: Reward =   +0.00 | Executada = True\n",
      "   Smart:    Reward =   +0.00 | Executada = True\n",
      "   Componentes Smart: {'base': np.float64(0.0)}\n",
      "   DiferenÃ§a:   +0.00 (igual)\n",
      "\n",
      "ğŸ“Š CenÃ¡rio: Venda bem-sucedida\n",
      "   ClÃ¡ssico: Reward =   +0.00 | Executada = True\n",
      "   Smart:    Reward =   +5.00 | Executada = True\n",
      "   Componentes Smart: {'base': np.float64(0.0), 'action_bonus': 5}\n",
      "   DiferenÃ§a:   +5.00 (+melhor)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª TESTE DO NOVO SISTEMA SMART REWARDS\n",
    "print(\"ğŸ§ª TESTANDO BATMAN SMART REWARDS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Criar ambiente com smart rewards\n",
    "if df_stock is not None:\n",
    "    smart_env = BatmanSmartTradingEnvironment(prices, state_manager, INITIAL_CAPITAL, SMART_REWARDS_CONFIG)\n",
    "    \n",
    "    print(f\"\\nğŸ® TESTE COMPARATIVO: ClÃ¡ssico vs Smart\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # CenÃ¡rios de teste\n",
    "    test_scenarios = [\n",
    "        {\"name\": \"Compra bem-sucedida\", \"action\": Actions.BUY},\n",
    "        {\"name\": \"Tentativa de compra sem cash\", \"action\": Actions.BUY, \"low_cash\": True},\n",
    "        {\"name\": \"HOLD normal\", \"action\": Actions.HOLD},\n",
    "        {\"name\": \"Venda bem-sucedida\", \"action\": Actions.SELL, \"setup_shares\": True}\n",
    "    ]\n",
    "    \n",
    "    for scenario in test_scenarios:\n",
    "        print(f\"\\nğŸ“Š CenÃ¡rio: {scenario['name']}\")\n",
    "        \n",
    "        # Ambiente clÃ¡ssico\n",
    "        classic_env = BatmanTradingEnvironment(prices, state_manager, INITIAL_CAPITAL)\n",
    "        classic_state = classic_env.reset()\n",
    "        \n",
    "        # Ambiente smart  \n",
    "        smart_state = smart_env.reset()\n",
    "        \n",
    "        # Setup especial para cenÃ¡rios\n",
    "        if scenario.get('low_cash'):\n",
    "            classic_env.cash = 10.0  # Pouco cash\n",
    "            smart_env.cash = 10.0\n",
    "            \n",
    "        if scenario.get('setup_shares'):\n",
    "            classic_env.shares = 1  # Ter shares para vender\n",
    "            smart_env.shares = 1\n",
    "            classic_env.cash -= classic_env.prices[classic_env.current_step]\n",
    "            smart_env.cash -= smart_env.prices[smart_env.current_step]\n",
    "        \n",
    "        # Executar aÃ§Ã£o em ambos\n",
    "        classic_next, classic_reward, classic_done, classic_info = classic_env.step(scenario['action'])\n",
    "        smart_next, smart_reward, smart_done, smart_info = smart_env.step(scenario['action'])\n",
    "        \n",
    "        # Comparar resultados\n",
    "        print(f\"   ClÃ¡ssico: Reward = {classic_reward:+7.2f} | Executada = {classic_info['action_executed']}\")\n",
    "        print(f\"   Smart:    Reward = {smart_reward:+7.2f} | Executada = {smart_info['action_executed']}\")\n",
    "        \n",
    "        if 'reward_components' in smart_info:\n",
    "            components = smart_info['reward_components']\n",
    "            print(f\"   Componentes Smart: {components}\")\n",
    "        \n",
    "        difference = smart_reward - classic_reward\n",
    "        print(f\"   DiferenÃ§a: {difference:+7.2f} ({'+' if difference > 0 else ''}{'melhor' if difference != 0 else 'igual'})\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Dados nÃ£o disponÃ­veis para teste!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1798a9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦‡ Batman Smart Q-Learning Agent implementado!\n",
      "   ğŸ“Š Tracking detalhado de componentes de reward\n",
      "   ğŸ¯ EstatÃ­sticas de exploraÃ§Ã£o automÃ¡ticas\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ AGENTE BATMAN ATUALIZADO PARA SMART REWARDS\n",
    "class BatmanSmartQLearningAgent(BatmanQLearningAgent):\n",
    "    \"\"\"\n",
    "    VersÃ£o do agente Batman otimizada para Smart Rewards\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.95, \n",
    "                 epsilon_start=1.0, epsilon_min=0.01, epsilon_decay=0.995, smart_config=None):\n",
    "        super().__init__(learning_rate, discount_factor, epsilon_start, epsilon_min, epsilon_decay)\n",
    "        self.smart_config = smart_config or SMART_REWARDS_CONFIG\n",
    "        \n",
    "        # EstatÃ­sticas especÃ­ficas do smart rewards\n",
    "        self.reward_component_history = []\n",
    "        self.exploration_stats = []\n",
    "        \n",
    "        print(\"ğŸš€ Batman Smart Q-Learning Agent inicializado!\")\n",
    "        print(f\"   ğŸ§  Otimizado para sistema de rewards inteligente\")\n",
    "        if self.smart_config['ENABLED']:\n",
    "            print(f\"   âœ… Smart Rewards ativo\")\n",
    "        else:\n",
    "            print(f\"   âšª Modo clÃ¡ssico\")\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        \"\"\"Treina episÃ³dio com tracking de smart rewards\"\"\"\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        exploration_count = 0\n",
    "        episode_components = {'action_bonus': 0, 'timing_bonus': 0, 'exploration_bonus': 0, \n",
    "                            'failure_penalty': 0, 'inaction_penalty': 0}\n",
    "        \n",
    "        while True:\n",
    "            # Escolher aÃ§Ã£o\n",
    "            action, is_exploration = self.get_action(state, training=True)\n",
    "            if is_exploration:\n",
    "                exploration_count += 1\n",
    "                \n",
    "            # Executar aÃ§Ã£o\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Tracking de componentes smart rewards\n",
    "            if 'reward_components' in info:\n",
    "                for component, value in info['reward_components'].items():\n",
    "                    if component in episode_components:\n",
    "                        episode_components[component] += value\n",
    "            \n",
    "            # Atualizar Q-value\n",
    "            self.update_q_value(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Acumular recompensa\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "        \n",
    "        # Decair epsilon\n",
    "        self.decay_epsilon()\n",
    "        \n",
    "        # Registrar estatÃ­sticas\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        self.exploration_counts.append(exploration_count)\n",
    "        self.reward_component_history.append(episode_components.copy())\n",
    "        \n",
    "        # Calcular retorno do episÃ³dio\n",
    "        episode_summary = env.get_episode_summary()\n",
    "        if episode_summary:\n",
    "            self.episode_returns.append(episode_summary['total_return'])\n",
    "            \n",
    "        # EstatÃ­sticas de exploraÃ§Ã£o (se ambiente suporta)\n",
    "        if hasattr(env, 'visited_states'):\n",
    "            self.exploration_stats.append(len(env.visited_states))\n",
    "        \n",
    "        return episode_summary\n",
    "    \n",
    "    def get_smart_training_stats(self):\n",
    "        \"\"\"Retorna estatÃ­sticas completas incluindo componentes smart\"\"\"\n",
    "        base_stats = self.get_training_stats()\n",
    "        \n",
    "        if not base_stats or not self.reward_component_history:\n",
    "            return base_stats\n",
    "            \n",
    "        # EstatÃ­sticas dos Ãºltimos 100 episÃ³dios\n",
    "        recent_components = self.reward_component_history[-100:]\n",
    "        recent_exploration = self.exploration_stats[-100:] if self.exploration_stats else []\n",
    "        \n",
    "        smart_stats = {\n",
    "            'avg_action_bonus': np.mean([ep['action_bonus'] for ep in recent_components]),\n",
    "            'avg_timing_bonus': np.mean([ep['timing_bonus'] for ep in recent_components]),\n",
    "            'avg_exploration_bonus': np.mean([ep['exploration_bonus'] for ep in recent_components]),\n",
    "            'avg_failure_penalty': np.mean([ep['failure_penalty'] for ep in recent_components]),\n",
    "            'avg_inaction_penalty': np.mean([ep['inaction_penalty'] for ep in recent_components]),\n",
    "            'avg_states_per_episode': np.mean(recent_exploration) if recent_exploration else 0\n",
    "        }\n",
    "        \n",
    "        # Combinar com estatÃ­sticas base\n",
    "        base_stats.update(smart_stats)\n",
    "        return base_stats\n",
    "\n",
    "print(\"ğŸ¦‡ Batman Smart Q-Learning Agent implementado!\")\n",
    "print(\"   ğŸ“Š Tracking detalhado de componentes de reward\")\n",
    "print(\"   ğŸ¯ EstatÃ­sticas de exploraÃ§Ã£o automÃ¡ticas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27bf4fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ INICIALIZANDO SISTEMA BATMAN SMART COMPLETO\n",
      "============================================================\n",
      "ğŸš€ Batman Smart Trading Environment inicializado!\n",
      "   âœ… Smart Rewards: ATIVADO\n",
      "   ğŸ¯ Action Bonus: 5\n",
      "   âš ï¸ Failure Penalty: -10\n",
      "   ğŸ•°ï¸ Timing Bonus: 20\n",
      "ğŸ¦‡ Agente Batman Q-Learning inicializado!\n",
      "   ğŸ§  Learning rate: 0.1\n",
      "   ğŸ’° Discount factor: 0.95\n",
      "   ğŸ” Epsilon inicial: 1.0\n",
      "ğŸš€ Batman Smart Q-Learning Agent inicializado!\n",
      "   ğŸ§  Otimizado para sistema de rewards inteligente\n",
      "   âœ… Smart Rewards ativo\n",
      "\n",
      "âœ… SISTEMA SMART INICIALIZADO:\n",
      "   ğŸ›ï¸ Ambiente: Batman Smart Trading Environment\n",
      "   ğŸ¦‡ Agente: Batman Smart Q-Learning Agent\n",
      "   ğŸ’° Capital: R$ 50,000.00\n",
      "   ğŸ“Š Ativo: PETR3.SA\n",
      "   ğŸ¯ Smart Rewards: ATIVO\n",
      "\n",
      "ğŸ“ˆ CONFIGURAÃ‡ÃƒO SMART REWARDS:\n",
      "   Action Bonus: 5\n",
      "   Timing Bonus: 20\n",
      "   Exploration Bonus: 1\n",
      "   Failure Penalty: -10\n",
      "   Inaction Penalty: -2\n",
      "   Hold Tolerance: 3\n",
      "\n",
      "ğŸ® PRONTO PARA TREINAMENTO!\n",
      "   Use: train_batman_smart_agent(smart_agent, smart_env, NUM_EPISODES)\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ INICIALIZAÃ‡ÃƒO DO SISTEMA SMART COMPLETO\n",
    "print(\"ğŸš€ INICIALIZANDO SISTEMA BATMAN SMART COMPLETO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Criar ambiente e agente com smart rewards\n",
    "if df_stock is not None:\n",
    "    # Ambiente smart\n",
    "    smart_env = BatmanSmartTradingEnvironment(prices, state_manager, INITIAL_CAPITAL, SMART_REWARDS_CONFIG)\n",
    "    \n",
    "    # Agente smart\n",
    "    smart_agent = BatmanSmartQLearningAgent(\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        discount_factor=DISCOUNT_FACTOR,\n",
    "        epsilon_start=EPSILON_START,\n",
    "        epsilon_min=EPSILON_MIN,\n",
    "        epsilon_decay=EPSILON_DECAY,\n",
    "        smart_config=SMART_REWARDS_CONFIG\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… SISTEMA SMART INICIALIZADO:\")\n",
    "    print(f\"   ğŸ›ï¸ Ambiente: Batman Smart Trading Environment\")\n",
    "    print(f\"   ğŸ¦‡ Agente: Batman Smart Q-Learning Agent\")\n",
    "    print(f\"   ğŸ’° Capital: R$ {INITIAL_CAPITAL:,.2f}\")\n",
    "    print(f\"   ğŸ“Š Ativo: {TICKER_SYMBOL}\")\n",
    "    print(f\"   ğŸ¯ Smart Rewards: {'ATIVO' if SMART_REWARDS_CONFIG['ENABLED'] else 'INATIVO'}\")\n",
    "    \n",
    "    if SMART_REWARDS_CONFIG['ENABLED']:\n",
    "        print(f\"\\nğŸ“ˆ CONFIGURAÃ‡ÃƒO SMART REWARDS:\")\n",
    "        print(f\"   Action Bonus: {SMART_REWARDS_CONFIG['ACTION_BONUS']}\")\n",
    "        print(f\"   Timing Bonus: {SMART_REWARDS_CONFIG['TIMING_BONUS']}\")\n",
    "        print(f\"   Exploration Bonus: {SMART_REWARDS_CONFIG['EXPLORATION_BONUS']}\")\n",
    "        print(f\"   Failure Penalty: {SMART_REWARDS_CONFIG['FAILURE_PENALTY']}\")\n",
    "        print(f\"   Inaction Penalty: {SMART_REWARDS_CONFIG['INACTION_PENALTY']}\")\n",
    "        print(f\"   Hold Tolerance: {SMART_REWARDS_CONFIG['HOLD_TOLERANCE']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ® PRONTO PARA TREINAMENTO!\")\n",
    "    print(f\"   Use: train_batman_smart_agent(smart_agent, smart_env, NUM_EPISODES)\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Erro: Dados nÃ£o disponÃ­veis para inicializaÃ§Ã£o!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "165eccd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‹ï¸ FunÃ§Ã£o de treinamento Batman Smart implementada!\n",
      "   ğŸ“Š Monitoramento completo de componentes smart\n",
      "   ğŸ¯ RelatÃ³rios detalhados de exploraÃ§Ã£o\n",
      "   ğŸ“ˆ Tracking de todos os bÃ´nus e penalidades\n"
     ]
    }
   ],
   "source": [
    "# ğŸ‹ï¸ TREINAMENTO BATMAN SMART COM MONITORAMENTO AVANÃ‡ADO\n",
    "def train_batman_smart_agent(agent, env, num_episodes, print_every=100):\n",
    "    \"\"\"\n",
    "    Treina o agente Batman Smart com monitoramento detalhado de componentes\n",
    "    \"\"\"\n",
    "    print(f\"ğŸš€ INICIANDO TREINAMENTO BATMAN SMART\")\n",
    "    print(f\"ğŸ“Š {num_episodes} episÃ³dios | RelatÃ³rios a cada {print_every}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    training_history = {\n",
    "        'episode': [],\n",
    "        'avg_reward': [],\n",
    "        'avg_return': [],\n",
    "        'epsilon': [],\n",
    "        'q_table_size': [],\n",
    "        # Smart rewards especÃ­ficos\n",
    "        'avg_action_bonus': [],\n",
    "        'avg_timing_bonus': [],\n",
    "        'avg_exploration_bonus': [],\n",
    "        'avg_failure_penalty': [],\n",
    "        'avg_inaction_penalty': [],\n",
    "        'avg_states_explored': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        # Treinar episÃ³dio\n",
    "        episode_summary = agent.train_episode(env)\n",
    "        \n",
    "        # RelatÃ³rio periÃ³dico\n",
    "        if episode % print_every == 0:\n",
    "            stats = agent.get_smart_training_stats()\n",
    "            \n",
    "            print(f\"ğŸ“ˆ EpisÃ³dio {episode}/{num_episodes}\")\n",
    "            print(f\"   ğŸ’° Reward mÃ©dio (Ãºltimos 100): {stats['avg_reward']:+.2f}\")\n",
    "            print(f\"   ğŸ“Š Retorno mÃ©dio (Ãºltimos 100): {stats['avg_return']:+.2%}\")\n",
    "            print(f\"   ğŸ” Epsilon atual: {stats['current_epsilon']:.3f}\")\n",
    "            print(f\"   ğŸ§  Tamanho Q-table: {stats['q_table_size']:,}\")\n",
    "            print(f\"   ğŸ¯ ExploraÃ§Ã£o mÃ©dia: {stats['avg_exploration']:.1f} aÃ§Ãµes/episÃ³dio\")\n",
    "            \n",
    "            # Componentes smart rewards\n",
    "            if SMART_REWARDS_CONFIG['ENABLED']:\n",
    "                print(f\"   ğŸš€ SMART REWARDS MÃ‰DIOS:\")\n",
    "                print(f\"      Action Bonus: {stats['avg_action_bonus']:+.1f}\")\n",
    "                print(f\"      Timing Bonus: {stats['avg_timing_bonus']:+.1f}\")\n",
    "                print(f\"      Exploration: {stats['avg_exploration_bonus']:+.1f}\")\n",
    "                print(f\"      Failure Penalty: {stats['avg_failure_penalty']:+.1f}\")\n",
    "                print(f\"      Inaction Penalty: {stats['avg_inaction_penalty']:+.1f}\")\n",
    "                print(f\"      Estados/EpisÃ³dio: {stats['avg_states_per_episode']:.1f}\")\n",
    "            \n",
    "            if episode_summary:\n",
    "                print(f\"   ğŸ’µ Ãšltimo episÃ³dio: R$ {episode_summary['final_value']:,.2f} \" + \n",
    "                      f\"({episode_summary['total_return']:+.2%})\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Salvar histÃ³rico\n",
    "            training_history['episode'].append(episode)\n",
    "            training_history['avg_reward'].append(stats['avg_reward'])\n",
    "            training_history['avg_return'].append(stats['avg_return'])\n",
    "            training_history['epsilon'].append(stats['current_epsilon'])\n",
    "            training_history['q_table_size'].append(stats['q_table_size'])\n",
    "            \n",
    "            # Smart rewards\n",
    "            if SMART_REWARDS_CONFIG['ENABLED']:\n",
    "                training_history['avg_action_bonus'].append(stats['avg_action_bonus'])\n",
    "                training_history['avg_timing_bonus'].append(stats['avg_timing_bonus'])\n",
    "                training_history['avg_exploration_bonus'].append(stats['avg_exploration_bonus'])\n",
    "                training_history['avg_failure_penalty'].append(stats['avg_failure_penalty'])\n",
    "                training_history['avg_inaction_penalty'].append(stats['avg_inaction_penalty'])\n",
    "                training_history['avg_states_explored'].append(stats['avg_states_per_episode'])\n",
    "    \n",
    "    print(\"âœ… TREINAMENTO BATMAN SMART CONCLUÃDO!\")\n",
    "    final_stats = agent.get_smart_training_stats()\n",
    "    print(f\"ğŸ“Š ESTATÃSTICAS FINAIS:\")\n",
    "    print(f\"   ğŸ§  Q-table final: {final_stats['q_table_size']:,} estados\")\n",
    "    print(f\"   ğŸ¯ Epsilon final: {final_stats['current_epsilon']:.3f}\")\n",
    "    print(f\"   ğŸ’° Reward mÃ©dio final: {final_stats['avg_reward']:+.2f}\")\n",
    "    print(f\"   ğŸ“ˆ Retorno mÃ©dio final: {final_stats['avg_return']:+.2%}\")\n",
    "    \n",
    "    if SMART_REWARDS_CONFIG['ENABLED']:\n",
    "        print(f\"   ğŸš€ SMART REWARDS FINAIS:\")\n",
    "        print(f\"      Total Action Bonus: {final_stats['avg_action_bonus']:+.1f}\")\n",
    "        print(f\"      Total Timing Bonus: {final_stats['avg_timing_bonus']:+.1f}\")\n",
    "        print(f\"      Estados explorados: {final_stats['avg_states_per_episode']:.1f}/episÃ³dio\")\n",
    "    \n",
    "    return training_history\n",
    "\n",
    "print(\"ğŸ‹ï¸ FunÃ§Ã£o de treinamento Batman Smart implementada!\")\n",
    "print(\"   ğŸ“Š Monitoramento completo de componentes smart\")\n",
    "print(\"   ğŸ¯ RelatÃ³rios detalhados de exploraÃ§Ã£o\")\n",
    "print(\"   ğŸ“ˆ Tracking de todos os bÃ´nus e penalidades\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49878a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ® EXECUTANDO TREINAMENTO BATMAN SMART\n",
      "============================================================\n",
      "ğŸš€ Iniciando treinamento para PETR3.SA com Smart Rewards\n",
      "   ğŸ“Š EpisÃ³dios: 1000\n",
      "   ğŸ¯ Sistema: Smart\n",
      "ğŸš€ INICIANDO TREINAMENTO BATMAN SMART\n",
      "ğŸ“Š 1000 episÃ³dios | RelatÃ³rios a cada 200\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ğŸ® EXECUTAR TREINAMENTO BATMAN SMART\n",
    "print(\"ğŸ® EXECUTANDO TREINAMENTO BATMAN SMART\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Executar treinamento com sistema smart\n",
    "if 'smart_env' in locals() and 'smart_agent' in locals():\n",
    "    print(f\"ğŸš€ Iniciando treinamento para {TICKER_SYMBOL} com Smart Rewards\")\n",
    "    print(f\"   ğŸ“Š EpisÃ³dios: {NUM_EPISODES}\")\n",
    "    print(f\"   ğŸ¯ Sistema: {'Smart' if SMART_REWARDS_CONFIG['ENABLED'] else 'ClÃ¡ssico'}\")\n",
    "    \n",
    "    # Treinamento\n",
    "    smart_training_history = train_batman_smart_agent(smart_agent, smart_env, NUM_EPISODES, print_every=200)\n",
    "    \n",
    "    print(f\"\\nğŸ† TREINAMENTO CONCLUÃDO!\")\n",
    "    print(f\"   Use as cÃ©lulas de avaliaÃ§Ã£o para ver os resultados\")\n",
    "    print(f\"   Ou ajuste os parÃ¢metros em SMART_REWARDS_CONFIG para otimizar\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Execute primeiro as cÃ©lulas de inicializaÃ§Ã£o do sistema smart!\")\n",
    "\n",
    "# ğŸ’¡ InstruÃ§Ãµes para experimentar\n",
    "print(f\"\\nğŸ’¡ PARA EXPERIMENTAR DIFERENTES CONFIGURAÃ‡Ã•ES:\")\n",
    "print(f\"   1. Modifique os valores em SMART_REWARDS_CONFIG\")\n",
    "print(f\"   2. Re-execute as cÃ©lulas de inicializaÃ§Ã£o\")\n",
    "print(f\"   3. Execute novamente o treinamento\")\n",
    "print(f\"   4. Compare os resultados\")\n",
    "\n",
    "print(f\"\\nğŸ”§ EXEMPLOS DE AJUSTES:\")\n",
    "print(f\"   â€¢ Aumentar ACTION_BONUS para mais trading\")\n",
    "print(f\"   â€¢ Aumentar TIMING_BONUS para melhor timing\")  \n",
    "print(f\"   â€¢ Reduzir FAILURE_PENALTY para menos puniÃ§Ã£o\")\n",
    "print(f\"   â€¢ Aumentar EXPLORATION_BONUS para mais exploraÃ§Ã£o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9dd25f",
   "metadata": {},
   "source": [
    "## ğŸ“Š AVALIAÃ‡ÃƒO E RESULTADOS BATMAN\n",
    "\n",
    "Teste do agente treinado e anÃ¡lise de performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8148ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š AvaliaÃ§Ã£o Completa do Agente Batman\n",
    "def evaluate_batman_agent(agent, env, num_test_episodes=10):\n",
    "    \"\"\"\n",
    "    Avalia o agente treinado em mÃºltiplos episÃ³dios de teste\n",
    "    \"\"\"\n",
    "    print(\"ğŸ§ª Avaliando agente Batman...\")\n",
    "    test_results = []\n",
    "    \n",
    "    for test_ep in range(num_test_episodes):\n",
    "        # Executar episÃ³dio de teste (sem exploraÃ§Ã£o)\n",
    "        state = env.reset()\n",
    "        episode_actions = []\n",
    "        episode_rewards = []\n",
    "        \n",
    "        while True:\n",
    "            # Usar apenas exploitaÃ§Ã£o (greedy policy)\n",
    "            action, _ = agent.get_action(state, training=False)\n",
    "            episode_actions.append(action)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        \n",
    "        # Registrar resultado do teste\n",
    "        episode_summary = env.get_episode_summary()\n",
    "        if episode_summary:\n",
    "            test_results.append({\n",
    "                'episode': test_ep + 1,\n",
    "                'final_value': episode_summary['final_value'],\n",
    "                'total_return': episode_summary['total_return'],\n",
    "                'total_reward': episode_summary['total_reward'],\n",
    "                'actions_taken': episode_summary['actions_taken'],\n",
    "                'num_days': episode_summary['num_days']\n",
    "            })\n",
    "    \n",
    "    # AnÃ¡lise dos resultados\n",
    "    if test_results:\n",
    "        avg_return = np.mean([r['total_return'] for r in test_results])\n",
    "        avg_final_value = np.mean([r['final_value'] for r in test_results])\n",
    "        avg_actions = np.mean([r['actions_taken'] for r in test_results])\n",
    "        win_rate = len([r for r in test_results if r['total_return'] > 0]) / len(test_results)\n",
    "        \n",
    "        print(f\"ğŸ“ˆ Resultados da AvaliaÃ§Ã£o Batman ({num_test_episodes} episÃ³dios):\")\n",
    "        print(f\"   ğŸ’° Valor final mÃ©dio: R$ {avg_final_value:,.2f}\")\n",
    "        print(f\"   ğŸ“Š Retorno mÃ©dio: {avg_return:+.2%}\")\n",
    "        print(f\"   ğŸ¯ Taxa de sucesso: {win_rate:.1%}\")\n",
    "        print(f\"   ğŸ”„ AÃ§Ãµes mÃ©dias por episÃ³dio: {avg_actions:.1f}\")\n",
    "        print(f\"   ğŸ“… Dias de trading: {test_results[0]['num_days']}\")\n",
    "        \n",
    "        # Comparar com Buy & Hold\n",
    "        buy_hold_return = (prices[-1] - prices[env.state_manager.window_size]) / prices[env.state_manager.window_size]\n",
    "        print(f\"   ğŸ“ˆ Buy & Hold: {buy_hold_return:+.2%}\")\n",
    "        print(f\"   ğŸ† Alpha vs B&H: {avg_return - buy_hold_return:+.2%}\")\n",
    "        \n",
    "        return {\n",
    "            'avg_return': avg_return,\n",
    "            'avg_final_value': avg_final_value,\n",
    "            'win_rate': win_rate,\n",
    "            'buy_hold_return': buy_hold_return,\n",
    "            'alpha': avg_return - buy_hold_return,\n",
    "            'test_results': test_results\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Criar grÃ¡ficos de anÃ¡lise\n",
    "def plot_batman_results(training_history, evaluation_results):\n",
    "    \"\"\"\n",
    "    Cria grÃ¡ficos de anÃ¡lise dos resultados Batman\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'ğŸ¦‡ Batman Q-Learning Results - {TICKER_SYMBOL}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. EvoluÃ§Ã£o do treinamento\n",
    "    axes[0,0].plot(training_history['episode'], training_history['avg_return'], 'b-', linewidth=2)\n",
    "    axes[0,0].set_title('EvoluÃ§Ã£o do Retorno (Treinamento)')\n",
    "    axes[0,0].set_xlabel('EpisÃ³dio')\n",
    "    axes[0,0].set_ylabel('Retorno MÃ©dio')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    axes[0,0].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # 2. EvoluÃ§Ã£o do epsilon\n",
    "    axes[0,1].plot(training_history['episode'], training_history['epsilon'], 'g-', linewidth=2)\n",
    "    axes[0,1].set_title('Decaimento da ExploraÃ§Ã£o (Epsilon)')\n",
    "    axes[0,1].set_xlabel('EpisÃ³dio')\n",
    "    axes[0,1].set_ylabel('Epsilon')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Crescimento da Q-table\n",
    "    axes[1,0].plot(training_history['episode'], training_history['q_table_size'], 'purple', linewidth=2)\n",
    "    axes[1,0].set_title('Crescimento da Q-Table')\n",
    "    axes[1,0].set_xlabel('EpisÃ³dio')\n",
    "    axes[1,0].set_ylabel('NÃºmero de Estados')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. ComparaÃ§Ã£o de performance\n",
    "    if evaluation_results:\n",
    "        methods = ['Batman RL', 'Buy & Hold']\n",
    "        returns = [evaluation_results['avg_return'], evaluation_results['buy_hold_return']]\n",
    "        colors = ['blue', 'orange']\n",
    "        \n",
    "        bars = axes[1,1].bar(methods, returns, color=colors, alpha=0.7)\n",
    "        axes[1,1].set_title('ComparaÃ§Ã£o de Performance')\n",
    "        axes[1,1].set_ylabel('Retorno')\n",
    "        axes[1,1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Adicionar valores nas barras\n",
    "        for bar, return_val in zip(bars, returns):\n",
    "            height = bar.get_height()\n",
    "            axes[1,1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                          f'{return_val:+.2%}',\n",
    "                          ha='center', va='bottom' if height > 0 else 'top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Executar avaliaÃ§Ã£o\n",
    "if df_stock is not None and 'training_history' in locals():\n",
    "    evaluation_results = evaluate_batman_agent(agent, env, num_test_episodes=20)\n",
    "    plot_batman_results(training_history, evaluation_results)\n",
    "else:\n",
    "    print(\"âš ï¸ Execute primeiro o treinamento para avaliar o agente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f429fd",
   "metadata": {},
   "source": [
    "## ğŸ”§ TESTE COM OUTRO ATIVO\n",
    "\n",
    "Para usar com outro ativo, simplesmente altere a variÃ¡vel `TICKER_SYMBOL` no inÃ­cio do notebook e execute novamente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1df1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ Exemplo: Como trocar para outro ativo\n",
    "\"\"\"\n",
    "Para testar com VALE3, por exemplo:\n",
    "\n",
    "1. Volte Ã  segunda cÃ©lula do notebook\n",
    "2. Altere: TICKER_SYMBOL = \"VALE3.SA\"  \n",
    "3. Execute novamente todas as cÃ©lulas\n",
    "\n",
    "O sistema automaticamente:\n",
    "âœ… BaixarÃ¡ os dados da VALE3\n",
    "âœ… ReconfigurarÃ¡ o ambiente\n",
    "âœ… TreinarÃ¡ o agente com os novos dados\n",
    "âœ… AvaliarÃ¡ a performance\n",
    "\n",
    "Ativos suportados (B3):\n",
    "- PETR3.SA, PETR4.SA (Petrobras)\n",
    "- VALE3.SA (Vale)\n",
    "- BRFS3.SA (BRF)\n",
    "- ITUB4.SA (ItaÃº)\n",
    "- BBAS3.SA (Banco do Brasil)\n",
    "- ABEV3.SA (Ambev)\n",
    "- E muitos outros...\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸ”§ Sistema Batman configurado para flexibilidade!\")\n",
    "print(f\"ğŸ“Š Atualmente usando: {TICKER_SYMBOL}\")\n",
    "print(\"ğŸ’¡ Para trocar o ativo, altere TICKER_SYMBOL e re-execute o notebook!\")\n",
    "\n",
    "# Resumo final do Batman\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¦‡ RESUMO BATMAN Q-LEARNING\")\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… Q-Learning clÃ¡ssico implementado\")\n",
    "print(\"âœ… Estados discretizados para estabilidade\") \n",
    "print(\"âœ… Tabela Q tradicional\")\n",
    "print(\"âœ… ExploraÃ§Ã£o Îµ-greedy\")\n",
    "print(\"âœ… Sistema flexÃ­vel para qualquer ativo\")\n",
    "print(\"âœ… Monitoramento completo de treinamento\")\n",
    "print(\"âœ… AvaliaÃ§Ã£o com benchmarks\")\n",
    "print(\"âœ… VisualizaÃ§Ãµes analÃ­ticas\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b35e5a7",
   "metadata": {},
   "source": [
    "# ğŸ¦‡ BATMAN APPROACH - Reinforcement Learning Trading\n",
    "\n",
    "## EstratÃ©gia: MetodolÃ³gica e Estruturada\n",
    "\n",
    "### Filosofia Batman\n",
    "- **PreparaÃ§Ã£o meticulosa**: Base teÃ³rica sÃ³lida seguindo padrÃµes acadÃªmicos\n",
    "- **Abordagem conservadora**: Q-Learning clÃ¡ssico com tabelas Q\n",
    "- **Metodologia comprovada**: Seguindo estrutura similar Ã s aulas do Prof. Paulo Caixeta\n",
    "- **Estados discretizados**: PreÃ§os convertidos em faixas para simplicidade\n",
    "- **Foco pedagÃ³gico**: Prioriza entendimento dos fundamentos de RL\n",
    "\n",
    "### Objetivo\n",
    "Desenvolver um agente de Reinforcement Learning para trading automatizado usando **Q-Learning tradicional**.\n",
    "O sistema deve ser **genÃ©rico** e funcionar com qualquer ativo (PETR3, VALE3, BRFS3, etc.).\n",
    "\n",
    "### CaracterÃ­sticas da ImplementaÃ§Ã£o\n",
    "- âœ… Q-Learning clÃ¡ssico com tabela Q\n",
    "- âœ… Estados discretos (faixas de preÃ§os)\n",
    "- âœ… ExploraÃ§Ã£o Îµ-greedy\n",
    "- âœ… Ambiente compatÃ­vel com padrÃµes Gymnasium\n",
    "- âœ… CÃ³digo flexÃ­vel para mÃºltiplos ativos\n",
    "- âœ… FundamentaÃ§Ã£o teÃ³rica clara"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
