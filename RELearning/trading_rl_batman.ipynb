{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13489014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Bibliotecas importadas com sucesso!\n",
      "🦇 Batman está preparado para o trading sistemático!\n"
     ]
    }
   ],
   "source": [
    "# Importações necessárias\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"📚 Bibliotecas importadas com sucesso!\")\n",
    "print(\"🦇 Batman está preparado para o trading sistemático!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d77e2ba",
   "metadata": {},
   "source": [
    "## 🎯 CONFIGURAÇÃO FLEXÍVEL DO SISTEMA\n",
    "\n",
    "Sistema genérico que aceita qualquer ativo da B3. Basta alterar o `TICKER_SYMBOL` para usar com VALE3, BRFS3, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "928ee4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Configurado para: PETR3.SA\n",
      "💰 Capital inicial: R$ 50,000.00\n",
      "📊 Período: 20y\n",
      "🧠 Episodes: 1000\n"
     ]
    }
   ],
   "source": [
    "# 🎮 CONFIGURAÇÃO PRINCIPAL - ALTERE AQUI PARA USAR OUTRO ATIVO\n",
    "TICKER_SYMBOL = \"PETR3.SA\"  # Pode ser: PETR3.SA, VALE3.SA, BRFS3.SA, etc.\n",
    "PERIOD = \"20y\"              # Período dos dados: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max\n",
    "INITIAL_CAPITAL = 50000.0   # Capital inicial em R$\n",
    "\n",
    "# Parâmetros do Q-Learning (Abordagem Batman - Conservadora)\n",
    "LEARNING_RATE = 0.1        # Taxa de aprendizado\n",
    "DISCOUNT_FACTOR = 0.95     # Fator de desconto (gamma)\n",
    "EPSILON_START = 1.0        # Exploração inicial\n",
    "EPSILON_MIN = 0.01         # Exploração mínima  \n",
    "EPSILON_DECAY = 0.995      # Decaimento da exploração\n",
    "NUM_EPISODES = 1000        # Número de episódios de treinamento\n",
    "\n",
    "# Estados discretos (Batman usa faixas simples)\n",
    "NUM_PRICE_BINS = 5       # Número de faixas de preço\n",
    "WINDOW_SIZE = 30            # Janela histórica\n",
    "\n",
    "print(f\"🎯 Configurado para: {TICKER_SYMBOL}\")\n",
    "print(f\"💰 Capital inicial: R$ {INITIAL_CAPITAL:,.2f}\")\n",
    "print(f\"📊 Período: {PERIOD}\")\n",
    "print(f\"🧠 Episodes: {NUM_EPISODES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a33f9",
   "metadata": {},
   "source": [
    "## 📈 COLETA DE DADOS GENÉRICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6479a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📡 Baixando dados de PETR3.SA...\n",
      "✅ Dados carregados com sucesso!\n",
      "   📊 Empresa: Petróleo Brasileiro S.A. - Petrobras\n",
      "   🏢 Setor: Energy\n",
      "   📅 Período: 2005-10-31 até 2025-10-31\n",
      "   📈 Total de dias: 4977\n",
      "   💰 Preço atual: R$ 31.51\n",
      "✅ Dados carregados com sucesso!\n",
      "   📊 Empresa: Petróleo Brasileiro S.A. - Petrobras\n",
      "   🏢 Setor: Energy\n",
      "   📅 Período: 2005-10-31 até 2025-10-31\n",
      "   📈 Total de dias: 4977\n",
      "   💰 Preço atual: R$ 31.51\n"
     ]
    }
   ],
   "source": [
    "# 📊 Função genérica para carregar dados de qualquer ativo\n",
    "def load_stock_data(ticker_symbol, period=\"1y\"):\n",
    "    \"\"\"\n",
    "    Carrega dados históricos de qualquer ativo da B3\n",
    "    \n",
    "    Args:\n",
    "        ticker_symbol (str): Símbolo do ativo (ex: 'PETR3.SA', 'VALE3.SA')\n",
    "        period (str): Período dos dados\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Dados históricos do ativo\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"📡 Baixando dados de {ticker_symbol}...\")\n",
    "        ticker = yf.Ticker(ticker_symbol)\n",
    "        \n",
    "        # Buscar informações da empresa\n",
    "        info = ticker.info\n",
    "        company_name = info.get('longName', 'N/A')\n",
    "        sector = info.get('sector', 'N/A')\n",
    "        \n",
    "        # Buscar dados históricos\n",
    "        df = ticker.history(period=period)\n",
    "        \n",
    "        if df.empty:\n",
    "            raise ValueError(f\"Nenhum dado encontrado para {ticker_symbol}\")\n",
    "        \n",
    "        print(f\"✅ Dados carregados com sucesso!\")\n",
    "        print(f\"   📊 Empresa: {company_name}\")\n",
    "        print(f\"   🏢 Setor: {sector}\")\n",
    "        print(f\"   📅 Período: {df.index[0].date()} até {df.index[-1].date()}\")\n",
    "        print(f\"   📈 Total de dias: {len(df)}\")\n",
    "        print(f\"   💰 Preço atual: R$ {df['Close'].iloc[-1]:.2f}\")\n",
    "        \n",
    "        return df, info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro ao carregar dados: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Carregar dados do ativo configurado\n",
    "df_stock, stock_info = load_stock_data(TICKER_SYMBOL, PERIOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74889ba0",
   "metadata": {},
   "source": [
    "## 🧠 BATMAN Q-LEARNING SYSTEM\n",
    "\n",
    "### Estados Discretos (Abordagem Conservadora)\n",
    "Batman utiliza uma abordagem metodológica com estados discretizados para garantir convergência estável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba541986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Estados Batman configurados:\n",
      "   📊 Faixas de preço: 5 bins\n",
      "   📈 Range: R$ 1.74 - R$ 39.56\n",
      "   🔍 Janela histórica: 30 dias\n",
      "   🧮 Total de estados possíveis: 931,322,574,615,478,515,625\n",
      "\n",
      "📋 Resumo dos dados:\n",
      "   📊 Total de observações: 4977\n",
      "   📈 Primeiro preço: R$ 4.44\n",
      "   📉 Último preço: R$ 31.51\n"
     ]
    }
   ],
   "source": [
    "# 🎯 Sistema de Estados Discretos (Batman Approach)\n",
    "class BatmanStateManager:\n",
    "    def __init__(self, prices, num_bins=10, window_size=5):\n",
    "        self.prices = prices\n",
    "        self.num_bins = num_bins\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Criar faixas de preços (discretização)\n",
    "        self.price_min = prices.min()\n",
    "        self.price_max = prices.max()\n",
    "        self.price_bins = np.linspace(self.price_min, self.price_max, num_bins + 1)\n",
    "        \n",
    "        print(f\"🎯 Estados Batman configurados:\")\n",
    "        print(f\"   📊 Faixas de preço: {num_bins} bins\")\n",
    "        print(f\"   📈 Range: R$ {self.price_min:.2f} - R$ {self.price_max:.2f}\")\n",
    "        print(f\"   🔍 Janela histórica: {window_size} dias\")\n",
    "        print(f\"   🧮 Total de estados possíveis: {num_bins ** window_size:,}\")\n",
    "    \n",
    "    def discretize_price(self, price):\n",
    "        \"\"\"Converte preço contínuo em faixa discreta\"\"\"\n",
    "        return np.digitize(price, self.price_bins) - 1\n",
    "    \n",
    "    def get_state(self, current_index):\n",
    "        \"\"\"Cria estado discreto baseado em janela histórica\"\"\"\n",
    "        if current_index < self.window_size:\n",
    "            # Para os primeiros dias, usar o primeiro preço\n",
    "            window_prices = [self.prices[0]] * (self.window_size - current_index - 1)\n",
    "            window_prices.extend(self.prices[:current_index + 1])\n",
    "        else:\n",
    "            window_prices = self.prices[current_index - self.window_size + 1:current_index + 1]\n",
    "        \n",
    "        # Discretizar cada preço da janela\n",
    "        discrete_state = tuple([self.discretize_price(price) for price in window_prices])\n",
    "        return discrete_state\n",
    "\n",
    "# Ações disponíveis (padrão do Prof. Paulo)\n",
    "class Actions:\n",
    "    HOLD = 0\n",
    "    BUY = 1\n",
    "    SELL = 2\n",
    "    \n",
    "    @classmethod\n",
    "    def get_all_actions(cls):\n",
    "        return [cls.HOLD, cls.BUY, cls.SELL]\n",
    "    \n",
    "    @classmethod \n",
    "    def action_name(cls, action):\n",
    "        names = {cls.HOLD: \"HOLD\", cls.BUY: \"BUY\", cls.SELL: \"SELL\"}\n",
    "        return names.get(action, \"UNKNOWN\")\n",
    "\n",
    "# Preparar dados\n",
    "if df_stock is not None:\n",
    "    prices = df_stock['Close'].values\n",
    "    state_manager = BatmanStateManager(prices, NUM_PRICE_BINS, WINDOW_SIZE)\n",
    "    \n",
    "    print(f\"\\n📋 Resumo dos dados:\")\n",
    "    print(f\"   📊 Total de observações: {len(prices)}\")\n",
    "    print(f\"   📈 Primeiro preço: R$ {prices[0]:.2f}\")\n",
    "    print(f\"   📉 Último preço: R$ {prices[-1]:.2f}\")\n",
    "else:\n",
    "    print(\"❌ Erro: Dados não carregados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d35de",
   "metadata": {},
   "source": [
    "## 🏛️ AMBIENTE DE TRADING BATMAN\n",
    "\n",
    "Ambiente simples e confiável, seguindo princípios de metodologia Batman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db684fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏛️ Ambiente Batman inicializado com sucesso!\n",
      "   💰 Capital inicial: R$ 50,000.00\n",
      "   📊 Dados disponíveis: 4977 dias\n",
      "   🎯 Início do trading no dia 30\n"
     ]
    }
   ],
   "source": [
    "# 🏛️ Ambiente de Trading Batman (Estável e Metodológico)\n",
    "class BatmanTradingEnvironment:\n",
    "    def __init__(self, prices, state_manager, initial_capital=10000.0):\n",
    "        self.prices = prices\n",
    "        self.state_manager = state_manager\n",
    "        self.initial_capital = initial_capital\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia o ambiente para novo episódio\"\"\"\n",
    "        self.current_step = self.state_manager.window_size\n",
    "        self.cash = self.initial_capital\n",
    "        self.shares = 0\n",
    "        self.portfolio_values = []\n",
    "        self.actions_history = []\n",
    "        self.episode_rewards = []\n",
    "        \n",
    "        return self.get_current_state()\n",
    "    \n",
    "    def get_current_state(self):\n",
    "        \"\"\"Retorna estado atual discretizado\"\"\"\n",
    "        return self.state_manager.get_state(self.current_step)\n",
    "    \n",
    "    def get_portfolio_value(self):\n",
    "        \"\"\"Calcula valor total do portfólio\"\"\"\n",
    "        current_price = self.prices[self.current_step]\n",
    "        return self.cash + (self.shares * current_price)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Executa uma ação e retorna (next_state, reward, done, info)\"\"\"\n",
    "        current_price = self.prices[self.current_step]\n",
    "        portfolio_value_before = self.get_portfolio_value()\n",
    "        \n",
    "        # Executar ação\n",
    "        action_executed = False\n",
    "        if action == Actions.BUY and self.cash >= current_price:\n",
    "            # Comprar 1 ação\n",
    "            self.shares += 1\n",
    "            self.cash -= current_price\n",
    "            action_executed = True\n",
    "            \n",
    "        elif action == Actions.SELL and self.shares > 0:\n",
    "            # Vender 1 ação  \n",
    "            self.shares -= 1\n",
    "            self.cash += current_price\n",
    "            action_executed = True\n",
    "            \n",
    "        # HOLD não executa nada, mas sempre é válido\n",
    "        if action == Actions.HOLD:\n",
    "            action_executed = True\n",
    "        \n",
    "        # Calcular recompensa (Batman usa mudança simples no portfólio)\n",
    "        portfolio_value_after = self.get_portfolio_value()\n",
    "        reward = portfolio_value_after - portfolio_value_before\n",
    "        \n",
    "        # Registrar histórico\n",
    "        self.portfolio_values.append(portfolio_value_after)\n",
    "        self.actions_history.append(action)\n",
    "        self.episode_rewards.append(reward)\n",
    "        \n",
    "        # Avançar para próximo dia\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.prices) - 1\n",
    "        \n",
    "        # Próximo estado (ou None se terminado)\n",
    "        next_state = self.get_current_state() if not done else None\n",
    "        \n",
    "        # Informações adicionais\n",
    "        info = {\n",
    "            'cash': self.cash,\n",
    "            'shares': self.shares, \n",
    "            'portfolio_value': portfolio_value_after,\n",
    "            'current_price': current_price,\n",
    "            'action_executed': action_executed,\n",
    "            'day': self.current_step\n",
    "        }\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def get_episode_summary(self):\n",
    "        \"\"\"Retorna resumo do episódio atual\"\"\"\n",
    "        if not self.portfolio_values:\n",
    "            return None\n",
    "            \n",
    "        total_return = (self.get_portfolio_value() - self.initial_capital) / self.initial_capital\n",
    "        max_value = max(self.portfolio_values)\n",
    "        min_value = min(self.portfolio_values) \n",
    "        \n",
    "        return {\n",
    "            'total_return': total_return,\n",
    "            'final_value': self.get_portfolio_value(),\n",
    "            'max_value': max_value,\n",
    "            'min_value': min_value,\n",
    "            'total_reward': sum(self.episode_rewards),\n",
    "            'num_days': len(self.portfolio_values),\n",
    "            'actions_taken': len([a for a in self.actions_history if a != Actions.HOLD])\n",
    "        }\n",
    "\n",
    "# Inicializar ambiente Batman\n",
    "if df_stock is not None:\n",
    "    env = BatmanTradingEnvironment(prices, state_manager, INITIAL_CAPITAL)\n",
    "    print(\"🏛️ Ambiente Batman inicializado com sucesso!\")\n",
    "    print(f\"   💰 Capital inicial: R$ {INITIAL_CAPITAL:,.2f}\")\n",
    "    print(f\"   📊 Dados disponíveis: {len(prices)} dias\")\n",
    "    print(f\"   🎯 Início do trading no dia {env.current_step}\")\n",
    "else:\n",
    "    print(\"❌ Erro: Ambiente não pode ser inicializado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e69ba",
   "metadata": {},
   "source": [
    "## 🦇 AGENTE Q-LEARNING BATMAN\n",
    "\n",
    "Implementação clássica do Q-Learning seguindo os padrões das aulas do Prof. Paulo Caixeta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b026a624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦇 Agente Batman Q-Learning inicializado!\n",
      "   🧠 Learning rate: 0.1\n",
      "   💰 Discount factor: 0.95\n",
      "   🔍 Epsilon inicial: 1.0\n",
      "\n",
      "🎯 Agente pronto para treinamento com 1000 episódios!\n"
     ]
    }
   ],
   "source": [
    "# 🦇 Agente Q-Learning Batman (Clássico e Confiável)\n",
    "class BatmanQLearningAgent:\n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.95, \n",
    "                 epsilon_start=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        # Tabela Q (Batman usa dicionário tradicional)\n",
    "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
    "        self.actions = Actions.get_all_actions()\n",
    "        \n",
    "        # Estatísticas de treinamento\n",
    "        self.episode_rewards = []\n",
    "        self.episode_returns = []\n",
    "        self.exploration_counts = []\n",
    "        \n",
    "        print(\"🦇 Agente Batman Q-Learning inicializado!\")\n",
    "        print(f\"   🧠 Learning rate: {learning_rate}\")\n",
    "        print(f\"   💰 Discount factor: {discount_factor}\")\n",
    "        print(f\"   🔍 Epsilon inicial: {epsilon_start}\")\n",
    "        \n",
    "    def get_action(self, state, training=True):\n",
    "        \"\"\"Seleciona ação usando ε-greedy policy\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            # Exploração\n",
    "            action = random.choice(self.actions)\n",
    "            return action, True  # True indica exploração\n",
    "        else:\n",
    "            # Exploitação (escolher melhor ação conhecida)\n",
    "            q_values = [self.q_table[state][action] for action in self.actions]\n",
    "            max_q = max(q_values)\n",
    "            \n",
    "            # Se múltiplas ações têm mesmo Q-value, escolher aleatoriamente\n",
    "            best_actions = [action for action, q_val in zip(self.actions, q_values) if q_val == max_q]\n",
    "            action = random.choice(best_actions)\n",
    "            return action, False  # False indica exploitação\n",
    "    \n",
    "    def update_q_value(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Atualiza valor Q usando a equação de Bellman\"\"\"\n",
    "        current_q = self.q_table[state][action]\n",
    "        \n",
    "        if done:\n",
    "            # Estado terminal\n",
    "            target_q = reward\n",
    "        else:\n",
    "            # Q-Learning: max Q-value do próximo estado\n",
    "            next_q_values = [self.q_table[next_state][a] for a in self.actions]\n",
    "            max_next_q = max(next_q_values) if next_q_values else 0\n",
    "            target_q = reward + self.discount_factor * max_next_q\n",
    "        \n",
    "        # Atualização Q-Learning\n",
    "        self.q_table[state][action] = current_q + self.learning_rate * (target_q - current_q)\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Reduz epsilon para diminuir exploração ao longo do tempo\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        \"\"\"Treina um episódio completo\"\"\"\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        exploration_count = 0\n",
    "        \n",
    "        while True:\n",
    "            # Escolher ação\n",
    "            action, is_exploration = self.get_action(state, training=True)\n",
    "            if is_exploration:\n",
    "                exploration_count += 1\n",
    "                \n",
    "            # Executar ação\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Atualizar Q-value\n",
    "            self.update_q_value(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Acumular recompensa\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "        \n",
    "        # Decair epsilon\n",
    "        self.decay_epsilon()\n",
    "        \n",
    "        # Registrar estatísticas\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        self.exploration_counts.append(exploration_count)\n",
    "        \n",
    "        # Calcular retorno do episódio\n",
    "        episode_summary = env.get_episode_summary()\n",
    "        if episode_summary:\n",
    "            self.episode_returns.append(episode_summary['total_return'])\n",
    "        \n",
    "        return episode_summary\n",
    "    \n",
    "    def get_q_table_size(self):\n",
    "        \"\"\"Retorna tamanho atual da tabela Q\"\"\"\n",
    "        return len(self.q_table)\n",
    "    \n",
    "    def get_training_stats(self):\n",
    "        \"\"\"Retorna estatísticas de treinamento\"\"\"\n",
    "        if not self.episode_rewards:\n",
    "            return None\n",
    "            \n",
    "        return {\n",
    "            'total_episodes': len(self.episode_rewards),\n",
    "            'avg_reward': np.mean(self.episode_rewards[-100:]),  # Últimos 100\n",
    "            'avg_return': np.mean(self.episode_returns[-100:]) if self.episode_returns else 0,\n",
    "            'current_epsilon': self.epsilon,\n",
    "            'q_table_size': self.get_q_table_size(),\n",
    "            'avg_exploration': np.mean(self.exploration_counts[-100:]) if self.exploration_counts else 0\n",
    "        }\n",
    "\n",
    "# Inicializar agente Batman\n",
    "agent = BatmanQLearningAgent(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    discount_factor=DISCOUNT_FACTOR,\n",
    "    epsilon_start=EPSILON_START,\n",
    "    epsilon_min=EPSILON_MIN,\n",
    "    epsilon_decay=EPSILON_DECAY\n",
    ")\n",
    "\n",
    "print(f\"\\n🎯 Agente pronto para treinamento com {NUM_EPISODES} episódios!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efaf64d",
   "metadata": {},
   "source": [
    "## 🏋️ TREINAMENTO BATMAN\n",
    "\n",
    "Treinamento metodológico e monitorado, seguindo padrões de estabilidade Batman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbe42182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando treinamento para PETR3.SA\n",
      "🦇 Iniciando treinamento Batman - 1000 episódios\n",
      "📊 Relatórios a cada 200 episódios\n",
      "============================================================\n",
      "📈 Episódio 200/1000\n",
      "   💰 Reward médio (últimos 100): +0.00\n",
      "   📊 Retorno médio (últimos 100): +2.19%\n",
      "   🔍 Epsilon atual: 0.367\n",
      "   🧠 Tamanho Q-table: 960\n",
      "   🎯 Exploração média: 2363.4 ações/episódio\n",
      "   💵 Último episódio: R$ 51,494.46 (+2.99%)\n",
      "----------------------------------------\n",
      "📈 Episódio 200/1000\n",
      "   💰 Reward médio (últimos 100): +0.00\n",
      "   📊 Retorno médio (últimos 100): +2.19%\n",
      "   🔍 Epsilon atual: 0.367\n",
      "   🧠 Tamanho Q-table: 960\n",
      "   🎯 Exploração média: 2363.4 ações/episódio\n",
      "   💵 Último episódio: R$ 51,494.46 (+2.99%)\n",
      "----------------------------------------\n",
      "📈 Episódio 400/1000\n",
      "   💰 Reward médio (últimos 100): +0.00\n",
      "   📊 Retorno médio (últimos 100): +2.26%\n",
      "   🔍 Epsilon atual: 0.135\n",
      "   🧠 Tamanho Q-table: 960\n",
      "   🎯 Exploração média: 867.0 ações/episódio\n",
      "   💵 Último episódio: R$ 50,984.43 (+1.97%)\n",
      "----------------------------------------\n",
      "📈 Episódio 400/1000\n",
      "   💰 Reward médio (últimos 100): +0.00\n",
      "   📊 Retorno médio (últimos 100): +2.26%\n",
      "   🔍 Epsilon atual: 0.135\n",
      "   🧠 Tamanho Q-table: 960\n",
      "   🎯 Exploração média: 867.0 ações/episódio\n",
      "   💵 Último episódio: R$ 50,984.43 (+1.97%)\n",
      "----------------------------------------\n",
      "📈 Episódio 600/1000\n",
      "   💰 Reward médio (últimos 100): +0.00\n",
      "   📊 Retorno médio (últimos 100): +2.05%\n",
      "   🔍 Epsilon atual: 0.049\n",
      "   🧠 Tamanho Q-table: 960\n",
      "   🎯 Exploração média: 314.2 ações/episódio\n",
      "   💵 Último episódio: R$ 50,973.74 (+1.95%)\n",
      "----------------------------------------\n",
      "📈 Episódio 600/1000\n",
      "   💰 Reward médio (últimos 100): +0.00\n",
      "   📊 Retorno médio (últimos 100): +2.05%\n",
      "   🔍 Epsilon atual: 0.049\n",
      "   🧠 Tamanho Q-table: 960\n",
      "   🎯 Exploração média: 314.2 ações/episódio\n",
      "   💵 Último episódio: R$ 50,973.74 (+1.95%)\n",
      "----------------------------------------\n",
      "📈 Episódio 800/1000\n",
      "   💰 Reward médio (últimos 100): +0.00\n",
      "   📊 Retorno médio (últimos 100): +2.26%\n",
      "   🔍 Epsilon atual: 0.018\n",
      "   🧠 Tamanho Q-table: 960\n",
      "   🎯 Exploração média: 116.2 ações/episódio\n",
      "   💵 Último episódio: R$ 50,438.95 (+0.88%)\n",
      "----------------------------------------\n",
      "📈 Episódio 800/1000\n",
      "   💰 Reward médio (últimos 100): +0.00\n",
      "   📊 Retorno médio (últimos 100): +2.26%\n",
      "   🔍 Epsilon atual: 0.018\n",
      "   🧠 Tamanho Q-table: 960\n",
      "   🎯 Exploração média: 116.2 ações/episódio\n",
      "   💵 Último episódio: R$ 50,438.95 (+0.88%)\n",
      "----------------------------------------\n",
      "📈 Episódio 1000/1000\n",
      "   💰 Reward médio (últimos 100): +0.00\n",
      "   📊 Retorno médio (últimos 100): +2.02%\n",
      "   🔍 Epsilon atual: 0.010\n",
      "   🧠 Tamanho Q-table: 960\n",
      "   🎯 Exploração média: 50.9 ações/episódio\n",
      "   💵 Último episódio: R$ 50,349.90 (+0.70%)\n",
      "----------------------------------------\n",
      "✅ Treinamento Batman concluído!\n",
      "📊 Estatísticas finais:\n",
      "   🧠 Q-table final: 960 estados\n",
      "   🎯 Epsilon final: 0.010\n",
      "   💰 Reward médio final: +0.00\n",
      "   📈 Retorno médio final: +2.02%\n",
      "📈 Episódio 1000/1000\n",
      "   💰 Reward médio (últimos 100): +0.00\n",
      "   📊 Retorno médio (últimos 100): +2.02%\n",
      "   🔍 Epsilon atual: 0.010\n",
      "   🧠 Tamanho Q-table: 960\n",
      "   🎯 Exploração média: 50.9 ações/episódio\n",
      "   💵 Último episódio: R$ 50,349.90 (+0.70%)\n",
      "----------------------------------------\n",
      "✅ Treinamento Batman concluído!\n",
      "📊 Estatísticas finais:\n",
      "   🧠 Q-table final: 960 estados\n",
      "   🎯 Epsilon final: 0.010\n",
      "   💰 Reward médio final: +0.00\n",
      "   📈 Retorno médio final: +2.02%\n"
     ]
    }
   ],
   "source": [
    "# 🏋️ Loop de Treinamento Batman (Metodológico e Monitorado)\n",
    "def train_batman_agent(agent, env, num_episodes, print_every=100):\n",
    "    \"\"\"\n",
    "    Treina o agente Batman com monitoramento detalhado\n",
    "    \"\"\"\n",
    "    print(f\"🦇 Iniciando treinamento Batman - {num_episodes} episódios\")\n",
    "    print(f\"📊 Relatórios a cada {print_every} episódios\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    training_history = {\n",
    "        'episode': [],\n",
    "        'avg_reward': [],\n",
    "        'avg_return': [],\n",
    "        'epsilon': [],\n",
    "        'q_table_size': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        # Treinar episódio\n",
    "        episode_summary = agent.train_episode(env)\n",
    "        \n",
    "        # Relatório periódico\n",
    "        if episode % print_every == 0:\n",
    "            stats = agent.get_training_stats()\n",
    "            \n",
    "            print(f\"📈 Episódio {episode}/{num_episodes}\")\n",
    "            print(f\"   💰 Reward médio (últimos 100): {stats['avg_reward']:+.2f}\")\n",
    "            print(f\"   📊 Retorno médio (últimos 100): {stats['avg_return']:+.2%}\")\n",
    "            print(f\"   🔍 Epsilon atual: {stats['current_epsilon']:.3f}\")\n",
    "            print(f\"   🧠 Tamanho Q-table: {stats['q_table_size']:,}\")\n",
    "            print(f\"   🎯 Exploração média: {stats['avg_exploration']:.1f} ações/episódio\")\n",
    "            \n",
    "            if episode_summary:\n",
    "                print(f\"   💵 Último episódio: R$ {episode_summary['final_value']:,.2f} \" + \n",
    "                      f\"({episode_summary['total_return']:+.2%})\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Salvar histórico\n",
    "            training_history['episode'].append(episode)\n",
    "            training_history['avg_reward'].append(stats['avg_reward'])\n",
    "            training_history['avg_return'].append(stats['avg_return'])\n",
    "            training_history['epsilon'].append(stats['current_epsilon'])\n",
    "            training_history['q_table_size'].append(stats['q_table_size'])\n",
    "    \n",
    "    print(\"✅ Treinamento Batman concluído!\")\n",
    "    final_stats = agent.get_training_stats()\n",
    "    print(f\"📊 Estatísticas finais:\")\n",
    "    print(f\"   🧠 Q-table final: {final_stats['q_table_size']:,} estados\")\n",
    "    print(f\"   🎯 Epsilon final: {final_stats['current_epsilon']:.3f}\")\n",
    "    print(f\"   💰 Reward médio final: {final_stats['avg_reward']:+.2f}\")\n",
    "    print(f\"   📈 Retorno médio final: {final_stats['avg_return']:+.2%}\")\n",
    "    \n",
    "    return training_history\n",
    "\n",
    "# Executar treinamento\n",
    "if df_stock is not None:\n",
    "    print(f\"🚀 Iniciando treinamento para {TICKER_SYMBOL}\")\n",
    "    training_history = train_batman_agent(agent, env, NUM_EPISODES, print_every=200)\n",
    "else:\n",
    "    print(\"❌ Não é possível treinar sem dados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7ef0eb",
   "metadata": {},
   "source": [
    "## 🔍 DEBUG - DIAGNÓSTICO DO TREINAMENTO\n",
    "\n",
    "Células de debug para investigar problemas de rewards zerados e Q-table não crescendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f0fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 DEBUG 1: Verificar ambiente básico\n",
    "print(\"🔍 DEBUG 1: VERIFICAÇÃO DO AMBIENTE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Testar reset do ambiente\n",
    "state = env.reset()\n",
    "print(f\"   Estado inicial: {state}\")\n",
    "print(f\"   Tipo do estado: {type(state)}\")\n",
    "print(f\"   Tamanho do estado: {len(state)}\")\n",
    "print(f\"   Cash inicial: R$ {env.cash:.2f}\")\n",
    "print(f\"   Shares iniciais: {env.shares}\")\n",
    "print(f\"   Preço atual: R$ {env.prices[env.current_step]:.2f}\")\n",
    "print(f\"   Dia atual: {env.current_step}\")\n",
    "print(f\"   Pode comprar uma ação? {env.cash >= env.prices[env.current_step]}\")\n",
    "print(f\"   Quantas ações pode comprar? {int(env.cash // env.prices[env.current_step])}\")\n",
    "\n",
    "# Testar algumas ações manualmente\n",
    "print(f\"\\n🎯 TESTE DE AÇÕES MANUAIS:\")\n",
    "for action_idx, action_name in [(0, 'HOLD'), (1, 'BUY'), (2, 'SELL')]:\n",
    "    # Criar ambiente limpo para cada teste\n",
    "    env_test = BatmanTradingEnvironment(prices, state_manager, INITIAL_CAPITAL)\n",
    "    state = env_test.reset()\n",
    "    \n",
    "    portfolio_before = env_test.get_portfolio_value()\n",
    "    next_state, reward, done, info = env_test.step(action_idx)\n",
    "    portfolio_after = env_test.get_portfolio_value()\n",
    "    \n",
    "    print(f\"   {action_name:4s}: Reward={reward:+8.2f} | Portfolio: R$ {portfolio_before:.0f} → R$ {portfolio_after:.0f} | Executada: {info['action_executed']}\")\n",
    "    print(f\"          Cash: R$ {info['cash']:.0f} | Shares: {info['shares']} | Preço: R$ {info['current_price']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f80724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 DEBUG 2: Análise de discretização de estados\n",
    "print(\"\\n🔍 DEBUG 2: ANÁLISE DE DISCRETIZAÇÃO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Testar discretização com diferentes janelas\n",
    "test_steps = [0, 10, 20, 50, 100]\n",
    "print(f\"   Bins de preço configurados: {NUM_PRICE_BINS}\")\n",
    "print(f\"   Tamanho da janela: {WINDOW_SIZE}\")\n",
    "print(f\"   Range de preços nos dados: R$ {prices.min():.2f} - R$ {prices.max():.2f}\")\n",
    "\n",
    "print(f\"\\n   Estados discretos em diferentes momentos:\")\n",
    "unique_states = set()\n",
    "for step in test_steps:\n",
    "    if step < len(prices):\n",
    "        state_discrete = state_manager.get_state(step)  # Já retorna estado discreto\n",
    "        unique_states.add(state_discrete)\n",
    "        price = prices[step]\n",
    "        print(f\"      Passo {step:3d}: Preço R$ {price:6.2f} → Estado {state_discrete}\")\n",
    "\n",
    "print(f\"\\n   Estados únicos encontrados: {len(unique_states)}\")\n",
    "print(f\"   Estados na Q-table atual: {len(agent.q_table)}\")\n",
    "\n",
    "# Verificar se estados estão sendo criados corretamente\n",
    "print(f\"\\n   Primeiros estados na Q-table:\")\n",
    "for i, (state_key, q_values) in enumerate(list(agent.q_table.items())[:5]):\n",
    "    print(f\"      {state_key}: {q_values}\")\n",
    "    if i >= 4:  # Mostrar apenas os primeiros 5\n",
    "        break\n",
    "\n",
    "# Testar diferentes bins para o mesmo passo\n",
    "print(f\"\\n   Análise detalhada de discretização:\")\n",
    "test_step = 50\n",
    "if test_step < len(prices):\n",
    "    price_at_step = prices[test_step]\n",
    "    bin_number = state_manager.discretize_price(price_at_step)\n",
    "    bin_range_min = state_manager.price_bins[bin_number] if bin_number < len(state_manager.price_bins)-1 else state_manager.price_bins[-2]\n",
    "    bin_range_max = state_manager.price_bins[bin_number + 1] if bin_number < len(state_manager.price_bins)-1 else state_manager.price_bins[-1]\n",
    "    \n",
    "    print(f\"      Preço no passo {test_step}: R$ {price_at_step:.2f}\")\n",
    "    print(f\"      Bin atribuído: {bin_number} (de 0 a {NUM_PRICE_BINS-1})\")\n",
    "    print(f\"      Range do bin: R$ {bin_range_min:.2f} - R$ {bin_range_max:.2f}\")\n",
    "    print(f\"      Tamanho do bin: R$ {bin_range_max - bin_range_min:.2f}\")\n",
    "\n",
    "# Verificar distribuição dos preços nos bins\n",
    "print(f\"\\n   Distribuição dos preços nos bins:\")\n",
    "all_bins = [state_manager.discretize_price(p) for p in prices[:100]]  # Primeiros 100 preços\n",
    "bin_counts = {}\n",
    "for bin_num in all_bins:\n",
    "    bin_counts[bin_num] = bin_counts.get(bin_num, 0) + 1\n",
    "\n",
    "for bin_num in sorted(bin_counts.keys())[:10]:  # Mostrar primeiros 10 bins\n",
    "    print(f\"      Bin {bin_num:2d}: {bin_counts[bin_num]:3d} preços\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c3f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 DEBUG 3: Episódio de treinamento detalhado\n",
    "print(\"\\n🔍 DEBUG 3: EPISÓDIO DETALHADO (1 episódio completo)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Resetar ambiente para debug\n",
    "env.reset()\n",
    "agent.epsilon = 0.5  # Forçar alguma exploração\n",
    "\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "state_history = []\n",
    "reward_history = []\n",
    "\n",
    "print(f\"   Capital inicial: R$ {INITIAL_CAPITAL:.0f}\")\n",
    "print(f\"   Epsilon atual: {agent.epsilon:.3f}\")\n",
    "print(f\"   Tamanho da Q-table antes: {len(agent.q_table)}\")\n",
    "\n",
    "print(f\"\\n   Primeiros 10 passos do episódio:\")\n",
    "state = env.get_current_state()\n",
    "\n",
    "for step in range(min(10, len(prices) - WINDOW_SIZE - 1)):\n",
    "    # Estado atual (já discreto)\n",
    "    action, is_exploration = agent.get_action(state, training=True)\n",
    "    \n",
    "    # Executar ação\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    # Guardar histórico\n",
    "    state_history.append(state)\n",
    "    reward_history.append(reward)\n",
    "    \n",
    "    # Treinar agente\n",
    "    agent.update_q_value(state, action, reward, next_state, done)\n",
    "    \n",
    "    # Log detalhado\n",
    "    action_names = ['HOLD', 'BUY', 'SELL']\n",
    "    exploration_mark = \"(EXPLORE)\" if is_exploration else \"(EXPLOIT)\"\n",
    "    print(f\"      {step+1:2d}. Estado: {state} | Ação: {action_names[action]} {exploration_mark}\")\n",
    "    print(f\"           Reward: {reward:+7.2f} | Portfolio: R$ {env.get_portfolio_value():.0f}\")\n",
    "    print(f\"           Cash: R$ {info['cash']:.0f} | Shares: {info['shares']} | Preço: R$ {info['current_price']:.2f} | Executada: {info['action_executed']}\")\n",
    "    \n",
    "    state = next_state\n",
    "    step_count += 1\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"\\n   RESUMO DO DEBUG:\")\n",
    "print(f\"      Passos executados: {step_count}\")\n",
    "print(f\"      Reward total: {total_reward:.2f}\")\n",
    "print(f\"      Estados únicos visitados: {len(set(state_history))}\")\n",
    "print(f\"      Tamanho da Q-table depois: {len(agent.q_table)}\")\n",
    "print(f\"      Rewards únicos: {set(reward_history)}\")\n",
    "print(f\"      Portfolio final: R$ {env.get_portfolio_value():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c991de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 DEBUG 4: Análise de recompensas e função objetivo\n",
    "print(\"\\n🔍 DEBUG 4: ANÁLISE DE RECOMPENSAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simular diferentes cenários de trading\n",
    "scenarios = [\n",
    "    {\"name\": \"Compra com alta\", \"action\": 1, \"price_change\": 0.05},\n",
    "    {\"name\": \"Compra com queda\", \"action\": 1, \"price_change\": -0.03},\n",
    "    {\"name\": \"Venda com alta\", \"action\": 2, \"price_change\": 0.04},\n",
    "    {\"name\": \"Hold com volatilidade\", \"action\": 0, \"price_change\": 0.02}\n",
    "]\n",
    "\n",
    "print(f\"   Simulando rewards para diferentes cenários:\")\n",
    "print(f\"   (usando preços artificiais para teste)\")\n",
    "\n",
    "for scenario in scenarios:\n",
    "    # Criar ambiente de teste com preços controlados (precisa ter WINDOW_SIZE + alguns dias)\n",
    "    base_price = 100.0\n",
    "    future_price = base_price * (1 + scenario[\"price_change\"])\n",
    "    \n",
    "    # Criar array com preços suficientes para o ambiente funcionar\n",
    "    test_prices = np.full(WINDOW_SIZE + 5, base_price)  # Preencher com preço base\n",
    "    test_prices[-1] = future_price  # Último preço com a mudança\n",
    "    \n",
    "    # Criar state_manager temporário para este teste\n",
    "    temp_state_manager = BatmanStateManager(test_prices, NUM_PRICE_BINS, WINDOW_SIZE)\n",
    "    test_env = BatmanTradingEnvironment(test_prices, temp_state_manager, INITIAL_CAPITAL)\n",
    "    \n",
    "    state = test_env.reset()\n",
    "    next_state, reward, done, info = test_env.step(scenario[\"action\"])\n",
    "    \n",
    "    print(f\"      {scenario['name']:20s}: Reward = {reward:+8.2f} | Mudança = {scenario['price_change']:+5.1%}\")\n",
    "    print(f\"                              Portfolio: R$ {info['portfolio_value']:.0f} | Executada: {info['action_executed']}\")\n",
    "\n",
    "# Verificar se o cálculo de reward está funcionando\n",
    "print(f\"\\n   Fórmula de reward atual:\")\n",
    "print(f\"      reward = portfolio_value_after - portfolio_value_before\")\n",
    "print(f\"      Isso significa que rewards são em R$ (mudança absoluta do portfolio)\")\n",
    "print(f\"      Um reward de 100.0 = R$ 100 de ganho\")\n",
    "print(f\"      Um reward de -50.0 = R$ 50 de perda\")\n",
    "print(f\"      Se reward = 0, significa que o portfolio não mudou (ação HOLD ou não executada)\")\n",
    "\n",
    "# Testar se ações estão sendo executadas corretamente\n",
    "print(f\"\\n   Verificação de execução de ações:\")\n",
    "test_env = BatmanTradingEnvironment(prices, state_manager, INITIAL_CAPITAL)\n",
    "test_env.reset()\n",
    "\n",
    "# Tentar comprar uma ação\n",
    "print(f\"      Antes da compra: Cash=R$ {test_env.cash:.0f}, Shares={test_env.shares}\")\n",
    "_, reward_buy, _, info_buy = test_env.step(1)  # BUY\n",
    "print(f\"      Depois da compra: Cash=R$ {info_buy['cash']:.0f}, Shares={info_buy['shares']}, Reward={reward_buy:+.4f}\")\n",
    "\n",
    "# Tentar vender\n",
    "if test_env.shares > 0:\n",
    "    _, reward_sell, _, info_sell = test_env.step(2)  # SELL\n",
    "    print(f\"      Depois da venda: Cash=R$ {info_sell['cash']:.0f}, Shares={info_sell['shares']}, Reward={reward_sell:+.4f}\")\n",
    "else:\n",
    "    print(f\"      Não há shares para vender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 DEBUG 5: Diagnóstico final - possíveis problemas\n",
    "print(\"\\n🔍 DEBUG 5: DIAGNÓSTICO DE PROBLEMAS COMUNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"🔍 CHECKLIST DE PROBLEMAS POTENCIAIS:\")\n",
    "\n",
    "# 1. Verificar se há diversidade nos estados\n",
    "unique_states_sample = set()\n",
    "for i in range(0, min(100, len(prices)), 10):\n",
    "    if i >= WINDOW_SIZE:  # Só pode calcular estado se tiver janela suficiente\n",
    "        state_discrete = state_manager.get_state(i)\n",
    "        unique_states_sample.add(state_discrete)\n",
    "\n",
    "print(f\"\\n   1️⃣ DIVERSIDADE DE ESTADOS:\")\n",
    "print(f\"      Estados únicos em 100 passos: {len(unique_states_sample)}\")\n",
    "print(f\"      Estados únicos esperados: ~{NUM_PRICE_BINS * 3}\")  # Aproximação\n",
    "if len(unique_states_sample) < 10:\n",
    "    print(f\"      ⚠️  PROBLEMA: Poucos estados únicos! Considere aumentar NUM_PRICE_BINS\")\n",
    "else:\n",
    "    print(f\"      ✅ OK: Boa diversidade de estados\")\n",
    "\n",
    "# 2. Verificar range de preços vs discretização\n",
    "price_range = prices.max() - prices.min()\n",
    "price_std = prices.std()\n",
    "print(f\"\\n   2️⃣ DISCRETIZAÇÃO DE PREÇOS:\")\n",
    "print(f\"      Range de preços: R$ {price_range:.2f}\")\n",
    "print(f\"      Desvio padrão: R$ {price_std:.2f}\")\n",
    "print(f\"      Bins configurados: {NUM_PRICE_BINS}\")\n",
    "print(f\"      Tamanho do bin: ~R$ {price_range/NUM_PRICE_BINS:.2f}\")\n",
    "if price_range/NUM_PRICE_BINS > price_std:\n",
    "    print(f\"      ⚠️  PROBLEMA: Bins muito grandes, pouca sensibilidade a mudanças!\")\n",
    "else:\n",
    "    print(f\"      ✅ OK: Bins adequados para capturar variações\")\n",
    "\n",
    "# 3. Verificar capacidade de trading\n",
    "min_price = prices.min()\n",
    "max_shares_possible = int(INITIAL_CAPITAL // min_price)\n",
    "print(f\"\\n   3️⃣ CAPACIDADE DE TRADING:\")\n",
    "print(f\"      Capital inicial: R$ {INITIAL_CAPITAL:.0f}\")\n",
    "print(f\"      Preço mínimo: R$ {min_price:.2f}\")\n",
    "print(f\"      Max ações possíveis: {max_shares_possible}\")\n",
    "if max_shares_possible < 10:\n",
    "    print(f\"      ⚠️  PROBLEMA: Capital muito baixo, poucas oportunidades de trading!\")\n",
    "else:\n",
    "    print(f\"      ✅ OK: Capital suficiente para trading\")\n",
    "\n",
    "# 4. Verificar parâmetros de aprendizagem\n",
    "print(f\"\\n   4️⃣ PARÂMETROS DE APRENDIZAGEM:\")\n",
    "print(f\"      Learning rate: {agent.learning_rate}\")\n",
    "print(f\"      Discount factor: {agent.discount_factor}\")\n",
    "print(f\"      Epsilon inicial: {agent.epsilon}\")\n",
    "if agent.learning_rate < 0.01:\n",
    "    print(f\"      ⚠️  ATENÇÃO: Learning rate muito baixo, aprendizagem pode ser lenta\")\n",
    "else:\n",
    "    print(f\"      ✅ OK: Learning rate adequado\")\n",
    "\n",
    "print(f\"\\n📋 PRÓXIMOS PASSOS SUGERIDOS:\")\n",
    "print(f\"   • Execute os debugs acima para identificar o problema específico\")\n",
    "print(f\"   • Se rewards = 0: verifique se ações estão sendo executadas (DEBUG 1 e 4)\")\n",
    "print(f\"   • Se Q-table não cresce: verifique diversidade de estados (DEBUG 2)\")\n",
    "print(f\"   • Considere ajustar: NUM_PRICE_BINS={NUM_PRICE_BINS*2}, INITIAL_CAPITAL={INITIAL_CAPITAL*2}\")\n",
    "print(f\"   • Para debugging ativo, execute um episódio com DEBUG 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bbbdc2",
   "metadata": {},
   "source": [
    "## 🎯 PROPOSTA DE MELHORIA NO SISTEMA DE REWARDS\n",
    "\n",
    "Baseado no diagnóstico, vamos melhorar o sistema de recompensas para incentivar aprendizagem mais efetiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f68a7d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 ANÁLISE DOS PROBLEMAS IDENTIFICADOS:\n",
      "============================================================\n",
      "📊 PROBLEMAS OBSERVADOS NO DIAGNÓSTICO:\n",
      "   1️⃣ Rewards = 0: Muitas ações não executadas ou HOLD constante\n",
      "   2️⃣ Q-table não cresce: Estados pouco diversificados\n",
      "   3️⃣ Aprendizagem lenta: Sem incentivos claros para explorar\n",
      "   4️⃣ Foco apenas em portfolio: Não premia comportamento desejado\n",
      "\n",
      "📈 CONFIGURAÇÃO ATUAL:\n",
      "   Capital inicial: R$ 50,000\n",
      "   Bins de preço: 5\n",
      "   Janela de estado: 30\n",
      "\n",
      "🚨 PROBLEMA COM REWARD ATUAL:\n",
      "   HOLD: Portfolio R$ 50000 → Reward = 0.0\n",
      "   BUY (falhou): Portfolio R$ 10 → Reward = 0.0, Executada: True\n",
      "\n",
      "💡 CONCLUSÃO: Sistema atual não incentiva exploração nem pune inação!\n"
     ]
    }
   ],
   "source": [
    "# 🔍 ANÁLISE DOS PROBLEMAS ATUAIS NO SISTEMA DE REWARDS\n",
    "print(\"🔍 ANÁLISE DOS PROBLEMAS IDENTIFICADOS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"📊 PROBLEMAS OBSERVADOS NO DIAGNÓSTICO:\")\n",
    "print(\"   1️⃣ Rewards = 0: Muitas ações não executadas ou HOLD constante\")\n",
    "print(\"   2️⃣ Q-table não cresce: Estados pouco diversificados\")  \n",
    "print(\"   3️⃣ Aprendizagem lenta: Sem incentivos claros para explorar\")\n",
    "print(\"   4️⃣ Foco apenas em portfolio: Não premia comportamento desejado\")\n",
    "\n",
    "print(f\"\\n📈 CONFIGURAÇÃO ATUAL:\")\n",
    "print(f\"   Capital inicial: R$ {INITIAL_CAPITAL:,.0f}\")\n",
    "print(f\"   Bins de preço: {NUM_PRICE_BINS}\")\n",
    "print(f\"   Janela de estado: {WINDOW_SIZE}\")\n",
    "\n",
    "# Simular problema atual\n",
    "print(f\"\\n🚨 PROBLEMA COM REWARD ATUAL:\")\n",
    "test_env = BatmanTradingEnvironment(prices, state_manager, INITIAL_CAPITAL)\n",
    "test_env.reset()\n",
    "\n",
    "# Cenário 1: HOLD (sempre reward = 0)\n",
    "portfolio_before = test_env.get_portfolio_value()\n",
    "_, reward_hold, _, _ = test_env.step(0)  # HOLD\n",
    "print(f\"   HOLD: Portfolio R$ {portfolio_before:.0f} → Reward = {reward_hold}\")\n",
    "\n",
    "# Cenário 2: Tentativa de compra sem capital suficiente\n",
    "test_env.cash = 10.0  # Muito pouco cash\n",
    "portfolio_before = test_env.get_portfolio_value()\n",
    "_, reward_failed_buy, _, info = test_env.step(1)  # BUY\n",
    "print(f\"   BUY (falhou): Portfolio R$ {portfolio_before:.0f} → Reward = {reward_failed_buy}, Executada: {info['action_executed']}\")\n",
    "\n",
    "print(f\"\\n💡 CONCLUSÃO: Sistema atual não incentiva exploração nem pune inação!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fabbcf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 PROPOSTA DE NOVO SISTEMA DE REWARDS\n",
      "============================================================\n",
      "🚀 SISTEMA PROPOSTO - 'Batman Smart Rewards':\n",
      "\n",
      "📈 COMPONENTES DO REWARD:\n",
      "   1️⃣ REWARD BASE: Mudança no portfolio (atual)\n",
      "   2️⃣ BÔNUS DE AÇÃO: +5 por executar BUY/SELL com sucesso\n",
      "   3️⃣ PENALIDADE DE INAÇÃO: -2 por HOLD excessivo\n",
      "   4️⃣ PENALIDADE DE FALHA: -10 por tentar ação impossível\n",
      "   5️⃣ BÔNUS DE TIMING: +20 por boa decisão (comprar na baixa, vender na alta)\n",
      "   6️⃣ INCENTIVO DE EXPLORAÇÃO: +1 por visitar estado novo\n",
      "\n",
      "📊 FÓRMULA PROPOSTA:\n",
      "   reward = portfolio_change + action_bonus + timing_bonus + exploration_bonus - penalties\n",
      "\n",
      "🎮 EXEMPLOS DO NOVO SISTEMA:\n",
      "   • Comprar e preço subir: +100 (portfolio) +5 (ação) +20 (timing) = +125\n",
      "   • Vender na alta: +80 (portfolio) +5 (ação) +20 (timing) = +105\n",
      "   • HOLD por muitos passos: 0 (portfolio) -2 (inação) = -2\n",
      "   • Tentar comprar sem cash: 0 (portfolio) -10 (falha) = -10\n",
      "   • Estado nunca visitado: reward atual +1 (exploração)\n",
      "\n",
      "⚙️ PARÂMETROS CONFIGURÁVEIS:\n",
      "   ACTION_BONUS = 5          # Bônus por executar ação\n",
      "   INACTION_PENALTY = -2     # Penalidade por HOLD\n",
      "   FAILURE_PENALTY = -10     # Penalidade por ação falhada\n",
      "   TIMING_BONUS = 20         # Bônus por bom timing\n",
      "   EXPLORATION_BONUS = 1     # Bônus por estado novo\n",
      "   HOLD_TOLERANCE = 3        # Max HOLDs seguidos sem penalidade\n",
      "\n",
      "🧠 BENEFÍCIOS ESPERADOS:\n",
      "   ✅ Incentiva trading ativo vs passivo\n",
      "   ✅ Pune tentativas inválidas (aprende restrições)\n",
      "   ✅ Premia bom timing (essência do trading)\n",
      "   ✅ Encoraja exploração (Q-table cresce)\n",
      "   ✅ Balanceado - não só portfolio matters\n",
      "\n",
      "🤔 QUER IMPLEMENTAR ESTE SISTEMA?\n",
      "   Responda 'sim' para implementar ou sugira modificações!\n"
     ]
    }
   ],
   "source": [
    "# 🎯 PROPOSTA DE SISTEMA DE REWARDS MELHORADO\n",
    "print(\"\\n🎯 PROPOSTA DE NOVO SISTEMA DE REWARDS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"🚀 SISTEMA PROPOSTO - 'Batman Smart Rewards':\")\n",
    "print()\n",
    "print(\"📈 COMPONENTES DO REWARD:\")\n",
    "print(\"   1️⃣ REWARD BASE: Mudança no portfolio (atual)\")\n",
    "print(\"   2️⃣ BÔNUS DE AÇÃO: +5 por executar BUY/SELL com sucesso\") \n",
    "print(\"   3️⃣ PENALIDADE DE INAÇÃO: -2 por HOLD excessivo\")\n",
    "print(\"   4️⃣ PENALIDADE DE FALHA: -10 por tentar ação impossível\")\n",
    "print(\"   5️⃣ BÔNUS DE TIMING: +20 por boa decisão (comprar na baixa, vender na alta)\")\n",
    "print(\"   6️⃣ INCENTIVO DE EXPLORAÇÃO: +1 por visitar estado novo\")\n",
    "\n",
    "print(f\"\\n📊 FÓRMULA PROPOSTA:\")\n",
    "print(f\"   reward = portfolio_change + action_bonus + timing_bonus + exploration_bonus - penalties\")\n",
    "\n",
    "print(f\"\\n🎮 EXEMPLOS DO NOVO SISTEMA:\")\n",
    "print(f\"   • Comprar e preço subir: +100 (portfolio) +5 (ação) +20 (timing) = +125\")\n",
    "print(f\"   • Vender na alta: +80 (portfolio) +5 (ação) +20 (timing) = +105\")  \n",
    "print(f\"   • HOLD por muitos passos: 0 (portfolio) -2 (inação) = -2\")\n",
    "print(f\"   • Tentar comprar sem cash: 0 (portfolio) -10 (falha) = -10\")\n",
    "print(f\"   • Estado nunca visitado: reward atual +1 (exploração)\")\n",
    "\n",
    "print(f\"\\n⚙️ PARÂMETROS CONFIGURÁVEIS:\")\n",
    "print(f\"   ACTION_BONUS = 5          # Bônus por executar ação\")\n",
    "print(f\"   INACTION_PENALTY = -2     # Penalidade por HOLD\")\n",
    "print(f\"   FAILURE_PENALTY = -10     # Penalidade por ação falhada\")\n",
    "print(f\"   TIMING_BONUS = 20         # Bônus por bom timing\")\n",
    "print(f\"   EXPLORATION_BONUS = 1     # Bônus por estado novo\")\n",
    "print(f\"   HOLD_TOLERANCE = 3        # Max HOLDs seguidos sem penalidade\")\n",
    "\n",
    "print(f\"\\n🧠 BENEFÍCIOS ESPERADOS:\")\n",
    "print(f\"   ✅ Incentiva trading ativo vs passivo\")\n",
    "print(f\"   ✅ Pune tentativas inválidas (aprende restrições)\")\n",
    "print(f\"   ✅ Premia bom timing (essência do trading)\")\n",
    "print(f\"   ✅ Encoraja exploração (Q-table cresce)\")\n",
    "print(f\"   ✅ Balanceado - não só portfolio matters\")\n",
    "\n",
    "print(f\"\\n🤔 QUER IMPLEMENTAR ESTE SISTEMA?\")\n",
    "print(f\"   Responda 'sim' para implementar ou sugira modificações!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17bb32e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 SIMULAÇÃO: COMO FUNCIONARIA O NOVO SISTEMA\n",
      "============================================================\n",
      "📊 COMPARAÇÃO: Sistema Atual vs Proposto\n",
      "------------------------------------------------------------\n",
      "Compra bem sucedida (preço sobe)   \n",
      "   Atual: +100 | Proposto: +125 | Diferença:  +25\n",
      "\n",
      "Venda inteligente (preço sobe após)\n",
      "   Atual:  +80 | Proposto: +105 | Diferença:  +25\n",
      "\n",
      "HOLD excessivo (5º seguido)        \n",
      "   Atual:   +0 | Proposto:   -2 | Diferença:   -2\n",
      "\n",
      "Tentativa de compra sem cash       \n",
      "   Atual:   +0 | Proposto:  -10 | Diferença:  -10\n",
      "\n",
      "Exploração (estado novo)           \n",
      "   Atual:  -20 | Proposto:  -14 | Diferença:   +6\n",
      "\n",
      "💡 OBSERVE: O novo sistema diferencia melhor entre boas e más decisões!\n"
     ]
    }
   ],
   "source": [
    "# 🧪 SIMULAÇÃO DO NOVO SISTEMA DE REWARDS\n",
    "print(\"\\n🧪 SIMULAÇÃO: COMO FUNCIONARIA O NOVO SISTEMA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Parâmetros do novo sistema\n",
    "ACTION_BONUS = 5\n",
    "INACTION_PENALTY = -2  \n",
    "FAILURE_PENALTY = -10\n",
    "TIMING_BONUS = 20\n",
    "EXPLORATION_BONUS = 1\n",
    "HOLD_TOLERANCE = 3\n",
    "\n",
    "def calculate_new_reward(portfolio_change, action, action_executed, price_change, is_new_state, consecutive_holds):\n",
    "    \"\"\"Simula o novo sistema de rewards\"\"\"\n",
    "    reward = portfolio_change  # Base reward (atual)\n",
    "    \n",
    "    # Bônus por executar ação\n",
    "    if action != 0 and action_executed:  # BUY/SELL executado\n",
    "        reward += ACTION_BONUS\n",
    "        \n",
    "    # Bônus de timing (boa decisão)\n",
    "    if action == 1 and action_executed and price_change > 0:  # Comprou e preço subiu\n",
    "        reward += TIMING_BONUS\n",
    "    elif action == 2 and action_executed and price_change > 0:  # Vendeu e preço subiu (bom!)\n",
    "        reward += TIMING_BONUS\n",
    "        \n",
    "    # Penalidade por inação excessiva\n",
    "    if action == 0 and consecutive_holds > HOLD_TOLERANCE:\n",
    "        reward += INACTION_PENALTY\n",
    "        \n",
    "    # Penalidade por tentar ação impossível\n",
    "    if action != 0 and not action_executed:\n",
    "        reward += FAILURE_PENALTY\n",
    "        \n",
    "    # Bônus por exploração\n",
    "    if is_new_state:\n",
    "        reward += EXPLORATION_BONUS\n",
    "        \n",
    "    return reward\n",
    "\n",
    "# Cenários de teste\n",
    "scenarios = [\n",
    "    {\"name\": \"Compra bem sucedida (preço sobe)\", \"portfolio_change\": 100, \"action\": 1, \"executed\": True, \"price_change\": 0.02, \"new_state\": False, \"holds\": 0},\n",
    "    {\"name\": \"Venda inteligente (preço sobe após)\", \"portfolio_change\": 80, \"action\": 2, \"executed\": True, \"price_change\": 0.03, \"new_state\": False, \"holds\": 0},\n",
    "    {\"name\": \"HOLD excessivo (5º seguido)\", \"portfolio_change\": 0, \"action\": 0, \"executed\": True, \"price_change\": 0, \"new_state\": False, \"holds\": 5},\n",
    "    {\"name\": \"Tentativa de compra sem cash\", \"portfolio_change\": 0, \"action\": 1, \"executed\": False, \"price_change\": 0, \"new_state\": False, \"holds\": 0},\n",
    "    {\"name\": \"Exploração (estado novo)\", \"portfolio_change\": -20, \"action\": 1, \"executed\": True, \"price_change\": -0.01, \"new_state\": True, \"holds\": 0},\n",
    "]\n",
    "\n",
    "print(\"📊 COMPARAÇÃO: Sistema Atual vs Proposto\")\n",
    "print(\"-\" * 60)\n",
    "for scenario in scenarios:\n",
    "    current_reward = scenario[\"portfolio_change\"]\n",
    "    new_reward = calculate_new_reward(\n",
    "        scenario[\"portfolio_change\"], \n",
    "        scenario[\"action\"], \n",
    "        scenario[\"executed\"],\n",
    "        scenario[\"price_change\"],\n",
    "        scenario[\"new_state\"],\n",
    "        scenario[\"holds\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"{scenario['name']:35s}\")\n",
    "    print(f\"   Atual: {current_reward:+4.0f} | Proposto: {new_reward:+4.0f} | Diferença: {new_reward - current_reward:+4.0f}\")\n",
    "    print()\n",
    "\n",
    "print(\"💡 OBSERVE: O novo sistema diferencia melhor entre boas e más decisões!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0be72f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ CONFIGURANDO SISTEMA BATMAN SMART REWARDS\n",
      "============================================================\n",
      "📊 PARÂMETROS CONFIGURADOS:\n",
      "   ACTION_BONUS        : 5\n",
      "   INACTION_PENALTY    : -2\n",
      "   FAILURE_PENALTY     : -10\n",
      "   TIMING_BONUS        : 20\n",
      "   EXPLORATION_BONUS   : 1\n",
      "   HOLD_TOLERANCE      : 3\n",
      "   ENABLED             : True\n",
      "\n",
      "💡 PARA AJUSTAR:\n",
      "   • Modifique os valores acima e re-execute as células seguintes\n",
      "   • Set ENABLED=False para voltar ao sistema original\n",
      "   • Experimente diferentes combinações para otimizar aprendizagem\n"
     ]
    }
   ],
   "source": [
    "# ⚙️ CONFIGURAÇÃO DO SISTEMA DE REWARDS MELHORADO\n",
    "print(\"⚙️ CONFIGURANDO SISTEMA BATMAN SMART REWARDS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Parâmetros do novo sistema de rewards (configuráveis)\n",
    "SMART_REWARDS_CONFIG = {\n",
    "    'ACTION_BONUS': 5,          # Bônus por executar BUY/SELL com sucesso\n",
    "    'INACTION_PENALTY': -2,     # Penalidade por HOLD excessivo\n",
    "    'FAILURE_PENALTY': -10,     # Penalidade por tentar ação impossível\n",
    "    'TIMING_BONUS': 20,         # Bônus por bom timing (comprar baixo, vender alto)\n",
    "    'EXPLORATION_BONUS': 1,     # Bônus por visitar estado novo\n",
    "    'HOLD_TOLERANCE': 3,        # Máximo de HOLDs seguidos sem penalidade\n",
    "    'ENABLED': True             # Ativar/desativar sistema melhorado\n",
    "}\n",
    "\n",
    "print(\"📊 PARÂMETROS CONFIGURADOS:\")\n",
    "for param, value in SMART_REWARDS_CONFIG.items():\n",
    "    print(f\"   {param:20s}: {value}\")\n",
    "\n",
    "print(\"\\n💡 PARA AJUSTAR:\")\n",
    "print(\"   • Modifique os valores acima e re-execute as células seguintes\")\n",
    "print(\"   • Set ENABLED=False para voltar ao sistema original\")\n",
    "print(\"   • Experimente diferentes combinações para otimizar aprendizagem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ced8578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Classe BatmanSmartTradingEnvironment implementada!\n",
      "   📊 Suporte a rewards configuráveis\n",
      "   🎯 Tracking de exploração automático\n",
      "   📈 Componentes de reward detalhados\n"
     ]
    }
   ],
   "source": [
    "# 🚀 AMBIENTE BATMAN COM SMART REWARDS IMPLEMENTADO\n",
    "class BatmanSmartTradingEnvironment(BatmanTradingEnvironment):\n",
    "    \"\"\"\n",
    "    Versão melhorada do ambiente Batman com sistema de rewards inteligente\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, prices, state_manager, initial_capital=10000.0, smart_config=None):\n",
    "        # Inicializar atributos antes de chamar super().__init__()\n",
    "        self.smart_config = smart_config or SMART_REWARDS_CONFIG\n",
    "        self.visited_states = set()\n",
    "        self.consecutive_holds = 0\n",
    "        self.previous_price = None\n",
    "        \n",
    "        # Agora chamar o construtor pai\n",
    "        super().__init__(prices, state_manager, initial_capital)\n",
    "        \n",
    "        print(\"🚀 Batman Smart Trading Environment inicializado!\")\n",
    "        if self.smart_config['ENABLED']:\n",
    "            print(\"   ✅ Smart Rewards: ATIVADO\")\n",
    "            print(f\"   🎯 Action Bonus: {self.smart_config['ACTION_BONUS']}\")\n",
    "            print(f\"   ⚠️ Failure Penalty: {self.smart_config['FAILURE_PENALTY']}\")\n",
    "            print(f\"   🕰️ Timing Bonus: {self.smart_config['TIMING_BONUS']}\")\n",
    "        else:\n",
    "            print(\"   ⚪ Smart Rewards: DESATIVADO (modo clássico)\")\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia ambiente com tracking de smart rewards\"\"\"\n",
    "        state = super().reset()\n",
    "        \n",
    "        # Reset smart rewards tracking\n",
    "        self.visited_states.clear()\n",
    "        self.consecutive_holds = 0\n",
    "        self.previous_price = self.prices[self.current_step] if len(self.prices) > self.current_step else None\n",
    "        \n",
    "        # Marcar estado inicial como visitado\n",
    "        self.visited_states.add(state)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def calculate_smart_reward(self, base_reward, action, action_executed, current_price, state):\n",
    "        \"\"\"Calcula reward usando sistema inteligente\"\"\"\n",
    "        \n",
    "        if not self.smart_config['ENABLED']:\n",
    "            return base_reward\n",
    "            \n",
    "        smart_reward = base_reward  # Começa com reward base (mudança portfolio)\n",
    "        reward_components = {'base': base_reward}\n",
    "        \n",
    "        # 1. Bônus por executar ação (incentiva trading ativo)\n",
    "        if action != Actions.HOLD and action_executed:\n",
    "            action_bonus = self.smart_config['ACTION_BONUS']\n",
    "            smart_reward += action_bonus\n",
    "            reward_components['action_bonus'] = action_bonus\n",
    "        \n",
    "        # 2. Penalidade por falha (aprende restrições)\n",
    "        if action != Actions.HOLD and not action_executed:\n",
    "            failure_penalty = self.smart_config['FAILURE_PENALTY']\n",
    "            smart_reward += failure_penalty  # Penalty é negativo\n",
    "            reward_components['failure_penalty'] = failure_penalty\n",
    "        \n",
    "        # 3. Penalidade por inação excessiva\n",
    "        if action == Actions.HOLD:\n",
    "            self.consecutive_holds += 1\n",
    "            if self.consecutive_holds > self.smart_config['HOLD_TOLERANCE']:\n",
    "                inaction_penalty = self.smart_config['INACTION_PENALTY']\n",
    "                smart_reward += inaction_penalty  # Penalty é negativo\n",
    "                reward_components['inaction_penalty'] = inaction_penalty\n",
    "        else:\n",
    "            self.consecutive_holds = 0\n",
    "        \n",
    "        # 4. Bônus de timing (premia boas decisões)\n",
    "        if self.previous_price is not None and action_executed:\n",
    "            price_change = (current_price - self.previous_price) / self.previous_price\n",
    "            \n",
    "            # Comprou e preço subiu = bom timing\n",
    "            if action == Actions.BUY and price_change > 0:\n",
    "                timing_bonus = self.smart_config['TIMING_BONUS']\n",
    "                smart_reward += timing_bonus\n",
    "                reward_components['timing_bonus'] = timing_bonus\n",
    "                \n",
    "            # Vendeu antes da alta = bom timing (conservador)\n",
    "            elif action == Actions.SELL and price_change > 0.01:  # 1% de alta\n",
    "                timing_bonus = self.smart_config['TIMING_BONUS']\n",
    "                smart_reward += timing_bonus\n",
    "                reward_components['timing_bonus'] = timing_bonus\n",
    "        \n",
    "        # 5. Bônus de exploração (incentiva visitar novos estados)\n",
    "        if state not in self.visited_states:\n",
    "            exploration_bonus = self.smart_config['EXPLORATION_BONUS']\n",
    "            smart_reward += exploration_bonus\n",
    "            reward_components['exploration_bonus'] = exploration_bonus\n",
    "            self.visited_states.add(state)\n",
    "        \n",
    "        # Atualizar preço anterior\n",
    "        self.previous_price = current_price\n",
    "        \n",
    "        return smart_reward, reward_components\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Executa ação com sistema de rewards inteligente\"\"\"\n",
    "        current_price = self.prices[self.current_step]\n",
    "        portfolio_value_before = self.get_portfolio_value()\n",
    "        \n",
    "        # Executar ação (mesmo código do ambiente original)\n",
    "        action_executed = False\n",
    "        if action == Actions.BUY and self.cash >= current_price:\n",
    "            self.shares += 1\n",
    "            self.cash -= current_price\n",
    "            action_executed = True\n",
    "            \n",
    "        elif action == Actions.SELL and self.shares > 0:\n",
    "            self.shares -= 1\n",
    "            self.cash += current_price\n",
    "            action_executed = True\n",
    "            \n",
    "        # HOLD sempre é válido\n",
    "        if action == Actions.HOLD:\n",
    "            action_executed = True\n",
    "        \n",
    "        # Calcular reward base (mudança no portfolio)\n",
    "        portfolio_value_after = self.get_portfolio_value()\n",
    "        base_reward = portfolio_value_after - portfolio_value_before\n",
    "        \n",
    "        # Estado atual para análise de exploração\n",
    "        current_state = self.get_current_state()\n",
    "        \n",
    "        # Aplicar smart rewards\n",
    "        smart_reward, reward_components = self.calculate_smart_reward(\n",
    "            base_reward, action, action_executed, current_price, current_state\n",
    "        )\n",
    "        \n",
    "        # Registrar histórico (usar smart reward)\n",
    "        self.portfolio_values.append(portfolio_value_after)\n",
    "        self.actions_history.append(action)\n",
    "        self.episode_rewards.append(smart_reward)\n",
    "        \n",
    "        # Avançar para próximo dia\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.prices) - 1\n",
    "        \n",
    "        # Próximo estado\n",
    "        next_state = self.get_current_state() if not done else None\n",
    "        \n",
    "        # Informações detalhadas (incluindo componentes do reward)\n",
    "        info = {\n",
    "            'cash': self.cash,\n",
    "            'shares': self.shares, \n",
    "            'portfolio_value': portfolio_value_after,\n",
    "            'current_price': current_price,\n",
    "            'action_executed': action_executed,\n",
    "            'day': self.current_step,\n",
    "            'base_reward': base_reward,\n",
    "            'smart_reward': smart_reward,\n",
    "            'reward_components': reward_components,\n",
    "            'consecutive_holds': self.consecutive_holds,\n",
    "            'states_explored': len(self.visited_states)\n",
    "        }\n",
    "        \n",
    "        return next_state, smart_reward, done, info\n",
    "\n",
    "print(\"🔧 Classe BatmanSmartTradingEnvironment implementada!\")\n",
    "print(\"   📊 Suporte a rewards configuráveis\")  \n",
    "print(\"   🎯 Tracking de exploração automático\")\n",
    "print(\"   📈 Componentes de reward detalhados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ece7caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 TESTANDO BATMAN SMART REWARDS\n",
      "============================================================\n",
      "🚀 Batman Smart Trading Environment inicializado!\n",
      "   ✅ Smart Rewards: ATIVADO\n",
      "   🎯 Action Bonus: 5\n",
      "   ⚠️ Failure Penalty: -10\n",
      "   🕰️ Timing Bonus: 20\n",
      "\n",
      "🎮 TESTE COMPARATIVO: Clássico vs Smart\n",
      "----------------------------------------\n",
      "\n",
      "📊 Cenário: Compra bem-sucedida\n",
      "   Clássico: Reward =   +0.00 | Executada = True\n",
      "   Smart:    Reward =   +5.00 | Executada = True\n",
      "   Componentes Smart: {'base': np.float64(0.0), 'action_bonus': 5}\n",
      "   Diferença:   +5.00 (+melhor)\n",
      "\n",
      "📊 Cenário: Tentativa de compra sem cash\n",
      "   Clássico: Reward =   +0.00 | Executada = True\n",
      "   Smart:    Reward =   +5.00 | Executada = True\n",
      "   Componentes Smart: {'base': np.float64(0.0), 'action_bonus': 5}\n",
      "   Diferença:   +5.00 (+melhor)\n",
      "\n",
      "📊 Cenário: HOLD normal\n",
      "   Clássico: Reward =   +0.00 | Executada = True\n",
      "   Smart:    Reward =   +0.00 | Executada = True\n",
      "   Componentes Smart: {'base': np.float64(0.0)}\n",
      "   Diferença:   +0.00 (igual)\n",
      "\n",
      "📊 Cenário: Venda bem-sucedida\n",
      "   Clássico: Reward =   +0.00 | Executada = True\n",
      "   Smart:    Reward =   +5.00 | Executada = True\n",
      "   Componentes Smart: {'base': np.float64(0.0), 'action_bonus': 5}\n",
      "   Diferença:   +5.00 (+melhor)\n"
     ]
    }
   ],
   "source": [
    "# 🧪 TESTE DO NOVO SISTEMA SMART REWARDS\n",
    "print(\"🧪 TESTANDO BATMAN SMART REWARDS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Criar ambiente com smart rewards\n",
    "if df_stock is not None:\n",
    "    smart_env = BatmanSmartTradingEnvironment(prices, state_manager, INITIAL_CAPITAL, SMART_REWARDS_CONFIG)\n",
    "    \n",
    "    print(f\"\\n🎮 TESTE COMPARATIVO: Clássico vs Smart\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Cenários de teste\n",
    "    test_scenarios = [\n",
    "        {\"name\": \"Compra bem-sucedida\", \"action\": Actions.BUY},\n",
    "        {\"name\": \"Tentativa de compra sem cash\", \"action\": Actions.BUY, \"low_cash\": True},\n",
    "        {\"name\": \"HOLD normal\", \"action\": Actions.HOLD},\n",
    "        {\"name\": \"Venda bem-sucedida\", \"action\": Actions.SELL, \"setup_shares\": True}\n",
    "    ]\n",
    "    \n",
    "    for scenario in test_scenarios:\n",
    "        print(f\"\\n📊 Cenário: {scenario['name']}\")\n",
    "        \n",
    "        # Ambiente clássico\n",
    "        classic_env = BatmanTradingEnvironment(prices, state_manager, INITIAL_CAPITAL)\n",
    "        classic_state = classic_env.reset()\n",
    "        \n",
    "        # Ambiente smart  \n",
    "        smart_state = smart_env.reset()\n",
    "        \n",
    "        # Setup especial para cenários\n",
    "        if scenario.get('low_cash'):\n",
    "            classic_env.cash = 10.0  # Pouco cash\n",
    "            smart_env.cash = 10.0\n",
    "            \n",
    "        if scenario.get('setup_shares'):\n",
    "            classic_env.shares = 1  # Ter shares para vender\n",
    "            smart_env.shares = 1\n",
    "            classic_env.cash -= classic_env.prices[classic_env.current_step]\n",
    "            smart_env.cash -= smart_env.prices[smart_env.current_step]\n",
    "        \n",
    "        # Executar ação em ambos\n",
    "        classic_next, classic_reward, classic_done, classic_info = classic_env.step(scenario['action'])\n",
    "        smart_next, smart_reward, smart_done, smart_info = smart_env.step(scenario['action'])\n",
    "        \n",
    "        # Comparar resultados\n",
    "        print(f\"   Clássico: Reward = {classic_reward:+7.2f} | Executada = {classic_info['action_executed']}\")\n",
    "        print(f\"   Smart:    Reward = {smart_reward:+7.2f} | Executada = {smart_info['action_executed']}\")\n",
    "        \n",
    "        if 'reward_components' in smart_info:\n",
    "            components = smart_info['reward_components']\n",
    "            print(f\"   Componentes Smart: {components}\")\n",
    "        \n",
    "        difference = smart_reward - classic_reward\n",
    "        print(f\"   Diferença: {difference:+7.2f} ({'+' if difference > 0 else ''}{'melhor' if difference != 0 else 'igual'})\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Dados não disponíveis para teste!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1798a9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦇 Batman Smart Q-Learning Agent implementado!\n",
      "   📊 Tracking detalhado de componentes de reward\n",
      "   🎯 Estatísticas de exploração automáticas\n"
     ]
    }
   ],
   "source": [
    "# 🎯 AGENTE BATMAN ATUALIZADO PARA SMART REWARDS\n",
    "class BatmanSmartQLearningAgent(BatmanQLearningAgent):\n",
    "    \"\"\"\n",
    "    Versão do agente Batman otimizada para Smart Rewards\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.95, \n",
    "                 epsilon_start=1.0, epsilon_min=0.01, epsilon_decay=0.995, smart_config=None):\n",
    "        super().__init__(learning_rate, discount_factor, epsilon_start, epsilon_min, epsilon_decay)\n",
    "        self.smart_config = smart_config or SMART_REWARDS_CONFIG\n",
    "        \n",
    "        # Estatísticas específicas do smart rewards\n",
    "        self.reward_component_history = []\n",
    "        self.exploration_stats = []\n",
    "        \n",
    "        print(\"🚀 Batman Smart Q-Learning Agent inicializado!\")\n",
    "        print(f\"   🧠 Otimizado para sistema de rewards inteligente\")\n",
    "        if self.smart_config['ENABLED']:\n",
    "            print(f\"   ✅ Smart Rewards ativo\")\n",
    "        else:\n",
    "            print(f\"   ⚪ Modo clássico\")\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        \"\"\"Treina episódio com tracking de smart rewards\"\"\"\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        exploration_count = 0\n",
    "        episode_components = {'action_bonus': 0, 'timing_bonus': 0, 'exploration_bonus': 0, \n",
    "                            'failure_penalty': 0, 'inaction_penalty': 0}\n",
    "        \n",
    "        while True:\n",
    "            # Escolher ação\n",
    "            action, is_exploration = self.get_action(state, training=True)\n",
    "            if is_exploration:\n",
    "                exploration_count += 1\n",
    "                \n",
    "            # Executar ação\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Tracking de componentes smart rewards\n",
    "            if 'reward_components' in info:\n",
    "                for component, value in info['reward_components'].items():\n",
    "                    if component in episode_components:\n",
    "                        episode_components[component] += value\n",
    "            \n",
    "            # Atualizar Q-value\n",
    "            self.update_q_value(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Acumular recompensa\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "        \n",
    "        # Decair epsilon\n",
    "        self.decay_epsilon()\n",
    "        \n",
    "        # Registrar estatísticas\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        self.exploration_counts.append(exploration_count)\n",
    "        self.reward_component_history.append(episode_components.copy())\n",
    "        \n",
    "        # Calcular retorno do episódio\n",
    "        episode_summary = env.get_episode_summary()\n",
    "        if episode_summary:\n",
    "            self.episode_returns.append(episode_summary['total_return'])\n",
    "            \n",
    "        # Estatísticas de exploração (se ambiente suporta)\n",
    "        if hasattr(env, 'visited_states'):\n",
    "            self.exploration_stats.append(len(env.visited_states))\n",
    "        \n",
    "        return episode_summary\n",
    "    \n",
    "    def get_smart_training_stats(self):\n",
    "        \"\"\"Retorna estatísticas completas incluindo componentes smart\"\"\"\n",
    "        base_stats = self.get_training_stats()\n",
    "        \n",
    "        if not base_stats or not self.reward_component_history:\n",
    "            return base_stats\n",
    "            \n",
    "        # Estatísticas dos últimos 100 episódios\n",
    "        recent_components = self.reward_component_history[-100:]\n",
    "        recent_exploration = self.exploration_stats[-100:] if self.exploration_stats else []\n",
    "        \n",
    "        smart_stats = {\n",
    "            'avg_action_bonus': np.mean([ep['action_bonus'] for ep in recent_components]),\n",
    "            'avg_timing_bonus': np.mean([ep['timing_bonus'] for ep in recent_components]),\n",
    "            'avg_exploration_bonus': np.mean([ep['exploration_bonus'] for ep in recent_components]),\n",
    "            'avg_failure_penalty': np.mean([ep['failure_penalty'] for ep in recent_components]),\n",
    "            'avg_inaction_penalty': np.mean([ep['inaction_penalty'] for ep in recent_components]),\n",
    "            'avg_states_per_episode': np.mean(recent_exploration) if recent_exploration else 0\n",
    "        }\n",
    "        \n",
    "        # Combinar com estatísticas base\n",
    "        base_stats.update(smart_stats)\n",
    "        return base_stats\n",
    "\n",
    "print(\"🦇 Batman Smart Q-Learning Agent implementado!\")\n",
    "print(\"   📊 Tracking detalhado de componentes de reward\")\n",
    "print(\"   🎯 Estatísticas de exploração automáticas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27bf4fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 INICIALIZANDO SISTEMA BATMAN SMART COMPLETO\n",
      "============================================================\n",
      "🚀 Batman Smart Trading Environment inicializado!\n",
      "   ✅ Smart Rewards: ATIVADO\n",
      "   🎯 Action Bonus: 5\n",
      "   ⚠️ Failure Penalty: -10\n",
      "   🕰️ Timing Bonus: 20\n",
      "🦇 Agente Batman Q-Learning inicializado!\n",
      "   🧠 Learning rate: 0.1\n",
      "   💰 Discount factor: 0.95\n",
      "   🔍 Epsilon inicial: 1.0\n",
      "🚀 Batman Smart Q-Learning Agent inicializado!\n",
      "   🧠 Otimizado para sistema de rewards inteligente\n",
      "   ✅ Smart Rewards ativo\n",
      "\n",
      "✅ SISTEMA SMART INICIALIZADO:\n",
      "   🏛️ Ambiente: Batman Smart Trading Environment\n",
      "   🦇 Agente: Batman Smart Q-Learning Agent\n",
      "   💰 Capital: R$ 50,000.00\n",
      "   📊 Ativo: PETR3.SA\n",
      "   🎯 Smart Rewards: ATIVO\n",
      "\n",
      "📈 CONFIGURAÇÃO SMART REWARDS:\n",
      "   Action Bonus: 5\n",
      "   Timing Bonus: 20\n",
      "   Exploration Bonus: 1\n",
      "   Failure Penalty: -10\n",
      "   Inaction Penalty: -2\n",
      "   Hold Tolerance: 3\n",
      "\n",
      "🎮 PRONTO PARA TREINAMENTO!\n",
      "   Use: train_batman_smart_agent(smart_agent, smart_env, NUM_EPISODES)\n"
     ]
    }
   ],
   "source": [
    "# 🚀 INICIALIZAÇÃO DO SISTEMA SMART COMPLETO\n",
    "print(\"🚀 INICIALIZANDO SISTEMA BATMAN SMART COMPLETO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Criar ambiente e agente com smart rewards\n",
    "if df_stock is not None:\n",
    "    # Ambiente smart\n",
    "    smart_env = BatmanSmartTradingEnvironment(prices, state_manager, INITIAL_CAPITAL, SMART_REWARDS_CONFIG)\n",
    "    \n",
    "    # Agente smart\n",
    "    smart_agent = BatmanSmartQLearningAgent(\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        discount_factor=DISCOUNT_FACTOR,\n",
    "        epsilon_start=EPSILON_START,\n",
    "        epsilon_min=EPSILON_MIN,\n",
    "        epsilon_decay=EPSILON_DECAY,\n",
    "        smart_config=SMART_REWARDS_CONFIG\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ SISTEMA SMART INICIALIZADO:\")\n",
    "    print(f\"   🏛️ Ambiente: Batman Smart Trading Environment\")\n",
    "    print(f\"   🦇 Agente: Batman Smart Q-Learning Agent\")\n",
    "    print(f\"   💰 Capital: R$ {INITIAL_CAPITAL:,.2f}\")\n",
    "    print(f\"   📊 Ativo: {TICKER_SYMBOL}\")\n",
    "    print(f\"   🎯 Smart Rewards: {'ATIVO' if SMART_REWARDS_CONFIG['ENABLED'] else 'INATIVO'}\")\n",
    "    \n",
    "    if SMART_REWARDS_CONFIG['ENABLED']:\n",
    "        print(f\"\\n📈 CONFIGURAÇÃO SMART REWARDS:\")\n",
    "        print(f\"   Action Bonus: {SMART_REWARDS_CONFIG['ACTION_BONUS']}\")\n",
    "        print(f\"   Timing Bonus: {SMART_REWARDS_CONFIG['TIMING_BONUS']}\")\n",
    "        print(f\"   Exploration Bonus: {SMART_REWARDS_CONFIG['EXPLORATION_BONUS']}\")\n",
    "        print(f\"   Failure Penalty: {SMART_REWARDS_CONFIG['FAILURE_PENALTY']}\")\n",
    "        print(f\"   Inaction Penalty: {SMART_REWARDS_CONFIG['INACTION_PENALTY']}\")\n",
    "        print(f\"   Hold Tolerance: {SMART_REWARDS_CONFIG['HOLD_TOLERANCE']}\")\n",
    "    \n",
    "    print(f\"\\n🎮 PRONTO PARA TREINAMENTO!\")\n",
    "    print(f\"   Use: train_batman_smart_agent(smart_agent, smart_env, NUM_EPISODES)\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Erro: Dados não disponíveis para inicialização!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "165eccd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏋️ Função de treinamento Batman Smart implementada!\n",
      "   📊 Monitoramento completo de componentes smart\n",
      "   🎯 Relatórios detalhados de exploração\n",
      "   📈 Tracking de todos os bônus e penalidades\n"
     ]
    }
   ],
   "source": [
    "# 🏋️ TREINAMENTO BATMAN SMART COM MONITORAMENTO AVANÇADO\n",
    "def train_batman_smart_agent(agent, env, num_episodes, print_every=100):\n",
    "    \"\"\"\n",
    "    Treina o agente Batman Smart com monitoramento detalhado de componentes\n",
    "    \"\"\"\n",
    "    print(f\"🚀 INICIANDO TREINAMENTO BATMAN SMART\")\n",
    "    print(f\"📊 {num_episodes} episódios | Relatórios a cada {print_every}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    training_history = {\n",
    "        'episode': [],\n",
    "        'avg_reward': [],\n",
    "        'avg_return': [],\n",
    "        'epsilon': [],\n",
    "        'q_table_size': [],\n",
    "        # Smart rewards específicos\n",
    "        'avg_action_bonus': [],\n",
    "        'avg_timing_bonus': [],\n",
    "        'avg_exploration_bonus': [],\n",
    "        'avg_failure_penalty': [],\n",
    "        'avg_inaction_penalty': [],\n",
    "        'avg_states_explored': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        # Treinar episódio\n",
    "        episode_summary = agent.train_episode(env)\n",
    "        \n",
    "        # Relatório periódico\n",
    "        if episode % print_every == 0:\n",
    "            stats = agent.get_smart_training_stats()\n",
    "            \n",
    "            print(f\"📈 Episódio {episode}/{num_episodes}\")\n",
    "            print(f\"   💰 Reward médio (últimos 100): {stats['avg_reward']:+.2f}\")\n",
    "            print(f\"   📊 Retorno médio (últimos 100): {stats['avg_return']:+.2%}\")\n",
    "            print(f\"   🔍 Epsilon atual: {stats['current_epsilon']:.3f}\")\n",
    "            print(f\"   🧠 Tamanho Q-table: {stats['q_table_size']:,}\")\n",
    "            print(f\"   🎯 Exploração média: {stats['avg_exploration']:.1f} ações/episódio\")\n",
    "            \n",
    "            # Componentes smart rewards\n",
    "            if SMART_REWARDS_CONFIG['ENABLED']:\n",
    "                print(f\"   🚀 SMART REWARDS MÉDIOS:\")\n",
    "                print(f\"      Action Bonus: {stats['avg_action_bonus']:+.1f}\")\n",
    "                print(f\"      Timing Bonus: {stats['avg_timing_bonus']:+.1f}\")\n",
    "                print(f\"      Exploration: {stats['avg_exploration_bonus']:+.1f}\")\n",
    "                print(f\"      Failure Penalty: {stats['avg_failure_penalty']:+.1f}\")\n",
    "                print(f\"      Inaction Penalty: {stats['avg_inaction_penalty']:+.1f}\")\n",
    "                print(f\"      Estados/Episódio: {stats['avg_states_per_episode']:.1f}\")\n",
    "            \n",
    "            if episode_summary:\n",
    "                print(f\"   💵 Último episódio: R$ {episode_summary['final_value']:,.2f} \" + \n",
    "                      f\"({episode_summary['total_return']:+.2%})\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Salvar histórico\n",
    "            training_history['episode'].append(episode)\n",
    "            training_history['avg_reward'].append(stats['avg_reward'])\n",
    "            training_history['avg_return'].append(stats['avg_return'])\n",
    "            training_history['epsilon'].append(stats['current_epsilon'])\n",
    "            training_history['q_table_size'].append(stats['q_table_size'])\n",
    "            \n",
    "            # Smart rewards\n",
    "            if SMART_REWARDS_CONFIG['ENABLED']:\n",
    "                training_history['avg_action_bonus'].append(stats['avg_action_bonus'])\n",
    "                training_history['avg_timing_bonus'].append(stats['avg_timing_bonus'])\n",
    "                training_history['avg_exploration_bonus'].append(stats['avg_exploration_bonus'])\n",
    "                training_history['avg_failure_penalty'].append(stats['avg_failure_penalty'])\n",
    "                training_history['avg_inaction_penalty'].append(stats['avg_inaction_penalty'])\n",
    "                training_history['avg_states_explored'].append(stats['avg_states_per_episode'])\n",
    "    \n",
    "    print(\"✅ TREINAMENTO BATMAN SMART CONCLUÍDO!\")\n",
    "    final_stats = agent.get_smart_training_stats()\n",
    "    print(f\"📊 ESTATÍSTICAS FINAIS:\")\n",
    "    print(f\"   🧠 Q-table final: {final_stats['q_table_size']:,} estados\")\n",
    "    print(f\"   🎯 Epsilon final: {final_stats['current_epsilon']:.3f}\")\n",
    "    print(f\"   💰 Reward médio final: {final_stats['avg_reward']:+.2f}\")\n",
    "    print(f\"   📈 Retorno médio final: {final_stats['avg_return']:+.2%}\")\n",
    "    \n",
    "    if SMART_REWARDS_CONFIG['ENABLED']:\n",
    "        print(f\"   🚀 SMART REWARDS FINAIS:\")\n",
    "        print(f\"      Total Action Bonus: {final_stats['avg_action_bonus']:+.1f}\")\n",
    "        print(f\"      Total Timing Bonus: {final_stats['avg_timing_bonus']:+.1f}\")\n",
    "        print(f\"      Estados explorados: {final_stats['avg_states_per_episode']:.1f}/episódio\")\n",
    "    \n",
    "    return training_history\n",
    "\n",
    "print(\"🏋️ Função de treinamento Batman Smart implementada!\")\n",
    "print(\"   📊 Monitoramento completo de componentes smart\")\n",
    "print(\"   🎯 Relatórios detalhados de exploração\")\n",
    "print(\"   📈 Tracking de todos os bônus e penalidades\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49878a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎮 EXECUTANDO TREINAMENTO BATMAN SMART\n",
      "============================================================\n",
      "🚀 Iniciando treinamento para PETR3.SA com Smart Rewards\n",
      "   📊 Episódios: 1000\n",
      "   🎯 Sistema: Smart\n",
      "🚀 INICIANDO TREINAMENTO BATMAN SMART\n",
      "📊 1000 episódios | Relatórios a cada 200\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 🎮 EXECUTAR TREINAMENTO BATMAN SMART\n",
    "print(\"🎮 EXECUTANDO TREINAMENTO BATMAN SMART\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Executar treinamento com sistema smart\n",
    "if 'smart_env' in locals() and 'smart_agent' in locals():\n",
    "    print(f\"🚀 Iniciando treinamento para {TICKER_SYMBOL} com Smart Rewards\")\n",
    "    print(f\"   📊 Episódios: {NUM_EPISODES}\")\n",
    "    print(f\"   🎯 Sistema: {'Smart' if SMART_REWARDS_CONFIG['ENABLED'] else 'Clássico'}\")\n",
    "    \n",
    "    # Treinamento\n",
    "    smart_training_history = train_batman_smart_agent(smart_agent, smart_env, NUM_EPISODES, print_every=200)\n",
    "    \n",
    "    print(f\"\\n🏆 TREINAMENTO CONCLUÍDO!\")\n",
    "    print(f\"   Use as células de avaliação para ver os resultados\")\n",
    "    print(f\"   Ou ajuste os parâmetros em SMART_REWARDS_CONFIG para otimizar\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Execute primeiro as células de inicialização do sistema smart!\")\n",
    "\n",
    "# 💡 Instruções para experimentar\n",
    "print(f\"\\n💡 PARA EXPERIMENTAR DIFERENTES CONFIGURAÇÕES:\")\n",
    "print(f\"   1. Modifique os valores em SMART_REWARDS_CONFIG\")\n",
    "print(f\"   2. Re-execute as células de inicialização\")\n",
    "print(f\"   3. Execute novamente o treinamento\")\n",
    "print(f\"   4. Compare os resultados\")\n",
    "\n",
    "print(f\"\\n🔧 EXEMPLOS DE AJUSTES:\")\n",
    "print(f\"   • Aumentar ACTION_BONUS para mais trading\")\n",
    "print(f\"   • Aumentar TIMING_BONUS para melhor timing\")  \n",
    "print(f\"   • Reduzir FAILURE_PENALTY para menos punição\")\n",
    "print(f\"   • Aumentar EXPLORATION_BONUS para mais exploração\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9dd25f",
   "metadata": {},
   "source": [
    "## 📊 AVALIAÇÃO E RESULTADOS BATMAN\n",
    "\n",
    "Teste do agente treinado e análise de performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8148ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Avaliação Completa do Agente Batman\n",
    "def evaluate_batman_agent(agent, env, num_test_episodes=10):\n",
    "    \"\"\"\n",
    "    Avalia o agente treinado em múltiplos episódios de teste\n",
    "    \"\"\"\n",
    "    print(\"🧪 Avaliando agente Batman...\")\n",
    "    test_results = []\n",
    "    \n",
    "    for test_ep in range(num_test_episodes):\n",
    "        # Executar episódio de teste (sem exploração)\n",
    "        state = env.reset()\n",
    "        episode_actions = []\n",
    "        episode_rewards = []\n",
    "        \n",
    "        while True:\n",
    "            # Usar apenas exploitação (greedy policy)\n",
    "            action, _ = agent.get_action(state, training=False)\n",
    "            episode_actions.append(action)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        \n",
    "        # Registrar resultado do teste\n",
    "        episode_summary = env.get_episode_summary()\n",
    "        if episode_summary:\n",
    "            test_results.append({\n",
    "                'episode': test_ep + 1,\n",
    "                'final_value': episode_summary['final_value'],\n",
    "                'total_return': episode_summary['total_return'],\n",
    "                'total_reward': episode_summary['total_reward'],\n",
    "                'actions_taken': episode_summary['actions_taken'],\n",
    "                'num_days': episode_summary['num_days']\n",
    "            })\n",
    "    \n",
    "    # Análise dos resultados\n",
    "    if test_results:\n",
    "        avg_return = np.mean([r['total_return'] for r in test_results])\n",
    "        avg_final_value = np.mean([r['final_value'] for r in test_results])\n",
    "        avg_actions = np.mean([r['actions_taken'] for r in test_results])\n",
    "        win_rate = len([r for r in test_results if r['total_return'] > 0]) / len(test_results)\n",
    "        \n",
    "        print(f\"📈 Resultados da Avaliação Batman ({num_test_episodes} episódios):\")\n",
    "        print(f\"   💰 Valor final médio: R$ {avg_final_value:,.2f}\")\n",
    "        print(f\"   📊 Retorno médio: {avg_return:+.2%}\")\n",
    "        print(f\"   🎯 Taxa de sucesso: {win_rate:.1%}\")\n",
    "        print(f\"   🔄 Ações médias por episódio: {avg_actions:.1f}\")\n",
    "        print(f\"   📅 Dias de trading: {test_results[0]['num_days']}\")\n",
    "        \n",
    "        # Comparar com Buy & Hold\n",
    "        buy_hold_return = (prices[-1] - prices[env.state_manager.window_size]) / prices[env.state_manager.window_size]\n",
    "        print(f\"   📈 Buy & Hold: {buy_hold_return:+.2%}\")\n",
    "        print(f\"   🏆 Alpha vs B&H: {avg_return - buy_hold_return:+.2%}\")\n",
    "        \n",
    "        return {\n",
    "            'avg_return': avg_return,\n",
    "            'avg_final_value': avg_final_value,\n",
    "            'win_rate': win_rate,\n",
    "            'buy_hold_return': buy_hold_return,\n",
    "            'alpha': avg_return - buy_hold_return,\n",
    "            'test_results': test_results\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Criar gráficos de análise\n",
    "def plot_batman_results(training_history, evaluation_results):\n",
    "    \"\"\"\n",
    "    Cria gráficos de análise dos resultados Batman\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'🦇 Batman Q-Learning Results - {TICKER_SYMBOL}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Evolução do treinamento\n",
    "    axes[0,0].plot(training_history['episode'], training_history['avg_return'], 'b-', linewidth=2)\n",
    "    axes[0,0].set_title('Evolução do Retorno (Treinamento)')\n",
    "    axes[0,0].set_xlabel('Episódio')\n",
    "    axes[0,0].set_ylabel('Retorno Médio')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    axes[0,0].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # 2. Evolução do epsilon\n",
    "    axes[0,1].plot(training_history['episode'], training_history['epsilon'], 'g-', linewidth=2)\n",
    "    axes[0,1].set_title('Decaimento da Exploração (Epsilon)')\n",
    "    axes[0,1].set_xlabel('Episódio')\n",
    "    axes[0,1].set_ylabel('Epsilon')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Crescimento da Q-table\n",
    "    axes[1,0].plot(training_history['episode'], training_history['q_table_size'], 'purple', linewidth=2)\n",
    "    axes[1,0].set_title('Crescimento da Q-Table')\n",
    "    axes[1,0].set_xlabel('Episódio')\n",
    "    axes[1,0].set_ylabel('Número de Estados')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Comparação de performance\n",
    "    if evaluation_results:\n",
    "        methods = ['Batman RL', 'Buy & Hold']\n",
    "        returns = [evaluation_results['avg_return'], evaluation_results['buy_hold_return']]\n",
    "        colors = ['blue', 'orange']\n",
    "        \n",
    "        bars = axes[1,1].bar(methods, returns, color=colors, alpha=0.7)\n",
    "        axes[1,1].set_title('Comparação de Performance')\n",
    "        axes[1,1].set_ylabel('Retorno')\n",
    "        axes[1,1].axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Adicionar valores nas barras\n",
    "        for bar, return_val in zip(bars, returns):\n",
    "            height = bar.get_height()\n",
    "            axes[1,1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                          f'{return_val:+.2%}',\n",
    "                          ha='center', va='bottom' if height > 0 else 'top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Executar avaliação\n",
    "if df_stock is not None and 'training_history' in locals():\n",
    "    evaluation_results = evaluate_batman_agent(agent, env, num_test_episodes=20)\n",
    "    plot_batman_results(training_history, evaluation_results)\n",
    "else:\n",
    "    print(\"⚠️ Execute primeiro o treinamento para avaliar o agente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f429fd",
   "metadata": {},
   "source": [
    "## 🔧 TESTE COM OUTRO ATIVO\n",
    "\n",
    "Para usar com outro ativo, simplesmente altere a variável `TICKER_SYMBOL` no início do notebook e execute novamente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1df1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Exemplo: Como trocar para outro ativo\n",
    "\"\"\"\n",
    "Para testar com VALE3, por exemplo:\n",
    "\n",
    "1. Volte à segunda célula do notebook\n",
    "2. Altere: TICKER_SYMBOL = \"VALE3.SA\"  \n",
    "3. Execute novamente todas as células\n",
    "\n",
    "O sistema automaticamente:\n",
    "✅ Baixará os dados da VALE3\n",
    "✅ Reconfigurará o ambiente\n",
    "✅ Treinará o agente com os novos dados\n",
    "✅ Avaliará a performance\n",
    "\n",
    "Ativos suportados (B3):\n",
    "- PETR3.SA, PETR4.SA (Petrobras)\n",
    "- VALE3.SA (Vale)\n",
    "- BRFS3.SA (BRF)\n",
    "- ITUB4.SA (Itaú)\n",
    "- BBAS3.SA (Banco do Brasil)\n",
    "- ABEV3.SA (Ambev)\n",
    "- E muitos outros...\n",
    "\"\"\"\n",
    "\n",
    "print(\"🔧 Sistema Batman configurado para flexibilidade!\")\n",
    "print(f\"📊 Atualmente usando: {TICKER_SYMBOL}\")\n",
    "print(\"💡 Para trocar o ativo, altere TICKER_SYMBOL e re-execute o notebook!\")\n",
    "\n",
    "# Resumo final do Batman\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🦇 RESUMO BATMAN Q-LEARNING\")\n",
    "print(\"=\"*60)\n",
    "print(\"✅ Q-Learning clássico implementado\")\n",
    "print(\"✅ Estados discretizados para estabilidade\") \n",
    "print(\"✅ Tabela Q tradicional\")\n",
    "print(\"✅ Exploração ε-greedy\")\n",
    "print(\"✅ Sistema flexível para qualquer ativo\")\n",
    "print(\"✅ Monitoramento completo de treinamento\")\n",
    "print(\"✅ Avaliação com benchmarks\")\n",
    "print(\"✅ Visualizações analíticas\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b35e5a7",
   "metadata": {},
   "source": [
    "# 🦇 BATMAN APPROACH - Reinforcement Learning Trading\n",
    "\n",
    "## Estratégia: Metodológica e Estruturada\n",
    "\n",
    "### Filosofia Batman\n",
    "- **Preparação meticulosa**: Base teórica sólida seguindo padrões acadêmicos\n",
    "- **Abordagem conservadora**: Q-Learning clássico com tabelas Q\n",
    "- **Metodologia comprovada**: Seguindo estrutura similar às aulas do Prof. Paulo Caixeta\n",
    "- **Estados discretizados**: Preços convertidos em faixas para simplicidade\n",
    "- **Foco pedagógico**: Prioriza entendimento dos fundamentos de RL\n",
    "\n",
    "### Objetivo\n",
    "Desenvolver um agente de Reinforcement Learning para trading automatizado usando **Q-Learning tradicional**.\n",
    "O sistema deve ser **genérico** e funcionar com qualquer ativo (PETR3, VALE3, BRFS3, etc.).\n",
    "\n",
    "### Características da Implementação\n",
    "- ✅ Q-Learning clássico com tabela Q\n",
    "- ✅ Estados discretos (faixas de preços)\n",
    "- ✅ Exploração ε-greedy\n",
    "- ✅ Ambiente compatível com padrões Gymnasium\n",
    "- ✅ Código flexível para múltiplos ativos\n",
    "- ✅ Fundamentação teórica clara"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
