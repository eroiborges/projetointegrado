# Quantum Finance - Agente de Reinforcement Learning para Trading

## ğŸ“Š VisÃ£o Geral do Projeto

Este projeto implementa um agente de **Reinforcement Learning (RL)** utilizando o algoritmo **Deep Q-Network (DQN)** para operar automaticamente em 3 ativos da bolsa brasileira (B3): Vale (VALE3.SA), Petrobras (PETR4.SA) e BRF (BRFS3.SA).

**Objetivo:** Maximizar o lucro em um perÃ­odo de 6 meses com um capital inicial de R$ 10.000,00.

---

## ğŸ¯ Estrutura do Projeto

O projeto estÃ¡ dividido em 5 fases principais:

### Fase 1: PreparaÃ§Ã£o e ConfiguraÃ§Ã£o
- InstalaÃ§Ã£o e importaÃ§Ã£o de bibliotecas
- Download de dados histÃ³ricos via `yfinance`
- PreparaÃ§Ã£o e limpeza dos dados
- DivisÃ£o entre conjunto de treino (2018-2022) e teste (primeiros 6 meses de 2023)

### Fase 2: Modelagem do Problema
- CriaÃ§Ã£o de ambiente customizado `StockTradingEnv` (Gymnasium)
- DefiniÃ§Ã£o de **Estados**, **AÃ§Ãµes** e **Recompensas**

### Fase 3: ImplementaÃ§Ã£o do Algoritmo
- ImplementaÃ§Ã£o do agente **DQN (Deep Q-Network)**
- Rede neural para aproximar funÃ§Ã£o Q
- Experience Replay e Target Network
- Loop de treinamento

### Fase 4: AvaliaÃ§Ã£o do Desempenho
- ComparaÃ§Ã£o com estratÃ©gias baseline:
  - **Buy and Hold**: Compra e mantÃ©m aÃ§Ãµes
  - **Agente AleatÃ³rio**: DecisÃµes aleatÃ³rias
- MÃ©tricas de avaliaÃ§Ã£o:
  - Lucro/PrejuÃ­zo Total
  - Retorno Percentual
  - Sharpe Ratio (mÃ©trica avanÃ§ada)
- VisualizaÃ§Ãµes comparativas

### Fase 5: DocumentaÃ§Ã£o
- ExplicaÃ§Ã£o detalhada da modelagem
- AnÃ¡lise de resultados
- Insights e melhorias futuras

---

## ğŸ”§ Requisitos

### Bibliotecas NecessÃ¡rias

```bash
pip install yfinance gymnasium tensorflow pandas numpy matplotlib seaborn
```

### Principais DependÃªncias
- **Python 3.8+**
- **TensorFlow 2.x**: Deep Learning
- **Gymnasium**: Ambiente de RL
- **yfinance**: Download de dados financeiros
- **pandas, numpy**: ManipulaÃ§Ã£o de dados
- **matplotlib, seaborn**: VisualizaÃ§Ãµes

---

## ğŸ“ˆ Modelagem do Problema

### Estados (State)
Vetor de 7 dimensÃµes:
1. Saldo em dinheiro
2. Quantidade de aÃ§Ãµes da Vale
3. Quantidade de aÃ§Ãµes da Petrobras
4. Quantidade de aÃ§Ãµes da BRF
5. PreÃ§o atual da Vale
6. PreÃ§o atual da Petrobras
7. PreÃ§o atual da BRF

### AÃ§Ãµes (Actions)
EspaÃ§o discreto com 7 aÃ§Ãµes:
- `0`: Manter (nÃ£o fazer nada)
- `1`: Comprar 10 aÃ§Ãµes da Vale
- `2`: Vender 10 aÃ§Ãµes da Vale
- `3`: Comprar 10 aÃ§Ãµes da Petrobras
- `4`: Vender 10 aÃ§Ãµes da Petrobras
- `5`: Comprar 10 aÃ§Ãµes da BRF
- `6`: Vender 10 aÃ§Ãµes da BRF

### Recompensa (Reward)
MudanÃ§a no valor total do portfÃ³lio entre um dia e o prÃ³ximo:
```
Recompensa = Valor_PortfÃ³lio_D - Valor_PortfÃ³lio_{D-1}
```

---

## ğŸš€ Como Usar

### ExecuÃ§Ã£o no Google Colab (Recomendado)

1. Abra o arquivo `quantum_finance.ipynb` no Google Colab
2. Execute as cÃ©lulas sequencialmente
3. O notebook baixarÃ¡ os dados automaticamente e treinarÃ¡ o agente

### ExecuÃ§Ã£o Local

1. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

2. Execute o notebook:
```bash
jupyter notebook quantum_finance.ipynb
```

### ParÃ¢metros ConfigurÃ¡veis

No notebook, vocÃª pode ajustar:
- `episodes`: NÃºmero de episÃ³dios de treinamento (padrÃ£o: 50)
- `capital_inicial`: Capital inicial (padrÃ£o: R$ 10.000)
- `quantidade_acoes`: Quantidade fixa de aÃ§Ãµes por operaÃ§Ã£o (padrÃ£o: 10)
- `learning_rate`: Taxa de aprendizado da rede neural (padrÃ£o: 0.001)
- `episodes` no treinamento: Aumentar para melhor desempenho (recomendado: 100-200)

---

## ğŸ“Š Resultados Esperados

O notebook gera:
1. **GrÃ¡ficos de evoluÃ§Ã£o do portfÃ³lio** durante o treinamento
2. **ComparaÃ§Ã£o visual** entre DQN, Buy and Hold e Agente AleatÃ³rio
3. **Tabela resumo** com mÃ©tricas de desempenho
4. **Sharpe Ratio** para anÃ¡lise de risco-retorno

---

## ğŸ” Estrutura dos Arquivos

```
quantum_finance/
â”‚
â”œâ”€â”€ quantum_finance.ipynb    # Notebook principal com todo o cÃ³digo
â”œâ”€â”€ README.md                 # Este arquivo
â””â”€â”€ requirements.txt          # Lista de dependÃªncias (opcional)
```

---

## ğŸ“ CaracterÃ­sticas TÃ©cnicas

### Arquitetura DQN
- **Camadas**: 3 camadas densas (128-128-64 neurÃ´nios)
- **AtivaÃ§Ã£o**: ReLU com Dropout (0.2) para regularizaÃ§Ã£o
- **Experience Replay**: Buffer de 10.000 transiÃ§Ãµes
- **Target Network**: Atualizado a cada 5 episÃ³dios
- **Epsilon-Greedy**: ExploraÃ§Ã£o inicial 100%, decaindo atÃ© 1%

### Ambiente Customizado
- Herda de `gymnasium.Env`
- Implementa mÃ©todos: `reset()`, `step()`, `render()`
- ValidaÃ§Ã£o de aÃ§Ãµes (saldo suficiente, aÃ§Ãµes suficientes para venda)

---

## ğŸ’¡ Melhorias Futuras

1. **Feature Engineering**: Adicionar indicadores tÃ©cnicos (RSI, MACD, mÃ©dias mÃ³veis)
2. **AÃ§Ãµes ContÃ­nuas**: Permitir quantidades variÃ¡veis em vez de fixas
3. **Custos de TransaÃ§Ã£o**: Incluir taxas e impostos (IR, corretagem)
4. **Algoritmos AvanÃ§ados**: Testar Dueling DQN, Double DQN, PPO, A3C
5. **Risk Management**: Implementar stop-loss e take-profit
6. **Multi-timeframe**: Considerar dados de diferentes perÃ­odos
7. **MÃºltiplos Ativos**: Expandir para mais aÃ§Ãµes simultaneamente

---

## ğŸ“ Notas Importantes

- âš ï¸ **Este Ã© um projeto acadÃªmico**. NÃ£o use para trading real sem extensivos testes e validaÃ§Ãµes.
- ğŸ“Š Os dados sÃ£o histÃ³ricos e nÃ£o garantem desempenho futuro.
- ğŸ”„ O treinamento pode levar alguns minutos dependendo do nÃºmero de episÃ³dios.
- ğŸ² Sementes aleatÃ³rias estÃ£o fixas para reprodutibilidade dos resultados.

---

## ğŸ“š ReferÃªncias

- **Gymnasium**: https://gymnasium.farama.org/
- **Deep Q-Learning (DQN)**: Mnih et al., 2015
- **yfinance**: https://github.com/ranaroussi/yfinance
- **TensorFlow**: https://www.tensorflow.org/

---

## ğŸ‘¤ Autor

Projeto desenvolvido para o **MBA em Data Science e Analytics**.

---

## ğŸ“„ LicenÃ§a

Este projeto Ã© apenas para fins educacionais e acadÃªmicos.

---

**Desenvolvido com â¤ï¸ usando Reinforcement Learning e Python**

