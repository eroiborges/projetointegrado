{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8lDQuQEnS8_"
      },
      "source": [
        "# Aula 1 - Reinforcement Learning\n",
        "\n",
        "## Demo 1: Uma introdu√ß√£o ao aprendizado por refor√ßo usando o t√°xi do Gymnasium üöï\n",
        "\n",
        "### Prof. Paulo Caixeta (profpaulo.oliveira@fiap.com.br)\n",
        "\n",
        "\n",
        "Agradecimentos: prof Ahirton Lopes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziMM6qgmncE5"
      },
      "source": [
        "Nesta primeira demo, aplicaremos Aprendizagem por Refor√ßo (Reinforcement Learning - RL) para treinar um agente que resolva o exemplo ['T√°xi' do Gymnasium](https://gymnasium.farama.org/environments/toy_text/taxi/).\n",
        "\n",
        "T√≥picos:\n",
        "\n",
        "- Introdu√ß√£o (resumida) √† RL;\n",
        "- Configurando o Gymnasium e o exemplo do Taxi;\n",
        "- Usando o algoritmo Q-learning para treinar nosso agente de t√°xi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfYSvJF6ocqk"
      },
      "source": [
        "# 'Taxi': o que √©?\n",
        "\n",
        "T√°xi √© um dos muitos ambientes dispon√≠veis no Gymnasium. Esses ambientes s√£o usados para desenvolver e avaliar algoritmos de aprendizagem por refor√ßo.\n",
        "\n",
        "### O Cen√°rio:\n",
        "\n",
        "Imagine uma pequena grade de 5x5. Nesse mundo, existem:\n",
        "\n",
        "* Um t√°xi (que √© o nosso agente).\n",
        "\n",
        "* Um passageiro.\n",
        "\n",
        "* Quatro locais de embarque/desembarque designados pelas letras: R (Red), G (Green), Y (Yellow), B (Blue).\n",
        "\n",
        "### O Objetivo:\n",
        "\n",
        "A tarefa do t√°xi √©, de forma aut√¥noma, pegar um passageiro em um dos locais e deix√°-lo no seu destino, que √© outro desses quatro locais. O desafio √© fazer isso no menor n√∫mero de passos poss√≠vel.\n",
        "\n",
        "Neste tutorial, vamos sair de a√ß√µes aleat√≥rias‚Ä¶ at√© aprender a ser o melhor taxista da ciadade!\n",
        "\n",
        ".........................![agente aleat√≥rio](https://drive.google.com/uc?id=1l0XizDh9eGP3gVNCjJHrC0M3DeCWI8Fj)...............................................................\n",
        "![agente treinado](https://drive.google.com/uc?id=1a-OeLhXi3W-kvQuhGRyJ1dOSw4vrIBxr)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZUF3oE-o889"
      },
      "source": [
        "# Uma breve introdu√ß√£o ao Aprendizado por Refor√ßo\n",
        "\n",
        "Vamos usar a analogia de adestrar (por exemplo, ensinar a dar a pata) um cachorro:\n",
        "\n",
        "- Se ele executar o comando corretamente, voc√™ o recompensar√° com um petisco (feedback positivo)\n",
        "- Caso contr√°rio, n√£o recebe recompensa (feedback negativo)\n",
        "\n",
        "Ao continuar a fazer coisas que levam a _refor√ßos_ positivos, o c√£o executar√° o truque ao ouvir o comando para receber a guloseima.\n",
        "\n",
        "O aprendizado por refor√ßo √© um subdom√≠nio do aprendizado de m√°quina que envolve treinar um _agente_ (o cachorro) para aprender as sequ√™ncias corretas de _a√ß√µes_ a serem executadas (levantar a pata e colocar sobre a sua m√£o) em seu _ambiente_ (em resposta ao comando 'pata'), a fim de maximizar sua _recompensa_. (recebendo uma biscoito). Isso pode ser ilustrado mais formalmente como:\n",
        "\n",
        "![sutton barto rl](https://www.gocoder.one/static/RL-diagram-b3654cd3d5cc0e07a61a214977038f01.png \"Diagrama de aprendizado por refor√ßo\")\n",
        "\n",
        "Fonte: [Sutton &¬†Barto](http://incompleteideas.net/book/bookdraft2017nov5.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aWkaLM_o2pH"
      },
      "source": [
        "# Instalando o Gymnasium e¬†Taxi\n",
        "\n",
        "Usaremos o ambiente 'Taxi-v3' para este tutorial. Para instalar o gym (e numpy para depois), execute a c√©lula abaixo:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyjebRSHnK1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e3061d9-a725-424a-cb0b-a06479cf7516"
      },
      "source": [
        "!pip install gymnasium\n",
        "!pip install numpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAJK0BtorfEO"
      },
      "source": [
        "Em seguida, importamos as bibliotecas que usaremos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7As7qh4navx"
      },
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# bibliotecas de visualiza√ß√£o\n",
        "from IPython.display import display, clear_output\n",
        "from time import sleep"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyMhgh3RsLWk"
      },
      "source": [
        "Criando o ambiente 'Taxi-v3':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpYHWA95y_QJ"
      },
      "source": [
        "# creating Taxi environment with Gymnasium\n",
        "env = gym.make('Taxi-v3', render_mode='ansi')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWMjVABSsnWT"
      },
      "source": [
        "#  Agente¬†aleat√≥rio\n",
        "\n",
        "Come√ßaremos implementando um agente que n√£o aprende nada, apenas \"anda por a√≠\". Ele servir√° como nosso *baseline*.\n",
        "\n",
        "O primeiro passo √© dar ao nosso agente as informa√ß√µes do estado inicial. Um estado informa ao nosso agente como √© o ambiente atual.\n",
        "\n",
        "No T√°xi, um estado define as posi√ß√µes atuais do t√°xi, do passageiro e dos locais de embarque e desembarque. Abaixo est√£o exemplos de tr√™s estados diferentes para t√°xi:\n",
        "\n",
        "![estados de t√°xi](https://www.gocoder.one/static/taxi-states-0aad1b011cf3fe07b571712f2123335c.png \"Diferentes estados de t√°xi\")\n",
        "\n",
        "Nota: Bloco Amarelo = t√°xi, Letra em azul = local de retirada, Letra em roxo = destino de entrega"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNlV-YvdnlOH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb647c8b-de81-40a4-b34f-8e8a5fd6b691"
      },
      "source": [
        "# Criando o estado inicial\n",
        "state = env.reset()\n",
        "\n",
        "print(f\"Initial state: {state}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state: (344, {'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM7GRNHnvRaH"
      },
      "source": [
        "A seguir, executaremos um loop for para percorrer o jogo. Em cada itera√ß√£o, nosso agente ir√°:\n",
        "\n",
        "1. Fazer uma a√ß√£o aleat√≥ria a partir do espa√ßo de a√ß√£o (0‚Ää-‚Ääsul, 1‚Ää-‚Äänorte, 2‚Ää-‚Ääleste, 3‚Ää-‚Ääoeste, 4‚Ää- pegar passageiro, 5‚Ää- desembarcar passageiro)\n",
        "2. Receber o novo estado\n",
        "\n",
        "Aqui est√° nosso agente aleat√≥rio:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aycxOXzQnoLU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "6e944c67-fcf9-4a7c-8c80-da38e9019c9b"
      },
      "source": [
        "num_steps = 99\n",
        "for s in range(num_steps+1):\n",
        "\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    print(f\"step: {s} out of {num_steps}\")\n",
        "\n",
        "    # escolhe uma a√ß√£o aleat√≥ria para executar\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    # executa esta a√ß√£o\n",
        "    env.step(action)\n",
        "\n",
        "    # imprime o ambiente\n",
        "    print(env.render())\n",
        "\n",
        "    sleep(0.4) #tempo para podermos conseguir ver as mudan√ßas\n",
        "\n",
        "# encerra esta inst√¢ncia\n",
        "env.close()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 68 out of 99\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | :\u001b[43m \u001b[0m| : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2757117384.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# end this instance of the taxi environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I-Whw1WxDra"
      },
      "source": [
        "Ao executar a c√©lula acima, voc√™ ver√° seu agente fazendo movimentos aleat√≥rios. O intuito √© se familiarizar com o ambiente, a biblioteca e o exerc√≠cio\n",
        "\n",
        "A seguir, implementaremos o algoritmo Q-learning que permitir√° ao nosso agente aprender com as recompensas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcmS8OwLyDL5"
      },
      "source": [
        "# Agente Q-Learning\n",
        "\n",
        "Q-learning √© um algoritmo de aprendizagem por refor√ßo que busca encontrar a melhor pr√≥xima a√ß√£o poss√≠vel dado seu estado atual, a fim de maximizar a recompensa que recebe (o 'Q' em Q-learning significa qualidade).\n",
        "\n",
        "Vamos considerar o seguinte estado inicial:\n",
        "\n",
        "![estado do t√°xi](https://www.gocoder.one/static/start-state-6a115a72f07cea072c28503d3abf9819.png \"Um exemplo de estado do t√°xi\")\n",
        "\n",
        "Qual a√ß√£o (para cima, para baixo, para a esquerda, para a direita, para pegar ou largar) ele deve realizar para maximizar sua recompensa? (_Lembrando: azul = local de retirada e roxo = destino de entrega_)\n",
        "\n",
        "Primeiro, vamos dar uma olhada em como nosso agente √© ‚Äúrecompensado‚Äù por suas a√ß√µes. **Lembre-se de que, no aprendizado por refor√ßo, queremos que nosso agente execute a√ß√µes que maximizem as poss√≠veis recompensas futuras que ele recebe de seu ambiente.**\n",
        "\n",
        "## Sistema de recompensas \"T√°xi\"\n",
        "\n",
        "De acordo com a [documenta√ß√£o do t√°xi](https://gymnasium.farama.org/environments/toy_text/taxi/):\n",
        "\n",
        "> _\"‚Ä¶voc√™ recebe +20 pontos por uma entrega bem-sucedida e perde 1 ponto para cada intervalo de tempo necess√°rio. H√° tamb√©m uma penalidade de 10 pontos para a√ß√µes ilegais de coleta e entrega.\"_\n",
        "\n",
        "Olhando para o nosso estado original, as a√ß√µes poss√≠veis que ele pode realizar e as recompensas correspondentes que receber√° s√£o mostradas abaixo:\n",
        "\n",
        "![recompensas de t√°xi](https://www.gocoder.one/static/state-rewards-62ab43a53e07062b531b3199a8bab5b3.png \"Recompensas de t√°xi\")\n",
        "\n",
        "Na imagem acima, o agente perde 1 ponto por deslocamento que realiza. Ele tamb√©m perder√° 10 pontos se usar a a√ß√£o de retirada ou entrega aqui.\n",
        "\n",
        "Queremos que nosso agente v√° para o norte em dire√ß√£o ao local de coleta indicado por um R azul- **mas como ele saber√° qual a√ß√£o tomar se todos forem igualmente punitivos?**\n",
        "\n",
        "## Explora√ß√£o (Exploration)\n",
        "\n",
        "Atualmente, nosso agente n√£o tem como saber qual a√ß√£o o levar√° mais pr√≥ximo do R azul. √â aqui que entra a tentativa e erro - faremos nosso agente realizar a√ß√µes aleat√≥rias e observar quais recompensas ele recebe (ou seja, nosso agente ir√° **explorar**).\n",
        "\n",
        "Ao longo de muitas itera√ß√µes e muitos epis√≥dios, nosso agente ter√° observado que certas _sequ√™ncias_ de a√ß√µes ser√£o mais gratificantes que outras. Ao longo do caminho, nosso agente precisar√° acompanhar quais a√ß√µes levaram a quais recompensas.\n",
        "\n",
        "## Apresentando‚Ä¶ tabelas Q\n",
        "\n",
        "Uma tabela Q √© simplesmente uma tabela de consulta que armazena valores que representam as recompensas futuras m√°ximas que nosso agente pode esperar para uma determinada a√ß√£o em um determinado estado (_conhecidos como valores Q_). Isso dir√° ao nosso agente que, quando ele encontra um determinado estado, algumas a√ß√µes tem recompensa total maior. Torna-se uma 'mapa da mina' informando ao nosso agente qual √© a melhor a√ß√£o a ser tomada.\n",
        "\n",
        "A imagem abaixo ilustra como √© a 'tabela Q' do ambiente 'T√°xi-v3':\n",
        "\n",
        "- Cada **linha** corresponde a um estado √∫nico no ambiente 'T√°xi'\n",
        "- Cada **coluna** corresponde a uma a√ß√£o que nosso agente pode realizar\n",
        "- Cada **c√©lula** corresponde ao valor Q para esse par estado-a√ß√£o‚Ää-‚Ääum valor Q mais alto significa uma recompensa m√°xima mais alta que nosso agente pode esperar obter se realizar essa a√ß√£o naquele estado.\n",
        "\n",
        "![Tabela Q](https://www.gocoder.one/static/q-table-9461cc903f50b78d757ea30aeb3eb8bc.png \"Tabela Q\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVvgYgquUKBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6121bcc-8daf-4624-84cd-eace43fccaae"
      },
      "source": [
        "# Antes de come√ßarmos a treinar nosso agente,\n",
        "# precisaremos inicializar nossa tabela Q da seguinte forma:\n",
        "\n",
        "state_size = env.observation_space.n  # total number of states (S)\n",
        "action_size = env.action_space.n      # total number of actions (A)\n",
        "\n",
        "# inicializando a tabela Q zerada (n√£o conhecemos ainda nada sobre o ambiente)\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "\n",
        "print(f\"Q table: {qtable}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q table: [[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0uvTrFXmJk7"
      },
      "source": [
        "√Ä medida que nosso agente explora, ele atualizar√° a tabela Q com os valores Q que encontrar. Para calcular nossos valores Q, apresentaremos o algoritmo Q-learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdpsBFdJm9ve"
      },
      "source": [
        "# Algoritmo Q-Learning\n",
        "\n",
        "O algoritmo Q-learning √© fornecido abaixo. N√£o entraremos em detalhes, mas voc√™ pode ler mais sobre isso no [Cap√≠tulo 6 de Sutton & Barto (2018)](http://www.incompleteideas.net/book/RLbook2018trimmed.pdf).\n",
        "\n",
        "![Algoritmo de aprendizagem Q](https://www.gocoder.one/static/q-learning-algorithm-84b84bb5dc16ba8097e31aff7ea42748.png \"O algoritmo de aprendizagem Q\")\n",
        "\n",
        "O algoritmo Q-learning ajudar√° nosso agente a **atualizar o valor Q atual ($Q(S_t,A_t)$) com suas observa√ß√µes ap√≥s realizar uma a√ß√£o.** Ou seja, aumente Q se encontrar uma recompensa positiva ou diminua Q se encontrar uma recompensa negativa.\n",
        "\n",
        "#### Um ponto imortante aqui √©:\n",
        "observe que no T√°xi, nosso agente n√£o recebe uma recompensa positiva at√© que deixe um passageiro com sucesso (_+20 pontos_). Portanto, mesmo que nosso agente esteja indo na dire√ß√£o correta, haver√° um atraso na recompensa positiva que deveria receber. O seguinte termo na equa√ß√£o Q-learning aborda isso:\n",
        "\n",
        "![q m√°ximo](https://www.gocoder.one/static/max-q-e593ddcec76cda87ed189c31d60837b6.png \"Valor m√°ximo de Q\")\n",
        "\n",
        "Este termo ajusta nosso valor Q atual para incluir uma parte das recompensas que ele poder√° receber em algum momento no futuro (St+1). O termo `a` refere-se a todas as a√ß√µes poss√≠veis dispon√≠veis para esse estado. A equa√ß√£o tamb√©m cont√©m dois hiperpar√¢metros que podemos especificar:\n",
        "\n",
        "1. Taxa de aprendizagem (Œ±): qu√£o facilmente o agente deve aceitar novas informa√ß√µes em vez de informa√ß√µes aprendidas anteriormente\n",
        "2. Fator de desconto (Œ≥): quanto o agente deve levar em considera√ß√£o as recompensas que poder√° receber no futuro versus sua recompensa imediata\n",
        "\n",
        "Aqui est√° nossa implementa√ß√£o do algoritmo Q-learning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsOWYTX4VsDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca2d19a4-36b7-4eef-c72d-7d6e55872b7e"
      },
      "source": [
        "# hyperparametros (podem ser refinados)\n",
        "learning_rate = 0.9\n",
        "discount_rate = 0.8\n",
        "\n",
        "# vari√°veis\n",
        "reward = 10                                 # R_(t+1)\n",
        "state = env.observation_space.sample()      # S_t\n",
        "action = env.action_space.sample()          # A_t\n",
        "new_state = env.observation_space.sample()  # S_(t+1)\n",
        "\n",
        "# Algoritmo Qlearning: Q(s,a) := Q(s,a) + learning_rate * (reward + discount_rate * max Q(s',a') - Q(s,a))\n",
        "qtable[state, action] += learning_rate * (reward + discount_rate * np.max(qtable[new_state,:]) - qtable[state,action])\n",
        "\n",
        "print(f\"Q-value for (state, action) pair ({state}, {action}): {qtable[state,action]}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-value for (state, action) pair (395, 2): 9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx5IMA1idW9X"
      },
      "source": [
        "## Compara√ß√£o entre Exploration e Exploitation (Trade Off)\n",
        "\n",
        "Podemos deixar nosso agente explorar para atualizar nossa tabela Q usando o algoritmo Q-learning. √Ä medida que nosso agente aprende mais sobre o ambiente, podemos deix√°-lo usar esse conhecimento para realizar a√ß√µes mais otimizadas e convergir mais rapidamente‚Ää-‚Ääconhecido como **exploitation**.\n",
        "\n",
        "Durante o exploitation, nosso agente examinar√° sua tabela Q e selecionar√° a a√ß√£o com o valor Q mais alto (em vez de uma a√ß√£o aleat√≥ria). Com o tempo, nosso agente precisar√° explorar menos e, em vez disso, come√ßar \"exploitar\" o que sabe.\n",
        "\n",
        "Aqui est√° nossa implementa√ß√£o de uma estrat√©gia cl√°ssica de exploration-exploitation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aorBEvYdSLr"
      },
      "source": [
        "# vari√°veis dummy\n",
        "episode = random.randint(0,500)\n",
        "qtable = np.random.randn(env.observation_space.sample(), env.action_space.sample())\n",
        "\n",
        "# hyperparametros\n",
        "epsilon = 1.0     # probabilidade de EXPLORAR\n",
        "decay_rate = 0.01 # decaimento do epsilon\n",
        "\n",
        "if random.uniform(0,1) < epsilon:\n",
        "    # explora\n",
        "    action = env.action_space.sample()\n",
        "else:\n",
        "    # exploit\n",
        "    action = np.argmax(qtable[state,:])\n",
        "\n",
        "# epsilon diminui exponencialmente --> a cada etapa, o agente explora menos\n",
        "epsilon = np.exp(-decay_rate*episode)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_zL2Vrd1yLv"
      },
      "source": [
        "No exemplo acima, definimos algum valor `√©psilon` entre 0 e 1. Se `√©psilon` for 0.7, h√° 70% de chance de que nesta etapa nosso agente explore em vez de exploit. `epsilon` decai exponencialmente a cada passo, de modo que nosso agente explora cada vez menos ao longo do tempo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5mA3SflarKs"
      },
      "source": [
        "# Reunindo tudo\n",
        "\n",
        "Conclu√≠mos todos os blocos de constru√ß√£o necess√°rios para nosso agente de aprendizagem por refor√ßo.\n",
        "\n",
        "O processo de treinamento do nosso ter√° o seguinte esqueleto:\n",
        "\n",
        "1. Inicializar nossa tabela Q zerada\n",
        "2. \"Soltar\" o agente para explorar diversas vezes\n",
        "3. Atualizar continuamente a tabela Q usando o algoritmo Q-learning, equilibrando o _trade-off_ exploration-exploitation\n",
        "\n",
        "Aqui est√° a implementa√ß√£o completa:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiXAK2OdpteR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fa8b63d-41a0-4415-96f8-85ba69b6d694"
      },
      "source": [
        "# para adicionar um pouco de cor\n",
        "class bcolors:\n",
        "    RED = '\\u001b[31m'\n",
        "    GREEN = '\\u001b[32m'\n",
        "    RESET = '\\u001b[0m'\n",
        "\n",
        "# criando ambeinte novo\n",
        "env = gym.make('Taxi-v3', render_mode='ansi')\n",
        "\n",
        "# inicializando a tabela Q\n",
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "\n",
        "# hyperparametros\n",
        "learning_rate = 0.9\n",
        "discount_rate = 0.8\n",
        "epsilon = 1.0\n",
        "decay_rate= 0.005\n",
        "\n",
        "# vari√°veis de treinamento\n",
        "num_episodes = 2000 # equivalente √† √©pcoas, em outros algoritmos\n",
        "max_steps = 99 # m√°ximo de passos por epis√≥deo, caso n√£o atinja o objetivo antes\n",
        "\n",
        "print(\"IN√çCIO DO TREINAMENTO...\")\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "\n",
        "\t# Reseta o ambiente\n",
        "\tstate, info = env.reset(seed=42)\n",
        "\tstep = 0\n",
        "\tdone = False # marcador de fim de epis√≥dio\n",
        "\n",
        "\tfor step in range(max_steps):\n",
        "\n",
        "\t\t# Exploration-exploitation tradeoff\n",
        "\t\tif random.uniform(0,1) < epsilon:\n",
        "\t\t\t# Explora\n",
        "\t\t\taction = env.action_space.sample()\n",
        "\t\telse:\n",
        "\t\t\t# Exploit\n",
        "\t\t\taction = np.argmax(qtable[state,:])\n",
        "\n",
        "\t\t# Take an action and observe the reward\n",
        "\t\tnext_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "\t\t# Q-learning algorithm\n",
        "\t\tqtable[state, action] = qtable[state, action] + learning_rate * (reward + discount_rate * np.max(qtable[next_state, :]) - qtable[state, action])\n",
        "\n",
        "\t\t# atualiza o estado\n",
        "\t\tstate = next_state\n",
        "\n",
        "\t\t# verifica se chegou ao fim\n",
        "\t\tif done or truncated:\n",
        "\t\t\tbreak\n",
        "\n",
        "\t# Decrease epsilon\n",
        "\tepsilon = np.exp(-decay_rate*episode)\n",
        "\n",
        "# Get ready to watch our trained agent\n",
        "clear_output()\n",
        "print(f\"Tabela Q: {qtable}\")\n",
        "print(f\"Treino completo ap√≥s {num_episodes} episodios\")\n",
        "input(\"Pressione enter para ver o agente treinado\")\n",
        "sleep(1)\n",
        "clear_output()\n",
        "\n",
        "episodes_to_preview = 3\n",
        "for episode in range(episodes_to_preview):\n",
        "\n",
        "\t# Reseta o ambiente\n",
        "\tstate, info = env.reset(seed=42)\n",
        "\tstep = 0\n",
        "\tdone = False\n",
        "\tepisode_rewards = 0\n",
        "\n",
        "\tfor step in range(max_steps):\n",
        "\t\tclear_output(wait=True)\n",
        "\n",
        "\t\tprint(f\"TRAINED AGENT\")\n",
        "\t\tprint(f\"+++++EPISODE {episode+1}+++++\")\n",
        "\t\tprint(f\"Step {step+1}\")\n",
        "\n",
        "\t\t# Exploit\n",
        "\t\taction = np.argmax(qtable[state,:])\n",
        "\n",
        "\t\t# executa uma a√ß√£o e observa a recompensa\n",
        "\t\tnext_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "\t\t# Acumula a recomepnsa\n",
        "\t\tepisode_rewards += reward\n",
        "\n",
        "\t\tprint(env.render())\n",
        "\t\tprint(\"\")\n",
        "\t\tif episode_rewards < 0:\n",
        "\t\t\tprint(f\"Score: {bcolors.RED}{episode_rewards}{bcolors.RESET}\")\n",
        "\t\telse:\n",
        "\t\t\tprint(f\"Score: {bcolors.GREEN}{episode_rewards}{bcolors.RESET}\")\n",
        "\t\tsleep(0.5)\n",
        "\n",
        "\t\t# atualiza o estado\n",
        "\t\tstate = next_state\n",
        "\n",
        "\t\t# verifica se chegou ao final\n",
        "\t\tif done or truncated:\n",
        "\t\t\tbreak\n",
        "\n",
        "# Close the Taxi environment\n",
        "env.close()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINED AGENT\n",
            "+++++EPISODE 3+++++\n",
            "Step 13\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "\n",
            "Score: \u001b[32m8\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_tmtN9koho8"
      },
      "source": [
        "# O que vem a seguir?\n",
        "\n",
        "Existem muitos outros ambientes dispon√≠veis no Gymnasium para voc√™ experimentar (por exemplo, [Frozen Lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)). Voc√™ tamb√©m pode tentar otimizar a implementa√ß√£o acima para resolver o T√°xi em menos passos.\n",
        "\n",
        "Alguns outros recursos √∫teis incluem:\n",
        "- [S√©rie de palestras de aprendizagem por refor√ßo DeepMind x UCL [2021]](https://www.youtube.com/watch?v=TCCjZe0y4Qc&ab_channel=GoogleDeepMind) (no Youtube)\n",
        "- [Uma (longa) espiada na aprendizagem por refor√ßo](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html) por Lilian Weng\n",
        "- [Um bom artigo sobre RL e suas aplica√ß√µes no mundo real](https://www.altexsoft.com/blog/datascience/reinforcement-learning-explained-overview-comparisons-and-applications-in-business/)\n",
        "- [Document√°rio completo do AlphaGo](https://www.youtube.com/watch?v=WXuK6gekU1Y) (no Youtube)\n",
        "- [Aprendizagem por Refor√ßo](http://www.incompleteideas.net/book/RLbook2018trimmed.pdf) por Sutton e Barto\n",
        "- [Introdu√ß√£o pr√°tica ao aprendizado por refor√ßo profundo](https://www.gocoder.one/blog/hands-on-introduction-to-deep-reinforcement-learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# O que resolvemos via Reinforcement Learning?\n",
        "\n",
        "* Programa√ß√£o de elevador\n",
        "* Passeio de bicicleta\n",
        "* Dire√ß√£o de navio\n",
        "* Controle de biorreator\n",
        "* Controle de helic√≥ptero de acrobacias\n",
        "* Programa√ß√£o de partidas de aeroporto\n",
        "* Regulamenta√ß√£o e preserva√ß√£o de ecossistemas\n",
        "* Futebol Robocup\n",
        "* Jogo de videogame (Atari, Starcraft...)\n",
        "* Jogo de Go"
      ],
      "metadata": {
        "id": "nCdT5x2wqjrF"
      }
    }
  ]
}