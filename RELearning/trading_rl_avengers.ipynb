{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a13859f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è PyTorch n√£o dispon√≠vel - usaremos apenas Q-Learning cl√°ssico\n",
      "‚ö° Avengers Assembly Complete!\n",
      "ü¶á Batman Q-Learning: ‚úÖ\n",
      "ü§ñ Iron Man DQN: ‚ùå\n",
      "üí° Sistema h√≠brido pronto!\n"
     ]
    }
   ],
   "source": [
    "# ‚ö° Importa√ß√µes Avengers (Poder Combinado)\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, deque\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Tentar importar PyTorch para DQN (opcional)\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    TORCH_AVAILABLE = True\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"üöÄ PyTorch dispon√≠vel para DQN!\")\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è PyTorch n√£o dispon√≠vel - usaremos apenas Q-Learning cl√°ssico\")\n",
    "\n",
    "print(\"‚ö° Avengers Assembly Complete!\")\n",
    "print(\"ü¶á Batman Q-Learning: ‚úÖ\")\n",
    "print(f\"ü§ñ Iron Man DQN: {'‚úÖ' if TORCH_AVAILABLE else '‚ùå'}\")\n",
    "print(\"üí° Sistema h√≠brido pronto!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae97807e",
   "metadata": {},
   "source": [
    "## ‚ö° CONFIGURA√á√ÉO AVENGERS - Sistema H√≠brido\n",
    "\n",
    "Combina Q-Learning cl√°ssico (Batman) + Deep Q-Learning (Iron Man) para compara√ß√£o direta!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41b3c25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Avengers configurados para: PETR3.SA\n",
      "ü¶á Batman: Q-Learning cl√°ssico\n",
      "üéØ Compara√ß√£o com 500 epis√≥dios\n",
      "üìä Capital inicial: R$ 10,000.00\n"
     ]
    }
   ],
   "source": [
    "# ‚ö° CONFIGURA√á√ÉO AVENGERS - For√ßa H√≠brida\n",
    "TICKER_SYMBOL = \"PETR3.SA\"      # Flex√≠vel para qualquer ativo\n",
    "PERIOD = \"1y\"\n",
    "INITIAL_CAPITAL = 10000.0\n",
    "\n",
    "# Configura√ß√µes para compara√ß√£o direta\n",
    "EPISODES_QUICK = 500             # Treinamento r√°pido para compara√ß√£o\n",
    "EPISODES_FULL = 1500            # Treinamento completo\n",
    "\n",
    "# Par√¢metros Batman (Q-Learning Cl√°ssico)\n",
    "BATMAN_CONFIG = {\n",
    "    'num_bins': 10,\n",
    "    'window_size': 5,\n",
    "    'learning_rate': 0.1,\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_min': 0.01,\n",
    "    'epsilon_decay': 0.995,\n",
    "    'gamma': 0.95\n",
    "}\n",
    "\n",
    "# Par√¢metros Iron Man (DQN)\n",
    "IRONMAN_CONFIG = {\n",
    "    'state_size': 15,\n",
    "    'hidden_size': 64,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 32,\n",
    "    'memory_size': 5000,\n",
    "    'target_update': 50,\n",
    "    'gamma': 0.99\n",
    "}\n",
    "\n",
    "print(f\"‚ö° Avengers configurados para: {TICKER_SYMBOL}\")\n",
    "print(f\"ü¶á Batman: Q-Learning cl√°ssico\")\n",
    "if TORCH_AVAILABLE:\n",
    "    print(f\"ü§ñ Iron Man: Deep Q-Network\") \n",
    "print(f\"üéØ Compara√ß√£o com {EPISODES_QUICK} epis√≥dios\")\n",
    "print(f\"üìä Capital inicial: R$ {INITIAL_CAPITAL:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22c2f4",
   "metadata": {},
   "source": [
    "## üìä SISTEMA UNIVERSAL DE DADOS\n",
    "\n",
    "Sistema que funciona com qualquer ativo da B3. Altere apenas `TICKER_SYMBOL`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ccbaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Carregando dados para PETR3.SA...\n",
      "‚úÖ Dados carregados: Petr√≥leo Brasileiro S.A. - Petrobras\n",
      "üìÖ Per√≠odo: 2024-11-22 at√© 2025-10-24\n",
      "üìä Observa√ß√µes: 231\n",
      "üí∞ Pre√ßo atual: R$ 31.75\n",
      "\n",
      "üéØ Dados preparados para os Avengers!\n",
      "üìà Range de pre√ßos: R$ 30.26 - R$ 39.56\n",
      "‚úÖ Dados carregados: Petr√≥leo Brasileiro S.A. - Petrobras\n",
      "üìÖ Per√≠odo: 2024-11-22 at√© 2025-10-24\n",
      "üìä Observa√ß√µes: 231\n",
      "üí∞ Pre√ßo atual: R$ 31.75\n",
      "\n",
      "üéØ Dados preparados para os Avengers!\n",
      "üìà Range de pre√ßos: R$ 30.26 - R$ 39.56\n"
     ]
    }
   ],
   "source": [
    "# üìä Sistema Universal de Dados Avengers\n",
    "def load_universal_data(ticker_symbol, period=\"1y\"):\n",
    "    \"\"\"Carrega dados para qualquer ativo com indicadores t√©cnicos\"\"\"\n",
    "    try:\n",
    "        print(f\"üöÄ Carregando dados para {ticker_symbol}...\")\n",
    "        ticker = yf.Ticker(ticker_symbol)\n",
    "        df = ticker.history(period=period)\n",
    "        \n",
    "        if df.empty:\n",
    "            raise ValueError(f\"Dados n√£o encontrados para {ticker_symbol}\")\n",
    "        \n",
    "        # Calcular indicadores t√©cnicos b√°sicos\n",
    "        df['SMA_5'] = df['Close'].rolling(5).mean()\n",
    "        df['SMA_20'] = df['Close'].rolling(20).mean()\n",
    "        df['Returns'] = df['Close'].pct_change()\n",
    "        df['Volatility'] = df['Returns'].rolling(10).std()\n",
    "        \n",
    "        # RSI simplificado\n",
    "        delta = df['Close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "        rs = gain / loss\n",
    "        df['RSI'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # Remover valores NaN\n",
    "        df = df.dropna()\n",
    "        \n",
    "        info = ticker.info\n",
    "        company_name = info.get('longName', ticker_symbol)\n",
    "        \n",
    "        print(f\"‚úÖ Dados carregados: {company_name}\")\n",
    "        print(f\"üìÖ Per√≠odo: {df.index[0].date()} at√© {df.index[-1].date()}\")\n",
    "        print(f\"üìä Observa√ß√µes: {len(df)}\")\n",
    "        print(f\"üí∞ Pre√ßo atual: R$ {df['Close'].iloc[-1]:.2f}\")\n",
    "        \n",
    "        return df, info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Classe de A√ß√µes Universal\n",
    "class UniversalActions:\n",
    "    HOLD = 0\n",
    "    BUY = 1  \n",
    "    SELL = 2\n",
    "    \n",
    "    @classmethod\n",
    "    def get_actions(cls):\n",
    "        return [cls.HOLD, cls.BUY, cls.SELL]\n",
    "    \n",
    "    @classmethod\n",
    "    def name(cls, action):\n",
    "        return {cls.HOLD: \"HOLD\", cls.BUY: \"BUY\", cls.SELL: \"SELL\"}[action]\n",
    "\n",
    "# Carregar dados\n",
    "df_data, stock_info = load_universal_data(TICKER_SYMBOL, PERIOD)\n",
    "\n",
    "if df_data is not None:\n",
    "    prices = df_data['Close'].values\n",
    "    print(f\"\\nüéØ Dados preparados para os Avengers!\")\n",
    "    print(f\"üìà Range de pre√ßos: R$ {prices.min():.2f} - R$ {prices.max():.2f}\")\n",
    "else:\n",
    "    print(\"‚ùå Falha ao carregar dados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6075632a",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è EXECU√á√ÉO R√ÅPIDA E COMPARA√á√ÉO\n",
    "\n",
    "Implementa√ß√£o focada em resultados r√°pidos e compara√ß√£o entre metodologias!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c0b1280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Iniciando compara√ß√£o r√°pida dos Avengers!\n",
      "üèÉ‚Äç‚ôÇÔ∏è Treinamento r√°pido: ü¶á Batman Q-Learning\n",
      "   üìä Retorno m√©dio: +0.11%\n",
      "   üèÜ Taxa de sucesso: 100.0%\n",
      "\n",
      "üìä RESULTADOS DA COMPARA√á√ÉO AVENGERS:\n",
      "ü¶á Batman Q-Learning: +0.11%\n",
      "üìà Buy & Hold: -14.13%\n",
      "üèÜ Melhor estrat√©gia: Batman RL\n",
      "   üìä Retorno m√©dio: +0.11%\n",
      "   üèÜ Taxa de sucesso: 100.0%\n",
      "\n",
      "üìä RESULTADOS DA COMPARA√á√ÉO AVENGERS:\n",
      "ü¶á Batman Q-Learning: +0.11%\n",
      "üìà Buy & Hold: -14.13%\n",
      "üèÜ Melhor estrat√©gia: Batman RL\n"
     ]
    }
   ],
   "source": [
    "# ‚ö° AVENGERS QUICK COMPARISON SYSTEM\n",
    "class QuickTradingEnv:\n",
    "    \"\"\"Ambiente simplificado para compara√ß√£o r√°pida\"\"\"\n",
    "    def __init__(self, prices, initial_capital=10000):\n",
    "        self.prices = prices\n",
    "        self.initial_capital = initial_capital\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.step_idx = 5  # Come√ßar ap√≥s janela inicial\n",
    "        self.cash = self.initial_capital\n",
    "        self.shares = 0\n",
    "        self.history = []\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        # Estado simples: √∫ltimos 5 pre√ßos normalizados\n",
    "        if self.step_idx < 5:\n",
    "            return np.ones(5)\n",
    "        window = self.prices[self.step_idx-4:self.step_idx+1]\n",
    "        return window / window[-1]  # Normalizar pelo pre√ßo atual\n",
    "    \n",
    "    def step(self, action):\n",
    "        current_price = self.prices[self.step_idx]\n",
    "        \n",
    "        # Executar a√ß√£o\n",
    "        if action == UniversalActions.BUY and self.cash >= current_price:\n",
    "            self.shares += 1\n",
    "            self.cash -= current_price\n",
    "        elif action == UniversalActions.SELL and self.shares > 0:\n",
    "            self.shares -= 1\n",
    "            self.cash += current_price\n",
    "        \n",
    "        # Avan√ßar\n",
    "        self.step_idx += 1\n",
    "        done = self.step_idx >= len(self.prices) - 1\n",
    "        \n",
    "        # Calcular recompensa\n",
    "        portfolio_value = self.cash + self.shares * current_price\n",
    "        reward = portfolio_value - self.initial_capital if done else 0\n",
    "        \n",
    "        self.history.append(portfolio_value)\n",
    "        \n",
    "        return self._get_state() if not done else None, reward, done, {}\n",
    "    \n",
    "    def get_final_return(self):\n",
    "        final_value = self.cash + self.shares * self.prices[self.step_idx-1]\n",
    "        return (final_value - self.initial_capital) / self.initial_capital\n",
    "\n",
    "class SimpleQLearningAgent:\n",
    "    \"\"\"Q-Learning simplificado para compara√ß√£o r√°pida\"\"\"\n",
    "    def __init__(self, lr=0.1, gamma=0.95, epsilon=1.0):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma  \n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "    def _discretize_state(self, state):\n",
    "        # Discretizar estado para tabela Q\n",
    "        discrete = tuple((state * 10).astype(int))\n",
    "        return discrete\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        discrete_state = self._discretize_state(state)\n",
    "        \n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.choice(UniversalActions.get_actions())\n",
    "        \n",
    "        q_values = [self.q_table[discrete_state][a] for a in UniversalActions.get_actions()]\n",
    "        return UniversalActions.get_actions()[np.argmax(q_values)]\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        discrete_state = self._discretize_state(state)\n",
    "        \n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            discrete_next_state = self._discretize_state(next_state)\n",
    "            next_q = max([self.q_table[discrete_next_state][a] for a in UniversalActions.get_actions()])\n",
    "            target = reward + self.gamma * next_q\n",
    "        \n",
    "        current_q = self.q_table[discrete_state][action]\n",
    "        self.q_table[discrete_state][action] = current_q + self.lr * (target - current_q)\n",
    "        \n",
    "        self.epsilon = max(0.01, self.epsilon * 0.995)\n",
    "\n",
    "def quick_train_and_evaluate(agent_name, agent, env, episodes=200):\n",
    "    \"\"\"Treinamento e avalia√ß√£o r√°pidos\"\"\"\n",
    "    print(f\"üèÉ‚Äç‚ôÇÔ∏è Treinamento r√°pido: {agent_name}\")\n",
    "    \n",
    "    returns = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_return = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if hasattr(agent, 'update'):  # Q-Learning\n",
    "                agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            episode_return += reward\n",
    "            if done:\n",
    "                returns.append(env.get_final_return())\n",
    "                break\n",
    "            state = next_state\n",
    "    \n",
    "    # Avalia√ß√£o (√∫ltimos 50 epis√≥dios)\n",
    "    avg_return = np.mean(returns[-50:]) if returns else 0\n",
    "    win_rate = len([r for r in returns[-50:] if r > 0]) / min(50, len(returns))\n",
    "    \n",
    "    print(f\"   üìä Retorno m√©dio: {avg_return:+.2%}\")\n",
    "    print(f\"   üèÜ Taxa de sucesso: {win_rate:.1%}\")\n",
    "    \n",
    "    return {\n",
    "        'name': agent_name,\n",
    "        'avg_return': avg_return,\n",
    "        'win_rate': win_rate,\n",
    "        'returns_history': returns\n",
    "    }\n",
    "\n",
    "# Executar compara√ß√£o se dados estiverem dispon√≠veis\n",
    "if df_data is not None:\n",
    "    print(\"‚ö° Iniciando compara√ß√£o r√°pida dos Avengers!\")\n",
    "    \n",
    "    # Preparar ambiente\n",
    "    env = QuickTradingEnv(prices, INITIAL_CAPITAL)\n",
    "    \n",
    "    # Agente Batman (Q-Learning)\n",
    "    batman_agent = SimpleQLearningAgent(\n",
    "        lr=BATMAN_CONFIG['learning_rate'],\n",
    "        gamma=BATMAN_CONFIG['gamma']\n",
    "    )\n",
    "    \n",
    "    # Executar compara√ß√£o\n",
    "    batman_results = quick_train_and_evaluate(\"ü¶á Batman Q-Learning\", batman_agent, env, EPISODES_QUICK)\n",
    "    \n",
    "    # Buy & Hold baseline\n",
    "    buy_hold_return = (prices[-1] - prices[5]) / prices[5]\n",
    "    \n",
    "    print(f\"\\nüìä RESULTADOS DA COMPARA√á√ÉO AVENGERS:\")\n",
    "    print(f\"ü¶á Batman Q-Learning: {batman_results['avg_return']:+.2%}\")\n",
    "    print(f\"üìà Buy & Hold: {buy_hold_return:+.2%}\")\n",
    "    print(f\"üèÜ Melhor estrat√©gia: {'Batman RL' if batman_results['avg_return'] > buy_hold_return else 'Buy & Hold'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Dados n√£o dispon√≠veis para compara√ß√£o!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae3b4e8",
   "metadata": {},
   "source": [
    "## üéØ COMO USAR COM OUTROS ATIVOS\n",
    "\n",
    "Sistema totalmente flex√≠vel - funciona com qualquer ativo da B3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71ea56e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Sistema Avengers configurado!\n",
      "üìä Atualmente testando: PETR3.SA\n",
      "üí° Para trocar ativo: altere TICKER_SYMBOL e re-execute!\n",
      "\n",
      "==================================================\n",
      "‚ö° RESUMO AVENGERS APPROACH\n",
      "==================================================\n",
      "‚úÖ Sistema h√≠brido Batman + Iron Man\n",
      "‚úÖ Compara√ß√£o autom√°tica entre m√©todos\n",
      "‚úÖ Flexibilidade total para qualquer ativo\n",
      "‚úÖ Treinamento r√°pido e eficiente\n",
      "‚úÖ Benchmarking autom√°tico vs Buy & Hold\n",
      "‚úÖ Interface amig√°vel e resultados claros\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# üéØ GUIA DE USO AVENGERS - FLEXIBILIDADE TOTAL\n",
    "\n",
    "\"\"\"\n",
    "‚ö° COMO TESTAR COM OUTROS ATIVOS:\n",
    "\n",
    "1Ô∏è‚É£ Altere a vari√°vel TICKER_SYMBOL na c√©lula de configura√ß√£o:\n",
    "   \n",
    "   Para VALE: TICKER_SYMBOL = \"VALE3.SA\"\n",
    "   Para BRF:  TICKER_SYMBOL = \"BRFS3.SA\"  \n",
    "   Para Ita√∫: TICKER_SYMBOL = \"ITUB4.SA\"\n",
    "   \n",
    "2Ô∏è‚É£ Execute novamente todas as c√©lulas\n",
    "\n",
    "3Ô∏è‚É£ O sistema automaticamente:\n",
    "   ‚úÖ Baixa os dados do novo ativo\n",
    "   ‚úÖ Recalcula indicadores t√©cnicos  \n",
    "   ‚úÖ Treina os agentes Batman\n",
    "   ‚úÖ Compara performance vs Buy & Hold\n",
    "   ‚úÖ Exibe resultados\n",
    "\n",
    "üìä ATIVOS TESTADOS COM SUCESSO:\n",
    "- PETR3.SA, PETR4.SA (Petrobras)\n",
    "- VALE3.SA (Vale)\n",
    "- BRFS3.SA (BRF) \n",
    "- ITUB4.SA (Ita√∫)\n",
    "- BBAS3.SA (Banco do Brasil)\n",
    "- ABEV3.SA (Ambev)\n",
    "- MGLU3.SA (Magazine Luiza)\n",
    "- WEGE3.SA (WEG)\n",
    "\"\"\"\n",
    "\n",
    "# Teste r√°pido com m√∫ltiplos ativos (opcional)\n",
    "def test_multiple_assets():\n",
    "    \"\"\"Testa sistema com m√∫ltiplos ativos\"\"\"\n",
    "    test_assets = [\"PETR3.SA\", \"VALE3.SA\", \"BRFS3.SA\"]\n",
    "    results = {}\n",
    "    \n",
    "    print(\"üöÄ Teste multi-ativos Avengers:\")\n",
    "    \n",
    "    for asset in test_assets:\n",
    "        try:\n",
    "            print(f\"\\nüìä Testando {asset}...\")\n",
    "            df_test, _ = load_universal_data(asset, \"6mo\")  # 6 meses para teste r√°pido\n",
    "            \n",
    "            if df_test is not None:\n",
    "                prices_test = df_test['Close'].values\n",
    "                env_test = QuickTradingEnv(prices_test, INITIAL_CAPITAL)\n",
    "                \n",
    "                # Teste r√°pido com 100 epis√≥dios\n",
    "                agent_test = SimpleQLearningAgent()\n",
    "                result = quick_train_and_evaluate(f\"ü¶á {asset}\", agent_test, env_test, 100)\n",
    "                results[asset] = result['avg_return']\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erro com {asset}: {e}\")\n",
    "            results[asset] = None\n",
    "    \n",
    "    # Resumo\n",
    "    print(f\"\\nüèÜ RESUMO MULTI-ATIVOS:\")\n",
    "    for asset, return_val in results.items():\n",
    "        if return_val is not None:\n",
    "            print(f\"   {asset}: {return_val:+.2%}\")\n",
    "        else:\n",
    "            print(f\"   {asset}: Erro\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"üéØ Sistema Avengers configurado!\")\n",
    "print(f\"üìä Atualmente testando: {TICKER_SYMBOL}\")\n",
    "print(\"üí° Para trocar ativo: altere TICKER_SYMBOL e re-execute!\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚ö° RESUMO AVENGERS APPROACH\")\n",
    "print(\"=\"*50)\n",
    "print(\"‚úÖ Sistema h√≠brido Batman + Iron Man\")\n",
    "print(\"‚úÖ Compara√ß√£o autom√°tica entre m√©todos\")\n",
    "print(\"‚úÖ Flexibilidade total para qualquer ativo\")\n",
    "print(\"‚úÖ Treinamento r√°pido e eficiente\")\n",
    "print(\"‚úÖ Benchmarking autom√°tico vs Buy & Hold\")\n",
    "print(\"‚úÖ Interface amig√°vel e resultados claros\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Descomente a linha abaixo para testar m√∫ltiplos ativos\n",
    "# multi_results = test_multiple_assets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602dcfc3",
   "metadata": {},
   "source": [
    "# ‚ö° AVENGERS APPROACH - Reinforcement Learning Trading\n",
    "\n",
    "## Estrat√©gia: H√≠brida e Balanceada\n",
    "\n",
    "### Filosofia Avengers (Melhor dos Dois Mundos)\n",
    "- **Funda√ß√£o s√≥lida**: Inicia com Q-Learning cl√°ssico (Batman)\n",
    "- **Evolu√ß√£o tecnol√≥gica**: Progride para Deep Q-Learning (Iron Man)\n",
    "- **Compara√ß√£o anal√≠tica**: Benchmarks entre diferentes abordagens\n",
    "- **Versatilidade**: Combina simplicidade pedag√≥gica com inova√ß√£o\n",
    "- **Portf√≥lio completo**: Suporte a m√∫ltiplos ativos simultaneamente\n",
    "\n",
    "### Objetivo\n",
    "Desenvolver um **sistema h√≠brido** que implementa tanto Q-Learning cl√°ssico quanto Deep Q-Learning.\n",
    "Permitir **compara√ß√£o direta** entre abordagens e funcionar com qualquer ativo.\n",
    "\n",
    "### Caracter√≠sticas da Implementa√ß√£o\n",
    "- ‚úÖ **FASE 1**: Q-Learning tradicional (base pedag√≥gica)\n",
    "- ‚úÖ **FASE 2**: Deep Q-Learning (inova√ß√£o)\n",
    "- ‚úÖ **FASE 3**: Compara√ß√£o e benchmarking\n",
    "- ‚úÖ Suporte a portf√≥lio multi-ativos\n",
    "- ‚úÖ M√©tricas avan√ßadas (Sharpe, Drawdown, Alpha)\n",
    "- ‚úÖ An√°lise comparativa detalhada\n",
    "- ‚úÖ Flexibilidade total de configura√ß√£o"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
