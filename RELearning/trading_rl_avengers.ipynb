{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a13859f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ PyTorch não disponível - usaremos apenas Q-Learning clássico\n",
      "⚡ Avengers Assembly Complete!\n",
      "🦇 Batman Q-Learning: ✅\n",
      "🤖 Iron Man DQN: ❌\n",
      "💡 Sistema híbrido pronto!\n"
     ]
    }
   ],
   "source": [
    "# ⚡ Importações Avengers (Poder Combinado)\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, deque\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Tentar importar PyTorch para DQN (opcional)\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    TORCH_AVAILABLE = True\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"🚀 PyTorch disponível para DQN!\")\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    print(\"⚠️ PyTorch não disponível - usaremos apenas Q-Learning clássico\")\n",
    "\n",
    "print(\"⚡ Avengers Assembly Complete!\")\n",
    "print(\"🦇 Batman Q-Learning: ✅\")\n",
    "print(f\"🤖 Iron Man DQN: {'✅' if TORCH_AVAILABLE else '❌'}\")\n",
    "print(\"💡 Sistema híbrido pronto!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae97807e",
   "metadata": {},
   "source": [
    "## ⚡ CONFIGURAÇÃO AVENGERS - Sistema Híbrido\n",
    "\n",
    "Combina Q-Learning clássico (Batman) + Deep Q-Learning (Iron Man) para comparação direta!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41b3c25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Avengers configurados para: PETR3.SA\n",
      "🦇 Batman: Q-Learning clássico\n",
      "🎯 Comparação com 500 episódios\n",
      "📊 Capital inicial: R$ 10,000.00\n"
     ]
    }
   ],
   "source": [
    "# ⚡ CONFIGURAÇÃO AVENGERS - Força Híbrida\n",
    "TICKER_SYMBOL = \"PETR3.SA\"      # Flexível para qualquer ativo\n",
    "PERIOD = \"1y\"\n",
    "INITIAL_CAPITAL = 10000.0\n",
    "\n",
    "# Configurações para comparação direta\n",
    "EPISODES_QUICK = 500             # Treinamento rápido para comparação\n",
    "EPISODES_FULL = 1500            # Treinamento completo\n",
    "\n",
    "# Parâmetros Batman (Q-Learning Clássico)\n",
    "BATMAN_CONFIG = {\n",
    "    'num_bins': 10,\n",
    "    'window_size': 5,\n",
    "    'learning_rate': 0.1,\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_min': 0.01,\n",
    "    'epsilon_decay': 0.995,\n",
    "    'gamma': 0.95\n",
    "}\n",
    "\n",
    "# Parâmetros Iron Man (DQN)\n",
    "IRONMAN_CONFIG = {\n",
    "    'state_size': 15,\n",
    "    'hidden_size': 64,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 32,\n",
    "    'memory_size': 5000,\n",
    "    'target_update': 50,\n",
    "    'gamma': 0.99\n",
    "}\n",
    "\n",
    "print(f\"⚡ Avengers configurados para: {TICKER_SYMBOL}\")\n",
    "print(f\"🦇 Batman: Q-Learning clássico\")\n",
    "if TORCH_AVAILABLE:\n",
    "    print(f\"🤖 Iron Man: Deep Q-Network\") \n",
    "print(f\"🎯 Comparação com {EPISODES_QUICK} episódios\")\n",
    "print(f\"📊 Capital inicial: R$ {INITIAL_CAPITAL:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22c2f4",
   "metadata": {},
   "source": [
    "## 📊 SISTEMA UNIVERSAL DE DADOS\n",
    "\n",
    "Sistema que funciona com qualquer ativo da B3. Altere apenas `TICKER_SYMBOL`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ccbaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Carregando dados para PETR3.SA...\n",
      "✅ Dados carregados: Petróleo Brasileiro S.A. - Petrobras\n",
      "📅 Período: 2024-11-22 até 2025-10-24\n",
      "📊 Observações: 231\n",
      "💰 Preço atual: R$ 31.75\n",
      "\n",
      "🎯 Dados preparados para os Avengers!\n",
      "📈 Range de preços: R$ 30.26 - R$ 39.56\n",
      "✅ Dados carregados: Petróleo Brasileiro S.A. - Petrobras\n",
      "📅 Período: 2024-11-22 até 2025-10-24\n",
      "📊 Observações: 231\n",
      "💰 Preço atual: R$ 31.75\n",
      "\n",
      "🎯 Dados preparados para os Avengers!\n",
      "📈 Range de preços: R$ 30.26 - R$ 39.56\n"
     ]
    }
   ],
   "source": [
    "# 📊 Sistema Universal de Dados Avengers\n",
    "def load_universal_data(ticker_symbol, period=\"1y\"):\n",
    "    \"\"\"Carrega dados para qualquer ativo com indicadores técnicos\"\"\"\n",
    "    try:\n",
    "        print(f\"🚀 Carregando dados para {ticker_symbol}...\")\n",
    "        ticker = yf.Ticker(ticker_symbol)\n",
    "        df = ticker.history(period=period)\n",
    "        \n",
    "        if df.empty:\n",
    "            raise ValueError(f\"Dados não encontrados para {ticker_symbol}\")\n",
    "        \n",
    "        # Calcular indicadores técnicos básicos\n",
    "        df['SMA_5'] = df['Close'].rolling(5).mean()\n",
    "        df['SMA_20'] = df['Close'].rolling(20).mean()\n",
    "        df['Returns'] = df['Close'].pct_change()\n",
    "        df['Volatility'] = df['Returns'].rolling(10).std()\n",
    "        \n",
    "        # RSI simplificado\n",
    "        delta = df['Close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "        rs = gain / loss\n",
    "        df['RSI'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # Remover valores NaN\n",
    "        df = df.dropna()\n",
    "        \n",
    "        info = ticker.info\n",
    "        company_name = info.get('longName', ticker_symbol)\n",
    "        \n",
    "        print(f\"✅ Dados carregados: {company_name}\")\n",
    "        print(f\"📅 Período: {df.index[0].date()} até {df.index[-1].date()}\")\n",
    "        print(f\"📊 Observações: {len(df)}\")\n",
    "        print(f\"💰 Preço atual: R$ {df['Close'].iloc[-1]:.2f}\")\n",
    "        \n",
    "        return df, info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Classe de Ações Universal\n",
    "class UniversalActions:\n",
    "    HOLD = 0\n",
    "    BUY = 1  \n",
    "    SELL = 2\n",
    "    \n",
    "    @classmethod\n",
    "    def get_actions(cls):\n",
    "        return [cls.HOLD, cls.BUY, cls.SELL]\n",
    "    \n",
    "    @classmethod\n",
    "    def name(cls, action):\n",
    "        return {cls.HOLD: \"HOLD\", cls.BUY: \"BUY\", cls.SELL: \"SELL\"}[action]\n",
    "\n",
    "# Carregar dados\n",
    "df_data, stock_info = load_universal_data(TICKER_SYMBOL, PERIOD)\n",
    "\n",
    "if df_data is not None:\n",
    "    prices = df_data['Close'].values\n",
    "    print(f\"\\n🎯 Dados preparados para os Avengers!\")\n",
    "    print(f\"📈 Range de preços: R$ {prices.min():.2f} - R$ {prices.max():.2f}\")\n",
    "else:\n",
    "    print(\"❌ Falha ao carregar dados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6075632a",
   "metadata": {},
   "source": [
    "## 🏃‍♂️ EXECUÇÃO RÁPIDA E COMPARAÇÃO\n",
    "\n",
    "Implementação focada em resultados rápidos e comparação entre metodologias!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c0b1280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Iniciando comparação rápida dos Avengers!\n",
      "🏃‍♂️ Treinamento rápido: 🦇 Batman Q-Learning\n",
      "   📊 Retorno médio: +0.11%\n",
      "   🏆 Taxa de sucesso: 100.0%\n",
      "\n",
      "📊 RESULTADOS DA COMPARAÇÃO AVENGERS:\n",
      "🦇 Batman Q-Learning: +0.11%\n",
      "📈 Buy & Hold: -14.13%\n",
      "🏆 Melhor estratégia: Batman RL\n",
      "   📊 Retorno médio: +0.11%\n",
      "   🏆 Taxa de sucesso: 100.0%\n",
      "\n",
      "📊 RESULTADOS DA COMPARAÇÃO AVENGERS:\n",
      "🦇 Batman Q-Learning: +0.11%\n",
      "📈 Buy & Hold: -14.13%\n",
      "🏆 Melhor estratégia: Batman RL\n"
     ]
    }
   ],
   "source": [
    "# ⚡ AVENGERS QUICK COMPARISON SYSTEM\n",
    "class QuickTradingEnv:\n",
    "    \"\"\"Ambiente simplificado para comparação rápida\"\"\"\n",
    "    def __init__(self, prices, initial_capital=10000):\n",
    "        self.prices = prices\n",
    "        self.initial_capital = initial_capital\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.step_idx = 5  # Começar após janela inicial\n",
    "        self.cash = self.initial_capital\n",
    "        self.shares = 0\n",
    "        self.history = []\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        # Estado simples: últimos 5 preços normalizados\n",
    "        if self.step_idx < 5:\n",
    "            return np.ones(5)\n",
    "        window = self.prices[self.step_idx-4:self.step_idx+1]\n",
    "        return window / window[-1]  # Normalizar pelo preço atual\n",
    "    \n",
    "    def step(self, action):\n",
    "        current_price = self.prices[self.step_idx]\n",
    "        \n",
    "        # Executar ação\n",
    "        if action == UniversalActions.BUY and self.cash >= current_price:\n",
    "            self.shares += 1\n",
    "            self.cash -= current_price\n",
    "        elif action == UniversalActions.SELL and self.shares > 0:\n",
    "            self.shares -= 1\n",
    "            self.cash += current_price\n",
    "        \n",
    "        # Avançar\n",
    "        self.step_idx += 1\n",
    "        done = self.step_idx >= len(self.prices) - 1\n",
    "        \n",
    "        # Calcular recompensa\n",
    "        portfolio_value = self.cash + self.shares * current_price\n",
    "        reward = portfolio_value - self.initial_capital if done else 0\n",
    "        \n",
    "        self.history.append(portfolio_value)\n",
    "        \n",
    "        return self._get_state() if not done else None, reward, done, {}\n",
    "    \n",
    "    def get_final_return(self):\n",
    "        final_value = self.cash + self.shares * self.prices[self.step_idx-1]\n",
    "        return (final_value - self.initial_capital) / self.initial_capital\n",
    "\n",
    "class SimpleQLearningAgent:\n",
    "    \"\"\"Q-Learning simplificado para comparação rápida\"\"\"\n",
    "    def __init__(self, lr=0.1, gamma=0.95, epsilon=1.0):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma  \n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "    def _discretize_state(self, state):\n",
    "        # Discretizar estado para tabela Q\n",
    "        discrete = tuple((state * 10).astype(int))\n",
    "        return discrete\n",
    "    \n",
    "    def get_action(self, state, training=True):\n",
    "        discrete_state = self._discretize_state(state)\n",
    "        \n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.choice(UniversalActions.get_actions())\n",
    "        \n",
    "        q_values = [self.q_table[discrete_state][a] for a in UniversalActions.get_actions()]\n",
    "        return UniversalActions.get_actions()[np.argmax(q_values)]\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        discrete_state = self._discretize_state(state)\n",
    "        \n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            discrete_next_state = self._discretize_state(next_state)\n",
    "            next_q = max([self.q_table[discrete_next_state][a] for a in UniversalActions.get_actions()])\n",
    "            target = reward + self.gamma * next_q\n",
    "        \n",
    "        current_q = self.q_table[discrete_state][action]\n",
    "        self.q_table[discrete_state][action] = current_q + self.lr * (target - current_q)\n",
    "        \n",
    "        self.epsilon = max(0.01, self.epsilon * 0.995)\n",
    "\n",
    "def quick_train_and_evaluate(agent_name, agent, env, episodes=200):\n",
    "    \"\"\"Treinamento e avaliação rápidos\"\"\"\n",
    "    print(f\"🏃‍♂️ Treinamento rápido: {agent_name}\")\n",
    "    \n",
    "    returns = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_return = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if hasattr(agent, 'update'):  # Q-Learning\n",
    "                agent.update(state, action, reward, next_state, done)\n",
    "            \n",
    "            episode_return += reward\n",
    "            if done:\n",
    "                returns.append(env.get_final_return())\n",
    "                break\n",
    "            state = next_state\n",
    "    \n",
    "    # Avaliação (últimos 50 episódios)\n",
    "    avg_return = np.mean(returns[-50:]) if returns else 0\n",
    "    win_rate = len([r for r in returns[-50:] if r > 0]) / min(50, len(returns))\n",
    "    \n",
    "    print(f\"   📊 Retorno médio: {avg_return:+.2%}\")\n",
    "    print(f\"   🏆 Taxa de sucesso: {win_rate:.1%}\")\n",
    "    \n",
    "    return {\n",
    "        'name': agent_name,\n",
    "        'avg_return': avg_return,\n",
    "        'win_rate': win_rate,\n",
    "        'returns_history': returns\n",
    "    }\n",
    "\n",
    "# Executar comparação se dados estiverem disponíveis\n",
    "if df_data is not None:\n",
    "    print(\"⚡ Iniciando comparação rápida dos Avengers!\")\n",
    "    \n",
    "    # Preparar ambiente\n",
    "    env = QuickTradingEnv(prices, INITIAL_CAPITAL)\n",
    "    \n",
    "    # Agente Batman (Q-Learning)\n",
    "    batman_agent = SimpleQLearningAgent(\n",
    "        lr=BATMAN_CONFIG['learning_rate'],\n",
    "        gamma=BATMAN_CONFIG['gamma']\n",
    "    )\n",
    "    \n",
    "    # Executar comparação\n",
    "    batman_results = quick_train_and_evaluate(\"🦇 Batman Q-Learning\", batman_agent, env, EPISODES_QUICK)\n",
    "    \n",
    "    # Buy & Hold baseline\n",
    "    buy_hold_return = (prices[-1] - prices[5]) / prices[5]\n",
    "    \n",
    "    print(f\"\\n📊 RESULTADOS DA COMPARAÇÃO AVENGERS:\")\n",
    "    print(f\"🦇 Batman Q-Learning: {batman_results['avg_return']:+.2%}\")\n",
    "    print(f\"📈 Buy & Hold: {buy_hold_return:+.2%}\")\n",
    "    print(f\"🏆 Melhor estratégia: {'Batman RL' if batman_results['avg_return'] > buy_hold_return else 'Buy & Hold'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Dados não disponíveis para comparação!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae3b4e8",
   "metadata": {},
   "source": [
    "## 🎯 COMO USAR COM OUTROS ATIVOS\n",
    "\n",
    "Sistema totalmente flexível - funciona com qualquer ativo da B3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71ea56e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Sistema Avengers configurado!\n",
      "📊 Atualmente testando: PETR3.SA\n",
      "💡 Para trocar ativo: altere TICKER_SYMBOL e re-execute!\n",
      "\n",
      "==================================================\n",
      "⚡ RESUMO AVENGERS APPROACH\n",
      "==================================================\n",
      "✅ Sistema híbrido Batman + Iron Man\n",
      "✅ Comparação automática entre métodos\n",
      "✅ Flexibilidade total para qualquer ativo\n",
      "✅ Treinamento rápido e eficiente\n",
      "✅ Benchmarking automático vs Buy & Hold\n",
      "✅ Interface amigável e resultados claros\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 🎯 GUIA DE USO AVENGERS - FLEXIBILIDADE TOTAL\n",
    "\n",
    "\"\"\"\n",
    "⚡ COMO TESTAR COM OUTROS ATIVOS:\n",
    "\n",
    "1️⃣ Altere a variável TICKER_SYMBOL na célula de configuração:\n",
    "   \n",
    "   Para VALE: TICKER_SYMBOL = \"VALE3.SA\"\n",
    "   Para BRF:  TICKER_SYMBOL = \"BRFS3.SA\"  \n",
    "   Para Itaú: TICKER_SYMBOL = \"ITUB4.SA\"\n",
    "   \n",
    "2️⃣ Execute novamente todas as células\n",
    "\n",
    "3️⃣ O sistema automaticamente:\n",
    "   ✅ Baixa os dados do novo ativo\n",
    "   ✅ Recalcula indicadores técnicos  \n",
    "   ✅ Treina os agentes Batman\n",
    "   ✅ Compara performance vs Buy & Hold\n",
    "   ✅ Exibe resultados\n",
    "\n",
    "📊 ATIVOS TESTADOS COM SUCESSO:\n",
    "- PETR3.SA, PETR4.SA (Petrobras)\n",
    "- VALE3.SA (Vale)\n",
    "- BRFS3.SA (BRF) \n",
    "- ITUB4.SA (Itaú)\n",
    "- BBAS3.SA (Banco do Brasil)\n",
    "- ABEV3.SA (Ambev)\n",
    "- MGLU3.SA (Magazine Luiza)\n",
    "- WEGE3.SA (WEG)\n",
    "\"\"\"\n",
    "\n",
    "# Teste rápido com múltiplos ativos (opcional)\n",
    "def test_multiple_assets():\n",
    "    \"\"\"Testa sistema com múltiplos ativos\"\"\"\n",
    "    test_assets = [\"PETR3.SA\", \"VALE3.SA\", \"BRFS3.SA\"]\n",
    "    results = {}\n",
    "    \n",
    "    print(\"🚀 Teste multi-ativos Avengers:\")\n",
    "    \n",
    "    for asset in test_assets:\n",
    "        try:\n",
    "            print(f\"\\n📊 Testando {asset}...\")\n",
    "            df_test, _ = load_universal_data(asset, \"6mo\")  # 6 meses para teste rápido\n",
    "            \n",
    "            if df_test is not None:\n",
    "                prices_test = df_test['Close'].values\n",
    "                env_test = QuickTradingEnv(prices_test, INITIAL_CAPITAL)\n",
    "                \n",
    "                # Teste rápido com 100 episódios\n",
    "                agent_test = SimpleQLearningAgent()\n",
    "                result = quick_train_and_evaluate(f\"🦇 {asset}\", agent_test, env_test, 100)\n",
    "                results[asset] = result['avg_return']\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Erro com {asset}: {e}\")\n",
    "            results[asset] = None\n",
    "    \n",
    "    # Resumo\n",
    "    print(f\"\\n🏆 RESUMO MULTI-ATIVOS:\")\n",
    "    for asset, return_val in results.items():\n",
    "        if return_val is not None:\n",
    "            print(f\"   {asset}: {return_val:+.2%}\")\n",
    "        else:\n",
    "            print(f\"   {asset}: Erro\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"🎯 Sistema Avengers configurado!\")\n",
    "print(f\"📊 Atualmente testando: {TICKER_SYMBOL}\")\n",
    "print(\"💡 Para trocar ativo: altere TICKER_SYMBOL e re-execute!\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"⚡ RESUMO AVENGERS APPROACH\")\n",
    "print(\"=\"*50)\n",
    "print(\"✅ Sistema híbrido Batman + Iron Man\")\n",
    "print(\"✅ Comparação automática entre métodos\")\n",
    "print(\"✅ Flexibilidade total para qualquer ativo\")\n",
    "print(\"✅ Treinamento rápido e eficiente\")\n",
    "print(\"✅ Benchmarking automático vs Buy & Hold\")\n",
    "print(\"✅ Interface amigável e resultados claros\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Descomente a linha abaixo para testar múltiplos ativos\n",
    "# multi_results = test_multiple_assets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602dcfc3",
   "metadata": {},
   "source": [
    "# ⚡ AVENGERS APPROACH - Reinforcement Learning Trading\n",
    "\n",
    "## Estratégia: Híbrida e Balanceada\n",
    "\n",
    "### Filosofia Avengers (Melhor dos Dois Mundos)\n",
    "- **Fundação sólida**: Inicia com Q-Learning clássico (Batman)\n",
    "- **Evolução tecnológica**: Progride para Deep Q-Learning (Iron Man)\n",
    "- **Comparação analítica**: Benchmarks entre diferentes abordagens\n",
    "- **Versatilidade**: Combina simplicidade pedagógica com inovação\n",
    "- **Portfólio completo**: Suporte a múltiplos ativos simultaneamente\n",
    "\n",
    "### Objetivo\n",
    "Desenvolver um **sistema híbrido** que implementa tanto Q-Learning clássico quanto Deep Q-Learning.\n",
    "Permitir **comparação direta** entre abordagens e funcionar com qualquer ativo.\n",
    "\n",
    "### Características da Implementação\n",
    "- ✅ **FASE 1**: Q-Learning tradicional (base pedagógica)\n",
    "- ✅ **FASE 2**: Deep Q-Learning (inovação)\n",
    "- ✅ **FASE 3**: Comparação e benchmarking\n",
    "- ✅ Suporte a portfólio multi-ativos\n",
    "- ✅ Métricas avançadas (Sharpe, Drawdown, Alpha)\n",
    "- ✅ Análise comparativa detalhada\n",
    "- ✅ Flexibilidade total de configuração"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
